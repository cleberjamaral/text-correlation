b'\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n1\\n\\nLogical Representations and \\nComputational Methods for Markov \\nDecision Processes\\n\\nCraig Boutilier\\n\\nDepartment of Computer Science\\n\\nUniversity of Toronto\\n\\n\\xef\\xbf\\xbd\\nNASSLI Lecture Slides (c) 2002, C. Boutilier\\n\\nPlanning in Artificial Intelligence\\n\\nPlanning has a long history in AI\\n\\xe2\\x80\\xa2 strong interaction with logic-based knowledge \\n\\nrepresentation and reasoning schemes\\n\\nBasic planning problem:\\n\\xe2\\x80\\xa2 Given: start state, goal conditions, actions\\n\\xe2\\x80\\xa2 Find: sequence of actions leading from start to goal\\n\\xe2\\x80\\xa2 Typically: states correspond to possible worlds; \\n\\nactions and goals specified using a logical formalism \\n(e.g., STRIPS, situation calculus, temporal logic, etc.)\\n\\nSpecialized algorithms, planning as theorem \\nproving, etc. often exploit logical structure of \\nproblem is various ways to solve effectively\\n\\n\\n\\n2\\n\\n\\xef\\xbf\\xbd\\nNASSLI Lecture Slides (c) 2002, C. Boutilier\\n\\nA Planning Problem\\n\\n\\xef\\xbf\\xbd\\nNASSLI Lecture Slides (c) 2002, C. Boutilier\\n\\nDifficulties for the Classical Model\\n\\nUncertainty\\n\\xe2\\x80\\xa2 in action effects\\n\\xe2\\x80\\xa2 in knowledge of system state\\n\\xe2\\x80\\xa2 a \\xe2\\x80\\x9csequence of actions that guarantees goal \\n\\nachievement\\xe2\\x80\\x9d often does not exist\\n\\nMultiple, competing objectives\\n\\nOngoing processes\\n\\xe2\\x80\\xa2 lack of well-defined termination criteria\\n\\n\\n\\n3\\n\\n\\xef\\xbf\\xbd\\nNASSLI Lecture Slides (c) 2002, C. Boutilier\\n\\nSome Specific Difficulties\\n\\nMaintenance goals: \\xe2\\x80\\x9ckeep lab tidy\\xe2\\x80\\x9d\\n\\xe2\\x80\\xa2 goal is never achieved once and for all\\n\\xe2\\x80\\xa2 can\\xe2\\x80\\x99t be treated as a safety constraint\\n\\nPreempted/Multiple goals: \\xe2\\x80\\x9ccoffee vs. mail\\xe2\\x80\\x9d\\n\\xe2\\x80\\xa2 must address tradeoffs: priorities, risk, etc.\\n\\nAnticipation of Exogenous Events\\n\\xe2\\x80\\xa2 e.g., wait in the mailroom at 10:00 AM\\n\\xe2\\x80\\xa2 on-going processes driven by exogenous events\\n\\nSimilar concerns: logistics, process planning, \\nmedical decision making, etc.\\n\\n\\xef\\xbf\\xbd\\nNASSLI Lecture Slides (c) 2002, C. Boutilier\\n\\nMarkov Decision Processes\\n\\nClassical planning models:\\n\\xe2\\x80\\xa2 logical rep\\xe2\\x80\\x99n s of deterministic transition systems\\n\\xe2\\x80\\xa2 goal-based objectives\\n\\xe2\\x80\\xa2 plans as sequences\\n\\nMarkov decision processes generalize this view\\n\\xe2\\x80\\xa2 controllable, stochastic transition system\\n\\xe2\\x80\\xa2 general objective functions (rewards) that allow \\n\\ntradeoffs with transition probabilities to be made\\n\\xe2\\x80\\xa2 more general solution concepts (policies)\\n\\n\\n\\n4\\n\\n\\xef\\xbf\\xbd\\nNASSLI Lecture Slides (c) 2002, C. Boutilier\\n\\nLogical Representations of MDPs\\n\\nMDPs provide a nice conceptual model\\n\\nClassical representations and solution methods \\ntend to rely on state-space enumeration\\n\\n\\xe2\\x80\\xa2 combinatorial explosion if state given by set of \\npossible worlds/logical interpretations/variable assts\\n\\n\\xe2\\x80\\xa2 Bellman\\xe2\\x80\\x99s curse of dimensionality\\nRecent work has looked at extending AI-style \\nrepresentational and computational methods to \\nMDPs\\n\\n\\xe2\\x80\\xa2 we\\xe2\\x80\\x99ll look at some of these (with a special emphasis \\non \\xe2\\x80\\x9clogical\\xe2\\x80\\x9d methods)\\n\\n\\xef\\xbf\\xbd\\nNASSLI Lecture Slides (c) 2002, C. Boutilier\\n\\nCourse Overview\\n\\nLecture 1\\n\\xe2\\x80\\xa2 motivation\\n\\xe2\\x80\\xa2 introduction to MDPs: classical model and algorithms\\n\\nLecture 2\\n\\xe2\\x80\\xa2 AI/planning-style representations\\n\\xe2\\x80\\xa2 probabilistic STRIPs; dynamic Bayesian networks; \\n\\ndecision trees and BDDs; situation calculus\\n\\xe2\\x80\\xa2 some simple ways to exploit logical structure: \\n\\nabstraction and decomposition\\n\\n\\n\\n5\\n\\n\\xef\\xbf\\xbd\\nNASSLI Lecture Slides (c) 2002, C. Boutilier\\n\\nCourse Overview (con\\xe2\\x80\\x99t)\\n\\nLecture 3\\n\\xe2\\x80\\xa2 decision-theoretic regression\\n\\xe2\\x80\\xa2 propositional view as variable elimination\\n\\xe2\\x80\\xa2 exploiting decision tree/BDD structure\\n\\xe2\\x80\\xa2 approximation\\n\\xe2\\x80\\xa2 first-order DTR with situation calculus\\n\\nLecture 4\\n\\xe2\\x80\\xa2 linear function approximation\\n\\xe2\\x80\\xa2 exploiting logical structure of basis functions\\n\\xe2\\x80\\xa2 discovering basis functions\\n\\n\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\nNASSLI Lecture Slides (c) 2002, C. Boutilier\\n\\nCourse Overview (con\\xe2\\x80\\x99t)\\n\\nLecture 5\\n\\xe2\\x80\\xa2 temporal logic for specifying non-Markovian dynamics\\n\\xe2\\x80\\xa2 model minimization\\n\\xe2\\x80\\xa2 wrap up; further topics\\n\\n\\n\\n6\\n\\n\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\nNASSLI Lecture Slides (c) 2002, C. Boutilier\\n\\nMarkov Decision Processes\\nAn MDP has four components, S, A, R, Pr:\\n\\n\\xe2\\x80\\xa2 (finite) state set S   (|S| = n)\\n\\xe2\\x80\\xa2 (finite) action set A   (|A| = m)\\n\\xe2\\x80\\xa2 transition function Pr(s,a,t)\\n\\n\\xef\\xbf\\xbd each Pr(s,a,-) is a distribution over S\\n\\xef\\xbf\\xbd represented by set of n x n stochastic matrices\\n\\n\\xe2\\x80\\xa2 bounded, real-valued reward function R(s)\\n\\xef\\xbf\\xbd represented by an n-vector\\n\\xef\\xbf\\xbd can be generalized to include action costs: R(s,a)\\n\\xef\\xbf\\xbd can be stochastic (but replacable by expectation)\\n\\nModel easily generalizable to countable or \\ncontinuous state and action spaces\\n\\n\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\nNASSLI Lecture Slides (c) 2002, C. Boutilier\\n\\nSystem Dynamics\\n\\nFinite State Space S\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\t\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\n\\r\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\n\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\n \\xef\\xbf\\xbd \\xef\\xbf\\xbd\\xef\\xbf\\xbd!\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\"#\\n%$\\xef\\xbf\\xbd&(\\')!*\\xef\\xbf\\xbd \\xef\\xbf\\xbd,+ \\xef\\xbf\\xbd\\n- &(\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\' ./!\\xef\\xbf\\xbd\\xef\\xbf\\xbd0\\xef\\xbf\\xbd\\xef\\xbf\\xbd\"1\\n \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd,2\\r2 \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\n34353\\n\\n\\n\\n7\\n\\n\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\nNASSLI Lecture Slides (c) 2002, C. Boutilier\\n\\nSystem Dynamics\\n\\nFinite Action Space A \\xef\\xbf\\xbd \\' \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd + $ \\xef\\xbf\\xbd &(\\')!*\\xef\\xbf\\xbd \\xef\\xbf\\xbd,+ \\xef\\xbf\\xbd\\t\\n\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\n\\xef\\xbf\\xbd \\xef\\xbf\\xbd \\xef\\xbf\\xbd \\xef\\xbf\\xbd - \\xef\\xbf\\xbd,2\\r2 \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\n\\xef\\xbf\\xbd \\xef\\xbf\\xbd \\xef\\xbf\\xbd \\xef\\xbf\\xbd \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\t \\xef\\xbf\\xbd\\xef\\xbf\\xbd& . \\xef\\xbf\\xbd\\xef\\xbf\\xbd&\\n\\xef\\xbf\\xbd\\n\\n\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\nNASSLI Lecture Slides (c) 2002, C. Boutilier\\n\\nSystem Dynamics\\n\\nTransition Probabilities: Pr(si, a, sj)\\n\\n\\xef\\xbf\\xbd & \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\n3\\n\\xef\\xbf\\xbd\\r\\xef\\xbf\\xbd\\n\\n3\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\n\\n\\n\\n8\\n\\n\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\nNASSLI Lecture Slides (c) 2002, C. Boutilier\\n\\nSystem Dynamics\\n\\nTransition Probabilities: Pr(si, a, sk)\\n\\n\\xef\\xbf\\xbd & \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\n3\\n\\xef\\xbf\\xbd\\r\\xef\\xbf\\xbd\\n\\n3\\n\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\n\\n\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\t\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\n\\n\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\r\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\t\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\n\\xef\\xbf\\xbd \\xef\\xbf\\xbd \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\n\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\n\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd \\xef\\xbf\\xbd \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\n\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\n\\n\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\nNASSLI Lecture Slides (c) 2002, C. Boutilier\\n\\nReward Process\\n\\nReward Function: R(si)\\n- action costs possible\\n\\n\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\xef\\xbf\\xbd&(\" \\xef\\xbf\\xbd! \\t\" \\xef\\xbf\\xbd\\n\\n#\\n\\xef\\xbf\\xbd \\xef\\xbf\\xbd%$\\'&\\n\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd%( \\xef\\xbf\\xbd*)\\n\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\n $ (\\n\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\n\\n\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\n\\n\\n\\n9\\n\\n\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\nNASSLI Lecture Slides (c) 2002, C. Boutilier\\n\\nGraphical View of MDP\\n\\n\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\n\\n\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\n\\n\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\n\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\n\\n\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\n\\n\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\n\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\n\\n\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\n\\xef\\xbf\\xbd\\t\\xef\\xbf\\xbd\\n\\n\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\n \\xef\\xbf\\xbd\\n\\n\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\n\\xef\\xbf\\xbd\\t\\xef\\xbf\\xbd\\n\\n\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\nNASSLI Lecture Slides (c) 2002, C. Boutilier\\n\\nAssumptions\\n\\nMarkovian dynamics (history independence)\\n\\xe2\\x80\\xa2 Pr(St+1|At,St,At-1,St-1,..., S0) = Pr(St+1|At,St) \\n\\nMarkovian reward process\\n\\xe2\\x80\\xa2 Pr(Rt|At,St,At-1,St-1,..., S0) = Pr(Rt|At,St)\\n\\nStationary dynamics and reward\\n\\xe2\\x80\\xa2 Pr(St+1|At,St) = Pr(St\\xe2\\x80\\x99+1|At\\xe2\\x80\\x99,St\\xe2\\x80\\x99) for all t, t\\xe2\\x80\\x99\\n\\nFull observability\\n\\xe2\\x80\\xa2 though we can\\xe2\\x80\\x99t predict what state we will reach when \\n\\nwe execute an action, once it is realized, we know \\nwhat it is\\n\\n\\n\\n10\\n\\n\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\nNASSLI Lecture Slides (c) 2002, C. Boutilier\\n\\nPolicies\\n\\nNonstationary policy \\n\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\n\\n:S x T \\xef\\xbf\\xbd A\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\n(s,t) is action to do at state s with t-stages-to-go\\n\\nStationary policy \\n\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\n\\nS \\xef\\xbf\\xbd A\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\n(s) is action to do at state s (regardless of time)\\n\\n\\xe2\\x80\\xa2 analogous to reactive or universal plan\\nThese assume or have these properties:\\n\\n\\xe2\\x80\\xa2 full observability\\n\\xe2\\x80\\xa2 history-independence\\n\\xe2\\x80\\xa2 deterministic action choice\\n\\n\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\nNASSLI Lecture Slides (c) 2002, C. Boutilier\\n\\nValue of a Policy\\n\\nHow good is a policy \\xef\\xbf\\xbd\\n\\xef\\xbf\\xbd\\n\\nHow do we measure \\n\\xe2\\x80\\x9caccumulated\\xe2\\x80\\x9d reward?\\n\\nValue function V: S \\xef\\xbf\\xbd\\t\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\n\\r\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\r\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\r\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\n\\r\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\r\\n\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd \\xef\\xbf\\xbd!\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd#\"\\n\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\n\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\"\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd$\\n\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd (sometimes S x T)\\nV % (s) denotes value of policy at state s\\n\\n\\xe2\\x80\\xa2 how good is it to be at state s? depends on \\nimmediate reward, but also what you achieve \\nsubsequently\\n\\n\\xe2\\x80\\xa2 expected accumulated reward over horizon of interest\\n\\xe2\\x80\\xa2 note V & (s) \\')( (s); it measures utility\\n\\n\\n\\n11\\n\\n\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\nNASSLI Lecture Slides (c) 2002, C. Boutilier\\n\\nValue of a Policy (con\\xe2\\x80\\x99t)\\n\\nCommon formulations of value:\\n\\xe2\\x80\\xa2 Finite horizon n: total expected reward given \\xef\\xbf\\xbd\\n\\xe2\\x80\\xa2 Infinite horizon discounted: discounting keeps total \\n\\nbounded\\n\\xe2\\x80\\xa2 Infinite horizon, average reward per time step\\n\\n\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\nNASSLI Lecture Slides (c) 2002, C. Boutilier\\n\\nFinite Horizon Problems\\n\\nUtility (value) depends on stage-to-go\\n\\xe2\\x80\\xa2 hence so should policy: nonstationary \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd s,k \\xef\\xbf\\xbd\\n\\nis k-stage-to-go value function for \\xef\\xbf\\xbd\\n\\nHere Rt is a random variable denoting reward \\nreceived at stage t\\n\\n)(sV k\\xcf\\x80\\n\\n],|[)(\\n0\\n\\nsREsV\\nk\\n\\nt\\n\\ntk \\xcf\\x80\\xcf\\x80 \\xef\\xbf\\xbd\\n=\\n\\n=\\n\\n\\n\\n12\\n\\n\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\nNASSLI Lecture Slides (c) 2002, C. Boutilier\\n\\nSuccessive Approximation\\n\\nSuccessive approximation algorithm used to \\ncompute             by dynamic programming\\n\\n(a)\\n\\n(b) )\\'(\\n\\'\\n\\n)\\'),,(,Pr()()( 1 s\\ns\\n\\nVsksssRsV kk \\xef\\xbf\\xbd\\n\\xe2\\x88\\x92\\xe2\\x8b\\x85+= \\xcf\\x80\\xcf\\x80 \\xcf\\x80\\n\\n)(sV k\\xcf\\x80\\n\\nssRsV \\xe2\\x88\\x80= ),()(0\\xcf\\x80\\n\\n\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\n\\n\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\n\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\t\\xef\\xbf\\xbd\\n\\n\\n \\xef\\xbf\\xbd\\r\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\n\\n\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\nNASSLI Lecture Slides (c) 2002, C. Boutilier\\n\\nSuccessive Approximation\\n\\nLet  P \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd k  be matrix constructed from rows of \\naction chosen by policy\\n\\nIn matrix form:\\n\\nVk = R + P \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd k Vk-1\\nNotes:\\n\\n\\xe2\\x80\\xa2 \\xef\\xbf\\xbd requires T n-vectors for policy representation\\n\\xe2\\x80\\xa2 requires an n-vector for representation\\n\\xe2\\x80\\xa2 Markov property is critical in this formulation since \\n\\nvalue at s is defined independent of how s was \\nreached\\n\\nkV\\xcf\\x80\\n\\n\\n\\n13\\n\\n\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\nNASSLI Lecture Slides (c) 2002, C. Boutilier\\n\\nValue Iteration (Bellman 1957)\\nMarkov property allows exploitation of DP \\nprinciple for optimal policy construction\\n\\n\\xe2\\x80\\xa2 no need to enumerate |A|Tn possible policies\\nValue Iteration\\n\\n)\\'(\\n\\'\\n\\n)\\',,Pr(max)()( 1 s\\ns\\n\\nVsassRsV kk\\n\\na\\n\\xef\\xbf\\xbd\\n\\n\\xe2\\x88\\x92\\xe2\\x8b\\x85+=\\nssRsV \\xe2\\x88\\x80= ),()(0\\n\\n)\\'(\\n\\'\\n\\n)\\',,Pr(maxarg),(* 1 s\\ns\\n\\nVsasks k\\n\\na\\n\\xef\\xbf\\xbd\\n\\n\\xe2\\x88\\x92\\xe2\\x8b\\x85=\\xcf\\x80\\n\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\t\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\n\\r\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd \\n\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd \\xef\\xbf\\xbd \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd \\n\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\t\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd \\xef\\xbf\\xbd \\xef\\xbf\\xbd \\xef\\xbf\\xbd \\xef\\xbf\\xbd\\n\\n\\xef\\xbf\\xbd \\xef\\xbf\\xbd \\xef\\xbf\\xbd\\t\\xef\\xbf\\xbd!\\xef\\xbf\\xbd\"\\n\\xef\\xbf\\xbd\\xef\\xbf\\xbd$#%\\n\\r\\xef\\xbf\\xbd \\xef\\xbf\\xbd \\xef\\xbf\\xbd \\xef\\xbf\\xbd\\n\\n\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\nNASSLI Lecture Slides (c) 2002, C. Boutilier\\n\\nValue Iteration\\n\\n&(\\'\\t)\\n\\n&(\\'!*\\n&(\\'\\t+\\n&(\\'!,\\n\\ns4\\n\\ns1\\n\\ns3\\n\\ns2\\n\\n- \\xef\\xbf\\xbd \\n \\xef\\xbf\\xbd- \\xef\\xbf\\xbd\\n\\n&(\\'\\t+\\n\\n&(\\'!)\\n\\n&(\\'!*\\n\\n&(\\'\\t,\\n&(\\'!)\\n\\n&(\\'\\t*\\n&(\\'\\t+\\n&(\\'\\t,\\n\\n- \\xef\\xbf\\xbd/. \\xef\\xbf\\xbd- \\xef\\xbf\\xbd/.\\xef\\xbf\\xbd0\\n\\n1 \\xef\\xbf\\xbd32547698 \\xef\\xbf\\xbd\\xef\\xbf\\xbd:<; \\xef\\xbf\\xbd>= 8 1 \\xef\\xbf\\xbd@?A4\\xef\\xbf\\xbd6\\xef\\xbf\\xbd8 \\xef\\xbf\\xbd\\xef\\xbf\\xbd:<;CB =\\n1 \\xef\\xbf\\xbd B 47698 \\xef\\xbf\\xbdD:@; \\xef\\xbf\\xbdE= 8 1 \\xef\\xbf\\xbd@FA4\\xef\\xbf\\xbd6\\xef\\xbf\\xbd8 \\xef\\xbf\\xbdD:@; ?E=\\n\\n4 6 :<;CB =HG5I :@;CB = 8KJML\\xef\\xbf\\xbdNPO\\nQ\\n\\n\\n\\n14\\n\\n\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\nNASSLI Lecture Slides (c) 2002, C. Boutilier\\n\\nValue Iteration\\n\\ns4\\n\\ns1\\n\\ns3\\n\\ns2\\n\\n&(\\'\\t)\\n\\n&(\\'!*\\n&(\\'\\t+\\n&(\\'!,\\n\\n&(\\'!)\\n\\n&(\\'!*\\n&(\\'\\t+\\n&(\\'\\t,\\n\\n&(\\'!)\\n\\n&(\\'\\t*\\n&(\\'\\t+\\n&(\\'\\t,\\n\\n- \\xef\\xbf\\xbd \\n \\xef\\xbf\\xbd- \\xef\\xbf\\xbd- \\xef\\xbf\\xbd/. \\xef\\xbf\\xbd- \\xef\\xbf\\xbd/.\\xef\\xbf\\xbd0\\n\\n\\xce\\xa0\\xce\\xa0\\xce\\xa0\\xce\\xa0\\n6 :<;CB =HG JMLENPO Q\\n\\n\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\nNASSLI Lecture Slides (c) 2002, C. Boutilier\\n\\nValue Iteration\\n\\nNote how DP is used\\n\\xe2\\x80\\xa2 optimal soln to k-1 stage problem can be used without \\n\\nmodification as part of optimal soln to k-stage problem\\n\\nBecause of finite horizon, policy nonstationary\\n\\nIn practice, Bellman backup computed using:\\n\\nas\\ns\\n\\nVsassRsaQ kk \\xe2\\x88\\x80\\xe2\\x8b\\x85+= \\xef\\xbf\\xbd\\n\\xe2\\x88\\x92 ),\\'(\\n\\n\\'\\n)\\',,Pr()(),( 1\\n\\n),(max)( saQsV ka\\nk =\\n\\n\\n\\n15\\n\\n\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\nNASSLI Lecture Slides (c) 2002, C. Boutilier\\n\\nComplexity\\n\\nT iterations\\n\\nAt each iteration |A| computations of n x n matrix \\ntimes n-vector: O(|A|n3)\\n\\nTotal O(T|A|n3)\\n\\nCan exploit sparsity of matrix: O(T|A|n2)\\n\\n\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\nNASSLI Lecture Slides (c) 2002, C. Boutilier\\n\\nSummary\\n\\nResulting policy is optimal\\n\\n\\xe2\\x80\\xa2 convince yourself of this; convince that \\nnonMarkovian, randomized policies not necessary\\n\\nNote: optimal value function is unique, but \\noptimal policy is not\\n\\nkssVsV kk ,,),()(* \\xcf\\x80\\xcf\\x80\\xcf\\x80 \\xe2\\x88\\x80\\xe2\\x89\\xa5\\n\\n\\n\\n16\\n\\n\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\nNASSLI Lecture Slides (c) 2002, C. Boutilier\\n\\nDiscounted Infinite Horizon MDPs\\nTotal reward problematic (usually)\\n\\n\\xe2\\x80\\xa2 many or all policies have infinite expected reward\\n\\xe2\\x80\\xa2 some MDPs (e.g., zero-cost absorbing states) OK\\n\\n\\xe2\\x80\\x9cTrick\\xe2\\x80\\x9d: introduce discount factor 0 \\xef\\xbf\\xbd\\n\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\n\\n\\xe2\\x80\\xa2 future rewards discounted by \\xef\\xbf\\xbd per time step\\n\\nNote:\\n\\nMotivation: economic? failure prob? convenience?\\n\\n],|[)(\\n0\\n\\nsREsV\\nt\\n\\nttk \\xcf\\x80\\xce\\xb2\\xcf\\x80 \\xef\\xbf\\xbd\\n\\xe2\\x88\\x9e\\n\\n=\\n=\\n\\nmax\\n\\n0\\n\\nmax\\n\\n1\\n\\n1\\n][)( RREsV\\n\\nt\\n\\nt\\n\\n\\xce\\xb2\\n\\xce\\xb2\\xcf\\x80 \\xe2\\x88\\x92\\n\\n=\\xe2\\x89\\xa4 \\xef\\xbf\\xbd\\n\\xe2\\x88\\x9e\\n\\n=\\n\\n\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\nNASSLI Lecture Slides (c) 2002, C. Boutilier\\n\\nSome Notes\\n\\nOptimal policy maximizes value at each state\\n\\nOptimal policies guaranteed to exist (Howard60)\\n\\nCan restrict attention to stationary policies\\n\\n\\xe2\\x80\\xa2 why change action at state s at new time t?\\n\\nWe define                            for some optimal \\xef\\xbf\\xbd)()(* sVsV \\xcf\\x80=\\n\\n\\n\\n17\\n\\n\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\nNASSLI Lecture Slides (c) 2002, C. Boutilier\\n\\nValue Equations (Howard 1960)\\n\\nValue equation for fixed policy value\\n\\nBellman equation for optimal value function\\n\\n)\\'(\\n\\'\\n\\n)\\'),(,Pr()()( s\\ns\\n\\nVssssRsV \\xef\\xbf\\xbd \\xe2\\x8b\\x85+= \\xcf\\x80\\xcf\\x80 \\xcf\\x80\\n\\n)\\'(\\n\\'\\n\\n*)\\',,Pr(max)()(* s\\ns\\n\\nVsassRsV\\na\\n\\xef\\xbf\\xbd \\xe2\\x8b\\x85+=\\n\\n\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\nNASSLI Lecture Slides (c) 2002, C. Boutilier\\n\\nBackup Operators\\n\\nWe can think of the fixed policy equation and the \\nBellman equation as operators in a vector space\\n\\n\\xe2\\x80\\xa2 e.g., La(V) = V\\xe2\\x80\\x99 = R + \\xef\\xbf\\xbd PaV\\n\\xe2\\x80\\xa2 V\\xef\\xbf\\xbd is unique fixed point of policy backup operator L \\xef\\xbf\\xbd\\n\\xe2\\x80\\xa2 V* is unique fixed point of Bellman backup L*\\n\\nWe can compute V \\xef\\xbf\\xbd easily: policy evaluation\\n\\xe2\\x80\\xa2 simple linear system with n variables, n constraints\\n\\xe2\\x80\\xa2 solve V = R + \\xef\\xbf\\xbd PV\\n\\nCannot do this for optimal policy\\n\\xe2\\x80\\xa2 max operator makes things nonlinear\\n\\n\\n\\n18\\n\\n\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\nNASSLI Lecture Slides (c) 2002, C. Boutilier\\n\\nValue Iteration\\nCan compute optimal policy using value iteration, \\njust like FH problems (just include discount term)\\n\\n\\xe2\\x80\\xa2 no need to store argmax at each stage (stationary)\\n\\n)\\'(\\n\\'\\n\\n)\\',,Pr(max)()( 1 s\\ns\\n\\nVsassRsV kk\\n\\na\\n\\xef\\xbf\\xbd\\n\\n\\xe2\\x88\\x92\\xe2\\x8b\\x85+= \\xce\\xb2\\n\\n\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\nNASSLI Lecture Slides (c) 2002, C. Boutilier\\n\\nConvergence\\n\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\n\\nis a contraction mapping in Rn\\n\\n\\xe2\\x80\\xa2 || LV \\xe2\\x80\\x93 LV\\xe2\\x80\\x99 || \\xef\\xbf\\xbd\\t\\xef\\xbf\\xbd || V \\xe2\\x80\\x93 V\\xe2\\x80\\x99 || \\nWhen to stop value iteration?  when ||Vk - Vk-1|| \\n\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\n\\n\\xe2\\x80\\xa2 ||Vk+1 - Vk|| \\xef\\xbf\\xbd\\t\\xef\\xbf\\xbd ||Vk - Vk-1|| \\n\\xe2\\x80\\xa2 this ensures ||Vk \\xe2\\x80\\x93 V*|| \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\r\\xef\\xbf\\xbd\\xef\\xbf\\xbd /1- \\xef\\xbf\\xbd\\n\\nConvergence is assured\\n\\xe2\\x80\\xa2 any guess V:   || V* - L*V || = ||L*V* - L*V || \\xef\\xbf\\xbd\\t\\xef\\xbf\\xbd || V* - V || \\n\\xe2\\x80\\xa2 so fixed point theorems ensure convergence\\n\\n\\n\\n19\\n\\n\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\nNASSLI Lecture Slides (c) 2002, C. Boutilier\\n\\nHow to Act\\n\\nGiven V* (or approximation), use greedy policy:\\n\\n\\xe2\\x80\\xa2 if V within  \\r of V*, then V( \\xef\\xbf\\xbd ) within  2 \\r of V*\\nThere exists an \\xef\\xbf\\xbd s.t. optimal policy is returned\\n\\n\\xe2\\x80\\xa2 even if value estimate is off, greedy policy is optimal\\n\\xe2\\x80\\xa2 proving you are optimal can be difficult (methods like \\n\\naction elimination can be used)\\n\\n)\\'(\\n\\'\\n\\n*)\\',,Pr(maxarg)(* s\\ns\\n\\nVsass\\na\\n\\n\\xef\\xbf\\xbd \\xe2\\x8b\\x85=\\xcf\\x80\\n\\n\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\nNASSLI Lecture Slides (c) 2002, C. Boutilier\\n\\nPolicy Iteration\\n\\nGiven fixed policy, can compute its value exactly:\\n\\nPolicy iteration exploits this\\n\\n)\\'(\\n\\'\\n\\n)\\'),(,Pr()()( s\\ns\\n\\nVssssRsV \\xef\\xbf\\xbd \\xe2\\x8b\\x85+= \\xcf\\x80\\xcf\\x80 \\xcf\\x80\\xce\\xb2\\n\\n$ \\xef\\xbf\\xbd \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd \\xef\\xbf\\xbd \\xef\\xbf\\xbdH\\n\\xef\\xbf\\xbd\\xef\\xbf\\xbd>\\n\\r\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd \\xef\\xbf\\xbd \\xef\\xbf\\xbd \\xef\\xbf\\xbdE\\xef\\xbf\\xbd \\xef\\xbf\\xbd!\\xef\\xbf\\xbd\\t\\xef\\xbf\\xbd\\t\\xef\\xbf\\xbd \\n& \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\n \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\r\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\n\\xef\\xbf\\xbd \\n \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\r \\xef\\xbf\\xbd \\n\\r\\xef\\xbf\\xbd!\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\n \\xef\\xbf\\xbd \\xef\\xbf\\xbd \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\n\\xef\\xbf\\xbd # \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbdK\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbdE\\n\\xef\\xbf\\xbd\\xef\\xbf\\xbd \\xef\\xbf\\xbd \\xef\\xbf\\xbd \\xef\\xbf\\xbd!\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd \\xef\\xbf\\xbd \\xef\\xbf\\xbd \\xef\\xbf\\xbd \\xef\\xbf\\xbd\\n\\xef\\xbf\\xbd \\xef\\xbf\\xbd \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd \\xef\\xbf\\xbd \\xef\\xbf\\xbd \\xef\\xbf\\xbd \\n\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd \\n\\xef\\xbf\\xbd\\xef\\xbf\\xbd \\xef\\xbf\\xbd \\xef\\xbf\\xbd \\xef\\xbf\\xbd \\n\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\n\\n\\xef\\xbf\\xbd \\xef\\xbf\\xbd \\xef\\xbf\\xbd \\xef\\xbf\\xbd\\t\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd \\xef\\xbf\\xbd \\xef\\xbf\\xbd\\t\\xef\\xbf\\xbd \\xef\\xbf\\xbd \\xef\\xbf\\xbd \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\t\\xef\\xbf\\xbd/\\xef\\xbf\\xbd \\n\\xef\\xbf\\xbd\\xef\\xbf\\xbd \\xef\\xbf\\xbd \\xef\\xbf\\xbd \\xef\\xbf\\xbd \\xef\\xbf\\xbd \\xef\\xbf\\xbdE\\xef\\xbf\\xbd \\xef\\xbf\\xbd \\xef\\xbf\\xbd \\xef\\xbf\\xbd\\t#7\\xef\\xbf\\xbd\\t\\xef\\xbf\\xbdH\\n \\xef\\xbf\\xbd \\n\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd \\n \\xef\\xbf\\xbd \\xef\\xbf\\xbd\\n)\\'(\\n\\n\\'\\n)\\',,Pr(maxarg)(\\' s\\n\\ns\\nVsass\\n\\na\\n\\xef\\xbf\\xbd \\xe2\\x8b\\x85= \\xcf\\x80\\xcf\\x80\\n\\n\\n\\n20\\n\\n\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\nNASSLI Lecture Slides (c) 2002, C. Boutilier\\n\\nPolicy Iteration Notes\\n\\nConvergence assured (Howard)\\n\\xe2\\x80\\xa2 intuitively: no local maxima in value space, and each \\n\\npolicy must improve value; since finite number of \\npolicies, will converge to optimal policy\\n\\nVery flexible algorithm\\n\\xe2\\x80\\xa2 need only improve policy at one state (not each state)\\n\\nGives exact value of optimal policy\\n\\nGenerally converges much faster than VI\\n\\xe2\\x80\\xa2 each iteration more complex, but fewer iterations\\n\\xe2\\x80\\xa2 quadratic rather than linear rate of convergence\\n\\n\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\nNASSLI Lecture Slides (c) 2002, C. Boutilier\\n\\nModified Policy Iteration\\n\\nMPI a flexible alternative to VI and PI\\n\\nRun PI, but don\\xe2\\x80\\x99t solve linear system to evaluate \\npolicy; instead do several iterations of successive \\napproximation to evaluate policy\\n\\nYou can run SA until near convergence\\n\\xe2\\x80\\xa2 but in practice, you often only need a few backups to \\n\\nget estimate of V( \\xef\\xbf\\xbd ) to allow improvement in \\xef\\xbf\\xbd\\n\\n\\xe2\\x80\\xa2 quite efficient in practice\\n\\xe2\\x80\\xa2 choosing number of SA steps a practical issue\\n\\n\\n\\n21\\n\\n\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\nNASSLI Lecture Slides (c) 2002, C. Boutilier\\n\\nAsynchronous Value Iteration\\nNeedn\\xe2\\x80\\x99t do full backups of VF when running VI\\nGauss-Siedel: Start with Vk .Once you compute  \\nVk+1(s), you replace Vk(s) before proceeding to \\nthe next state (assume some ordering of states)\\n\\n\\xe2\\x80\\xa2 tends to converge much more quickly\\n\\xe2\\x80\\xa2 note:  Vk no longer k-stage-to-go VF\\n\\nAVI: set some V0; Choose random state s and do \\na Bellman backup at that state alone to produce \\nV1; Choose random state s\\xe2\\x80\\xa6\\n\\n\\xe2\\x80\\xa2 if each state backed up frequently enough, \\nconvergence assured\\n\\n\\xe2\\x80\\xa2 useful for online algorithms (reinforcement learning)\\n\\n\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\nNASSLI Lecture Slides (c) 2002, C. Boutilier\\n\\nSome Remarks on Search Trees\\n\\nAnalogy of Value Iteration to decision trees\\n\\xe2\\x80\\xa2 decision tree (expectimax search) is really value \\n\\niteration with computation focussed on reachable \\nstates\\n\\nReal-time Dynamic Programming (RTDP)\\n\\xe2\\x80\\xa2 simply real-time search applied to MDPs\\n\\xe2\\x80\\xa2 can exploit heuristic estimates of value function\\n\\xe2\\x80\\xa2 can bound search depth using discount factor\\n\\xe2\\x80\\xa2 can cache/learn values\\n\\xe2\\x80\\xa2 can use pruning techniques\\n\\n\\n\\n22\\n\\n\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\nNASSLI Lecture Slides (c) 2002, C. Boutilier\\n\\nReferences\\n\\n\\xef\\xbf\\xbd\\nM. L. Puterman, Markov Decision Processes: Discrete Stochastic \\nDynamic Programming, Wiley, 1994.\\n\\xef\\xbf\\xbd\\n\\nD. P. Bertsekas, Dynamic Programming: Deterministic and \\nStochastic Models, Prentice-Hall, 1987.\\n\\xef\\xbf\\xbd\\n\\nR. Bellman, Dynamic Programming, Princeton, 1957.\\n\\xef\\xbf\\xbd\\n\\nR. Howard, Dynamic Programming and Markov Processes, MIT \\nPress, 1960.\\n\\xef\\xbf\\xbd\\n\\nC. Boutilier, T. Dean, S. Hanks, Decision Theoretic Planning: \\nStructural Assumptions and Computational Leverage, Journal of Artif. \\nIntelligence Research 11:1-94, 1999.\\n\\xef\\xbf\\xbd\\n\\nA. Barto, S. Bradke, S. Singh, Learning to Act using Real-Time \\nDynamic Programming, Artif. Intelligence 72(1-2):81-138, 1995.\\n\\n\\n'