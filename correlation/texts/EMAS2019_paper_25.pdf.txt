b'\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIncorporating social practices in BDI agent systems\\n\\nStephen Cranefield1 and Frank Dignum2,3,4\\n\\n1 University of Otago, Dunedin, New Zealand, stephen.cranefield@otago.ac.nz\\n2 Umea\\xcc\\x8a University, Umea\\xcc\\x8a, Sweden, frank.dignum@umu.se\\n3 Czech University of Technology in Prague, Czech Republic,\\n\\nfrank.dignum@aic.fel.cvut.cz\\n4 Utrecht University, The Netherlands, f.p.m.dignum@uu.nl\\n\\nAbstract. When agents interact with humans, either through embodied agents\\nor because they are embedded in a robot, it would be easy if they could use fixed\\ninteraction protocols as they do with other agents. However, people do not keep\\nfixed protocols in their day-to-day interactions and the environments are often\\ndynamic, making it impossible to use fixed protocols. Deliberating about interac-\\ntions from fundamentals is not very scalable either, because in that case all possi-\\nble reactions of a human have to be considered in the plans. In this paper we argue\\nthat social practices can be used as an inspiration for designing flexible and scal-\\nable interaction mechanisms that are also robust. However, using social practices\\nrequires extending the traditional BDI deliberation cycle to monitor landmark\\nstates and perform expected actions by leveraging existing plans. We define and\\nimplement this mechanism in Jason using a periodically run meta-deliberation\\nplan, supported by a metainterpreter, and illustrate its use in a realistic scenario.\\n\\n1 Introduction\\n\\nImagine the scenario where a disabled person, living alone, is assisted by a care robot.\\nThe robot makes sure that the person gets up every morning and that he drinks some\\ncoffee and takes his morning pills (if needed). Then they read the newspaper together,\\nwhich means that the person looks at the pictures in the paper and the robot reads the\\narticles out loud for the person to hear.\\n\\nWhen agents in the role of this type of personal assistant or care robot have to\\ninteract with humans over a longer time period and in a dynamic environment (that is\\nnot controlled by the agent), the interaction management becomes very difficult. When\\nfixed protocols are used for the interaction they are often not appropriate in all situations\\nand cause breakdowns and consequent loss of trust in the system. However, to have real-\\ntime deliberation about the best response during the interaction is not very scalable,\\nbecause in real life the contexts are dynamic and complex and thus the agent would\\nneed to take many parameters into consideration at each step. Thus we need something\\nin between a completely scripted interaction that is too brittle and a completely open\\ninteraction that is not scalable.\\n\\nAs we have done before in the agent community, we take inspiration from human in-\\nteractions and the way they are managed by individuals. Humans classify situations into\\nstandard contexts in which a certain social practice can be applied. Social science has\\n\\nlouisedennis\\nPlaced Image\\n\\n\\n\\n2 Stephen Cranefield and Frank Dignum\\n\\nstudied this phenomenon in social practice theory. Social practice theory comes forth\\nfrom a variety of different sub-disciplines of social science. It started from philosoph-\\nical sociology with proponents like Bourdieu [3] and Giddens [8]. Later on Reckwitz\\n[16] and Shove [18] have expanded on these ideas, and also Schatzki [17] made some\\nvaluable contributions.\\n\\nThese authors all claim that important features of human life should be understood\\nin terms of organized constellations of interacting persons, which together constitute\\nsocial practices. People are not just creating these practices, but our deliberations are\\nalso based on the fact that most of our life is shaped by social practices. Thus, we use\\nsocial practices to categorize situations and decide upon ways of behaviour based on\\nsocial practices. The main intuition behind this is that our life is quite cyclic, in that\\nmany activities come back with a certain regularity. We have meals every day, go to\\nwork on Monday until Friday, go to the supermarket once a week, etc. These so-called\\nPatterns of Life [7] can be exploited to create standard situations and expectations. It\\nmakes sense to categorize recurrent situations as social practices with a kind of standard\\nbehaviour for each of them.\\n\\nUnfortunately social practice theory has not been widely used in computer science\\nor in HCI and thus there are no ready-to-use tools in order to incorporate them in agents.\\nIt is clear from the above description that social practices are more than just a proto-\\ncol or a frame to be used by the agent in its deliberation. Therefore, in this paper we\\nmake the following contributions. We propose a mechanism for BDI agents to maintain\\nawareness about active social practices, and to leverage their existing plans to act in ac-\\ncordance with these practices. This is presented as a meta-deliberation plan that can be\\ndirectly executed by Jason agents, or treated as a specification for an optimised imple-\\nmentation in an extended agent platform. This plan has been deployed in the (simulated)\\ncare robot scenario, to confirm that awareness of and adherence to a social practice en-\\nables the robot to have a more successful interaction with the patient over a longer\\nperiod of time. As some of the features needed to implement this scenario, and to sup-\\nport our meta-deliberation plan, are not currently available in Jason, we also present a\\nJason metainterpreter, which provides this extended functionality, but can also be used\\nindependently to support other research on extensions to BDI practical reasoning.\\n\\nIn the next section, we give an introduction to the purpose and structure of social\\npractices. In Section 3, we elaborate on the care robot scenario and how we have mod-\\nelled it in Jason. In Section 4, we describe the role of social practices in this scenario,\\nand discuss the requirements this imposes for a BDI agent. In Section 5, we present\\nour mechanism for extending Jason to leverage social practices, and the metainterpreter\\nneeded to support this. We finish the paper with some conclusions and suggestions for\\nfuture work.\\n\\n2 Social Practices\\n\\nSocial practices are defined as accepted ways of doing things, contextual and materially\\nmediated, that are shared between actors and routinized over time [16]. They can be\\nseen as patterns which can be filled in by a multitude of single and often unique actions.\\n\\n\\n\\nIncorporating social practices in BDI agent systems 3\\n\\nThrough (joint) performance, the patterns provided by the practice are filled out and\\nreproduced.\\n\\nAccording to [16, 18] a social practice consists of three parts:\\n\\n\\xe2\\x80\\x93 Material: covers all physical aspects of the performance of a practice, including the\\nhuman body and objects that are available (relates to physical aspects of a context).\\n\\n\\xe2\\x80\\x93 Meaning: refers to the issues which are considered to be relevant with respect to\\nthat material, i.e. understandings, beliefs and emotions (relates to social aspects of\\na situation).\\n\\n\\xe2\\x80\\x93 Competence: refers to skills and knowledge which are required to perform the prac-\\ntice (relates to the notion of deliberation about a situation).\\n\\nWhereas the first and third parts intuitively can be made more precise for an imple-\\nmentation, the second part is rather vague. Let us consider these three parts of a social\\npractice in the scenario of the care robot scenario introduced in Section 1. The mate-\\nrial refers to the room where the robot serves morning coffee for the disabled person.\\nIt includes the materials that are needed to make coffee (such as coffee and a coffee\\nmaker) and serve it (such as a cup and tray). However, it also includes the table and\\nother furniture in the room, the newspaper (if present), the TV, radio, computer, tablet,\\nand the robot and person (and possible other people that are present).\\n\\nThe competence part describes what activities every party can perform and expec-\\ntations about what they will actually do. For example, the robot is capable of making\\ncoffee and serving it. The person can drink his coffee by himself. They can jointly read\\nthe newspaper or watch TV. The expectation is that the robot wakes the person if he is\\nnot awake yet, makes the coffee and gives it to the person. After that they will read the\\nnewspaper together to provide mental stimulation. Note, these are expectations, not a\\nprotocol. So, parties can deviate from it and they can also fill the parts in, in ways they\\nsee fit best. The meaning part has to do with all the social interpretations that come with\\nthe social practice, e.g. drinking coffee in the morning might give the person a sense of\\nwell-being that he can use to face the challenges of the rest of the day. When the coffee\\nis cold or weak the person might interpret it as disinterest on the part of the robot in\\nhis well-being. The goal of reading the newspaper might also be not just to get the in-\\nformation from it, but a form of entertainment and feeling related to the robot, because\\nyou do something together.\\n\\nFrom the above description it can already be seen that social practices are more en-\\ncompassing than conventions and norms. Conventions focus on the strategic advantage\\nthat an individual gets by conforming to the convention. The reason to follow a conven-\\ntion is that if all parties involved comply, a kind of optimal coordination is reached, i.e. if\\nwe all drive on the left side of the road, traffic will be smoother than when everyone\\nchooses the side to drive on freely. Thus, conventions focus on the actual actions being\\nperformed and how they optimize the coordination. Social practices focus on common\\nexpectations and ways to achieve them. For example, if we go to a presentation, we sit\\ndown as soon as we see chairs standing in rows in the room. However, we could also\\nkeep standing (as is often done outside).\\n\\nSocial practices are also different from norms. Norms usually dictate a very specific\\nbehaviour rather than creating a set of loosely coupled expectations as is the case for\\nsocial practices. E.g. if the norm states that a car has to stop for a red light, it gives a\\n\\n\\n\\n4 Stephen Cranefield and Frank Dignum\\n\\nvery specific directive. If a norm is more abstract (like \\xe2\\x80\\x9cdrive carefully\\xe2\\x80\\x9d) then we need\\nto translate this into concrete norms for specific situations.\\n\\nOne framework that seems very close to social practices is the notion of scripts.\\nHowever, social practices are not just mere scripts in the sense of Minsky [14]. Practices\\nare more flexible than the classical frames defined by scripts in that they can be extended\\nand changed by learning, and the \\xe2\\x80\\x9cslots\\xe2\\x80\\x9d only need to be filled in as far as they are\\nneeded to determine a course of action. Using these structures changes planning in many\\ncommon situations to pattern recognition and filling in parameters. They support, rather\\nthan restrict, deliberation about behaviour. For example, the social practice of \\xe2\\x80\\x9cgoing\\nto work\\xe2\\x80\\x9d incorporates usual means of transport that can be used, timing constraints,\\nweather and traffic conditions, etc. So, normally you take a car to work, but if the\\nweather is exceptionally bad, the social practice does not force the default action, but\\nrather gives input for deliberation about a new plan in this situation, such as taking\\na bus or train (or even staying home). So, social practices can be seen as a kind of\\nflexible script. Moreover, scripts do not incorporate any social meaning for the activities\\nperformed in them as social practices do.\\n\\nSocial practices have been used in applications already in a variety of ways. In [15,\\n12] they have been used as part of social simulations. In those applications, social prac-\\ntices are used as a standard package of actions with a special status. Thus individuals\\ncan use them with a certain probability given the circumstances are right. However,\\nthese applications do not use the internal structure of social practices for the planning\\nof the individuals. Social practices have been used for applications in natural language\\nand dialogue management in [1, 9]. Here, the social practices are used to guide the plan-\\nning process, but are geared towards a particular dialogue rather than as part of a more\\ngeneral interaction. In [13] it is shown how social practices can be used by a traditional\\nepistemic multi-agent planner to provide efficient and robust plans in cooperative set-\\ntings. However, in this case the planner was not part of a BDI agent with its own goals\\nand plans, but completely dedicated to finding a plan for the situation at hand. In [6]\\na first structure of social practices was presented that is more amenable for the use by\\nagents. The paper is, unfortunately, only conceptual and no implementation was made\\nyet. In this paper we will follow the structure described in [6]. However, we mainly\\nconcentrate on the plan patterns that are a core part of the social practices and show\\nhow they work with BDI agents in the Jason platform.\\n\\nThe complete structure for social practices (based on [6]) is as follows:\\n\\nContext\\n\\n\\xe2\\x80\\x93 Roles describe the competencies and expectations about a certain type of actor.\\nThus the robot is expected to be able to make a cup of coffee.\\n\\n\\xe2\\x80\\x93 Actors are all people and autonomous systems involved, that have capability to\\nreason and (inter)act. This indicates the agents that are expected to fulfil a part in\\nthe practice. In our scenario, these are the robot and the person.\\n\\n\\xe2\\x80\\x93 Resources are objects that are used by the actions in the practice, such as cups,\\ncoffee, trays, curtains, and chairs. So, they are assumed to be available both for\\nstandard actions and for the planning within the practice.\\n\\n\\n\\nIncorporating social practices in BDI agent systems 5\\n\\n\\xe2\\x80\\x93 Affordances are the properties of the context that permit social actions and depend\\non the match between context conditions and actor characteristics. For example,\\nthe bed might be used as a chair, or a mug as a cup.\\n\\n\\xe2\\x80\\x93 Places indicates where all objects and actors are usually located relatively to each\\nother, in space or time: the cups are in the cupboard in the kitchen, the person is in\\nthe chair (or in bed), etc.\\n\\nMeaning\\n\\xe2\\x80\\x93 Purpose determines the social interpretation of actions and of certain physical sit-\\n\\nuations. For example, the purpose of reading the newspaper is to get information\\nabout current affairs and to entertain the person.\\n\\n\\xe2\\x80\\x93 Promotes indicates the values that are promoted (or demoted, by promoting the\\nopposite) by the social practice. Giving coffee to the person will promote the value\\nof \\xe2\\x80\\x9ccaring\\xe2\\x80\\x9d.\\n\\n\\xe2\\x80\\x93 Counts-as are rules of the type \\xe2\\x80\\x9cX counts as Y in C\\xe2\\x80\\x9d linking brute facts (X) and\\ninstitutional facts (Y) in the context (C). For example, reading the newspaper with\\nthe person counts as entertaining the person.\\n\\nExpectations\\n\\xe2\\x80\\x93 Plan patterns describe usual patterns of actions defined by the landmarks that are\\n\\nexpected to occur (states of affairs around which the inter-agent coordination is\\nstructured). For example, the care robot first checks if the person is awake then\\nmakes sure there is coffee served.\\n\\n\\xe2\\x80\\x93 Norms describe the rules of (expected) behaviour within the practice. E.g., the robot\\nshould ask the person if he wants coffee, before starting to make it.\\n\\n\\xe2\\x80\\x93 Strategies indicate condition-action pairs that can occur at any time during the prac-\\ntice. For example, if the person drops the coffee, the robot will clean it up. If the\\nrobot notices the person is asleep (again) it will try to wake him.\\n\\n\\xe2\\x80\\x93 A Start condition, or trigger, indicates how the social practice starts, e.g. the prac-\\ntice of having morning coffee starts at 8 am.\\n\\n\\xe2\\x80\\x93 A Duration, or End condition, indicates how the social practice ends, e.g., the morn-\\ning routine takes around 45 minutes and ends when the newspaper is read and the\\ncoffee is finished.\\n\\nActivities\\n\\xe2\\x80\\x93 Possible actions describes the expected actions of actors in the social practice, e.g.\\n\\nmaking coffee, reading the newspaper, and opening curtains.\\n\\xe2\\x80\\x93 Requirements indicate the type of capabilities or competences that the agent is ex-\\n\\npected to have in order to perform the activities within this practice. For example,\\nthe robot is expected to know how to make coffee and read the newspaper.\\n\\nIn [5] there is a first formalization of all these aspects based on dynamic logic. Due\\nto space limitations we will not include this formalization here, but just discuss a few\\npoints that are important for the current implementation of social practices in Jason.\\nThe core element of the social practice for an agent is the plan pattern, which gives it\\nhandles to plan its behaviour. Plan patterns are parallel, choice or sequential combina-\\ntions of plan parts expressed as \\xce\\xb3\\xcf\\x86. These plan parts stand for all possible sequence of\\n\\n\\n\\n6 Stephen Cranefield and Frank Dignum\\n\\nactions \\xce\\xb3 that contain actions contributing towards the achievement of \\xcf\\x86 (starting from\\na particular situation). \\xcf\\x86 is the purpose of that part of the practice. There can be more\\neffects, but they are not all specified. So, in our morning routine practice the plan pattern\\ncan be defined as \\xce\\xb31\\xcf\\x861; (\\xce\\xb32\\xcf\\x862&\\xce\\xb33\\xcf\\x863); \\xce\\xb34\\xcf\\x864, where \\xcf\\x861 denotes the person being awake,\\n\\xcf\\x862 denotes the coffee being served, \\xcf\\x863 denotes the pills being taken, and \\xcf\\x864 denotes the\\nperson being mentally stimulated.\\n\\nThus, the purpose of the first part of the morning routine is that the person is awake.\\nThis might be done by opening the curtains, giving some loud noise or otherwise. If\\nthe purpose is achieved by opening the curtains, not only is the person awake, but the\\ncurtains are also open. The latter is merely a side effect of achieving the purpose.\\n\\nTwo more things should be noted about these patterns. One is that the overall pat-\\ntern is supposed to achieve the overall purpose of the social practice. This is a formal\\nconstraint, but we only treat this implicitly. The other is that after a part of the plan pat-\\ntern is finished, it automatically triggers the start of the next part of the pattern. In the\\nfull formalism this is assured, but is not explicit from only this fragment. In the same\\nway, a social practice is started when the start condition becomes true. It then becomes\\navailable for execution and can be used by any agent present in the situation.\\n\\nFinally, the formalism of social practices also guarantees that there is a common\\nbelief in the elements of the social practice and if actions are taken everyone has at least\\na common belief about the effects in as far as they are important for the social practice.\\nThus it guarantees a common situation awareness.\\n\\n3 The care robot scenario\\n\\nIn this section we elaborate on the care robot scenario outlined in the introduction, and\\ndescribe how we have modelled and implemented it using Jason.\\n\\nWe assume the high-level operation of the robot is based on a BDI interpreter, and\\nthat it comes equipped with goals and plans to trigger and enact its care activities (most\\nlikely with some customisation of key parameters possible). In this section we consider\\nonly a small subset of the robot\\xe2\\x80\\x99s duties: to wake the patient at a certain time in the\\nmorning, to provide coffee as required, and to provide mental stimulation. We do not\\nspecify any goals of the robot outside the practice here, but normally the care robot\\nwould also have its own goals such as powering its battery, (vacuum) cleaning a room\\nand taking care of the health of the patient.\\n\\nSocial practices provide patterns of coordination for multiple agents in terms of\\nlandmark states rather than explicit sequences of actions. Therefore they do not make\\nlimiting assumptions about the temporal aspects of actions and their effects leading up\\nto a landmark. Only the landmarks themselves are explicitly temporally ordered. The\\nlandmarks are usually very naturally given by the people involved. They will describe\\na social practice in terms of the phases of which it consists and use the landmarks to\\ndenote fixed points that have to be reached before the next phase can start, e.g. being up\\nand in the living room, before drinking tea. To illustrate this we include some temporal\\ncomplexity in the scenario by including durative actions (i.e. those that take place across\\nan interval of time), an action with a delayed effect, and a joint durative action, which\\nhas its desired effect only if two participants perform it during overlapping time inter-\\n\\n\\n\\nIncorporating social practices in BDI agent systems 7\\n\\n1 /* Initial beliefs and rules */\\n2 durative(makePodCoffee).\\n3 durative(readNewspaper).\\n4 joint(readNewspaper).\\n5 durative_action_continuation_pred(readNewspaper, continueReadingNewspaper).\\n6 durative_action_continuation_pred(makePodCoffee, continueMakingPodCoffee).\\n7 continueReadingNewspaper :-\\n8 started(readNewspaper, T1) &\\n9 not started_durative_action(readNewspaper, patient, _) &\\n\\n10 time(T2) &\\n11 T2 <= T1 + 20.\\n12 continueReadingNewspaper :-\\n13 started_durative_action(readNewspaper, patient, _) &\\n14 not stopped_durative_action(readNewspaper, patient, _).\\n15\\n\\n16 continueMakingPodCoffee :- state(coffee, not_made).\\n17\\n\\n18 wake_up_phrase(\"Good morning sleepyhead!\").\\n19\\n\\n20 /* Plans */\\n21 {begin ebdg(state(patient,awake))}\\n22 +!state(patient,awake) : wake_up_phrase(P) <- talkToPatient(P).\\n23 +!state(patient,awake) <- shakePatient.\\n24 +!state(patient,awake) <- openCurtains; .wait(state(patient, awake), 30000).\\n25 {end}\\n26\\n\\n27 {begin ebdg(state(patient,mentally_stimulated))}\\n28 +!state(patient, mentally_stimulated) <- .wait(state(patient, awake)); .fail.\\n29 +!state(patient, mentally_stimulated) <- play_mozart.\\n30 +!state(patient, mentally_stimulated) <-\\n31 !solve([body_term(\\n32 \"\", readNewspaper[participants([patient,robot])])]).\\n33 {end}\\n34\\n\\n35 +!state(coffee, served) <- !state(coffee, made); serveCoffee.\\n36\\n\\n37 {begin ebdg(state(coffee,made))}\\n38 +!state(coffee, made) : resource(coffee_pods) & resource(coffee_pod_machine) <-\\n39 makePodCoffee; .wait(state(coffee, made), 10000).\\n40 +!state(coffee, made) : resource(instant_coffee) <- makeInstantCoffee.\\n41 {end}\\n42\\n\\n43 { include(\"metainterpreter.asl\") }\\n\\nListing 1: Plans for the care robot domain\\n\\nvals. Durative and joint actions are implemented using a Jason metainterpreter5 that is\\ndescribed in Section 5. To simulate the passing of time, we use a \\xe2\\x80\\x9cticker\\xe2\\x80\\x9d agent with\\na recursive plan that periodically performs a tick action to update the time recorded in\\nthe environment. We use Jason\\xe2\\x80\\x99s synchronous execution mode, so the robot, patient and\\nticker agents perform a single reasoning cycle in every step of the simulation.\\n\\nListing 1 shows the robot\\xe2\\x80\\x99s initial beliefs, rules and plans. It has four sets of plans\\n(lines 21 onwards). These have declarative goals (i.e. their triggering goals express\\ndesired states) and use Jason preprocessing directives to transform them according to a\\npredefined declarative achievement goal pattern [2].\\n\\nThe first set of plans (lines 21\\xe2\\x80\\x9325) are for achieving a state where the patient is\\nawake, with alternative plans for talking to the patient, shaking him, and opening the\\ncurtains and waiting for the light to wake him. The exclusive backtracking declarative\\n\\n5 A metainterpreter is a programming language interpreter written in the same, or a similar, lan-\\nguage to the one being interpreted. It can be used to prototype extensions to the base language.\\n\\n\\n\\n8 Stephen Cranefield and Frank Dignum\\n\\ngoal (\\xe2\\x80\\x9cebdg\\xe2\\x80\\x9d) pattern specifies that additional failure-handling logic should be added to\\nensure that all the plans will be tried (once each) until the goal is achieved, or all plans\\nfail. Opening the curtains has a delayed effect: it will eventually wake the patient6.\\n\\nThe second set of plans (lines 27\\xe2\\x80\\x9333) are used for the goal of having the patient\\nmentally simulated, and also use the ebdg pattern. The first plan waits for the patient to\\nbe awake, and then fails so that the other plans will be tried. The other two alternatives\\ninvolve playing the music of Mozart to the patient, and initiating the joint action of read-\\ning the newspaper with the patient. As joint actions are not directly supported by Jason,\\nlines 31\\xe2\\x80\\x9332 call this action via the solve goal that is handled by our metainterpreter.\\n\\nThese plans are followed by a single plan for serving coffee. This has the subgoal\\nof having the coffee made, and then the action of serving the coffee is performed.\\n\\nThe final set of plans are for reaching a state in which the coffee is made. The\\noptions are to use a coffee pod and wait for it to finish, or to make instant coffee.\\n\\nThe initial segment of the listing contains initial beliefs and rules related to the\\nprocessing of durative actions: declarations of which actions are declarative and/or joint,\\nand predicates and associated rules defining the circumstances in which the robot will\\ncontinue performing the durative actions.\\n\\nThe environment sends a percept to all participants of a joint action when any other\\nparticipant performs the action for the first time or performs a stop action with the joint\\naction as an argument. The patient agent has a plan to take his pills once he is awake.\\nHe also has a plan that will respond to the robot beginning the joint newspaper reading\\naction by also beginning that action. He will continue reading the newspaper for 40\\ntime units if he is in a good mood, but only 20 if he is in a bad mood. Being woken by\\ndaylight (after the curtains are opened) leaves him in a good mood; being shaken awake\\nleaves him in a bad mood, and talking will not wake him up. Thus, if the robot begins\\nwith goals to have the patient awake and mentally stimulated, the patient will be left in\\na bad mood by being shaken awake and the newspaper reading will be shorter (and less\\nstimulating) that if he were in a good mood.\\n\\n4 A care robot with social practices\\n\\nSection 3 introduced the care robot scenario. In this section, we consider how the robot\\ncould be enhanced using social practices. We focus on the robot\\xe2\\x80\\x99s awareness of a social\\npractice\\xe2\\x80\\x99s context, and its temporal structure as a partially ordered set of landmarks,\\neach described in terms of a purpose and a sequence of actions to be performed7.\\n\\nAs noted previously, it is assumed that the robot comes equipped with appropriate\\ngoals and plans, and that it is possible to customise certain parameters such as the\\ntime the user likes to wake up, and the time and style of coffee that he likes to have.\\nHowever, customising each plan in isolation will not easily provide the coordination\\nbetween activities and dynamic adaptability to different contexts that can be provided\\nby social practices. To perform most effectively, the robot should choose, for a given\\ncontext, the plans for each goal that will achieve the best outcomes for the patient,\\n\\n6 Actions are implemented in Jason by defining an execute method in Java class modelling the\\nenvironment. The delay is currently hard-coded in this class.\\n\\n7 Currently we only handle a single action for each landmark\\n\\n\\n\\nIncorporating social practices in BDI agent systems 9\\n\\nand furthermore, consider constraints on goal orderings that arise from preferences and\\nhabit. For example, if the patient prefers to be woken at a certain time in a given context\\n(e.g. when his family is due to visit) and/or in a certain way (e.g. by the curtains being\\nopened), his mood is likely to be adversely affected if he is woken at a different time,\\nand his engagement with subsequent activities (such as reading the newspaper together)\\nmay be reduced. In this section we describe how this type of contextual information can\\nbe addressed by the use of a social practice.\\n\\nIn Section 3, we described the various plans and actions available to the robot. We\\nnow assume that the following \\xe2\\x80\\x9cmorning routine\\xe2\\x80\\x9d social practice has emerged8. We\\npresent this as a set of beliefs in the form used by our social practice reasoning plans\\nthat will be discussed in Section 5. Note that we only illustrate a small subset of what\\nwould be likely to be a real morning routine for a patient and his/her care robot, but this\\nis sufficient to highlight the nature of social practices and their relation to BDI agents.\\n\\nsocial_practice(morningRoutine,\\n[state(location, home), resource(coffee_pods), resource(coffee_pod_machine),\\nresource(pills), resource(newspaper_subscription),\\n(time(T) & T < 1200)]).\\n\\nlandmark(morningRoutine, pa, [],\\n[action(robot, openCurtains)], state(patient, awake)).\\n\\nlandmark(morningRoutine, pt, [pa],\\n[action(patient, takePills)], state(pills, taken)).\\n\\nlandmark(morningRoutine, cs, [pa],\\n[action(robot, makePodCoffee)], state(coffee, served)).\\n\\nlandmark(morningRoutine, ms, [pt,cs],\\n[action([robot,patient], readNewspaper)], state(patient, mentally_stimulated)).\\n\\nThe first belief above encodes the name of the social practice and a list of conditions\\nthat must all hold for it to become active: there are constraints on the location, the\\nresources available, and the time (here, the number 1200 is a proxy for some real-world\\ntime that ends the morning routine period).\\n\\nThe other four beliefs model the landmarks, specifying the social practice they are\\npart of, an identifier for the landmark, a list of landmarks that must have been reached\\npreviously, a list of actions and their actors that are associated with the landmark, and\\nfinally, a goal that is the purpose of the landmark. The landmarks are: (1) to have the\\npatient awake due to the robot opening the curtains, (2) for the patient to have taken his\\npills, (3) to have the coffee served, which should involve the robot making pod coffee,\\nand (4) for the patient to be mentally stimulated due to the newspaper being read jointly.\\nThese landmarks are partially ordered with 1 before 2 and 3, which both precede 4.\\n\\nComparing this social practice to the robot plans shown in Listing 1, it can be seen\\nthat it avoids an ineffective attempt to wake the patient by talking to him, and prevents\\nhim from being left in a bad mood after being shaken awake. It agrees with the first-\\nordered plan for making coffee (by making pod coffee), and avoids an ill-fated attempt\\nby the robot to provide mental stimulation by playing Mozart. Furthermore, it specifies\\nan ordering on these activities that is not intrinsic to the plans themselves. Note also,\\nthat the social practice does not provide complete information on how to reach the\\n\\n8 It is beyond the scope of this paper to consider how social practices might be learned.\\n\\n\\n\\n10 Stephen Cranefield and Frank Dignum\\n\\nlandmark of having coffee served: it indicates that the robot should make pod coffee, but\\ndoesn\\xe2\\x80\\x99t specify the action of serving the coffee. While a planning system could deduce\\nthe missing action using a model of actions and their effects [13], a BDI agent does\\nnot have this capability. Instead, a BDI agent using social practices must reason about\\nhow its existing plans could be used to satisfy landmarks given potentially incomplete\\ninformation about the actions it must perform.\\n\\nFurthermore, the robot may already have goals to wake the patient, provide mental\\nstimulation, etc., and the activation of a social practice should not create independent\\ninstances of those goals. Thus, the activation of a social practice should override the\\nagent\\xe2\\x80\\x99s normal behaviour (for the relevant goals) during the period of activation.\\n\\nAs social practices are structured in terms of ordered landmarks, which model ex-\\npected states to be reached in a pattern of inter-agent coordination, it is necessary for the\\nagent to monitor the status of landmarks once their prior landmarks have been achieved,\\nand to actively work towards the fulfilment of the current landmarks for which it has\\nassociated actions. In the next section, we present a meta-deliberation cycle for Jason\\nagents that addresses this and the other issues outlined above, and which enables the\\nsuccessful execution of our care robot enhanced with social practices.\\n\\n5 Implementation\\n\\n5.1 Meta-level reasoning about social practices\\n\\nMaintaining awareness of social practices (SPs), and contributing to them in an appro-\\npriate way, requires agents to detect when each known social practice becomes active\\nor inactive, to monitor the state of the landmarks in an active social practice, and to\\ntrigger the appropriate activity if an active SP has an action for the agent associated\\nwith the next landmark. This is a type of meta-level reasoning that the agent should per-\\nform periodically, and it may override the performance of any standard BDI processing\\nof goals, which is not informed by social practices. We note that, on an abstract level,\\nthe same was done in [1] where the plan pattern was translated into a global pattern in\\nDrools (Java based expert system) and the specific interactions within each phase were\\nprogrammed in a chatbot.\\n\\nThe question then arises of how best to implement such a meta-level reasoner in\\na BDI architecture. The best performance can, no doubt, be achieved by extending a\\nBDI platform using its underlying implementation language. However, it would require\\nthe change of the basic deliberation cycle to include not only reasoning about goals,\\nplans and intentions, but also taking into account social practice context. Thus this\\napproach requires significant knowledge of the implementation and requires using an\\nimperative coding style that is not best suited to reasoning about goals [10] and for\\nrapid prototyping and dissemination of new reasoning techniques. Therefore, in this\\nwork we define the meta-level reasoner as a plan for a metadeliberate goal that\\nreasons about social practices, sleeps and then calls itself recursively. This, and some\\nother plans it triggers, are shown in Listing 2. The plans make use of some extensions\\nto Jason, handled by a metainterpreter that is described in the following subsection9.\\n\\n9 See https://github.com/scranefield/jason-social-practices for source code.\\n\\n\\n\\nIncorporating social practices in BDI agent systems 11\\n\\n1 /* Rules */\\n2 // Omitted: has_plan_generating_action/4 and for_all/1\\n3 relevant_sp(SP) :-\\n4 social_practice(SP, Requirements) & forall(Requirements).\\n5 sp_selection(Options, CurrentSP) :-\\n6 selected_sp(CurrentSP) & .member(CurrentSP, Options).\\n7 sp_selection([SP|_], SP).\\n8\\n\\n9 /* Initial goal */\\n10 !metadeliberate.\\n11\\n\\n12 /* Plans */\\n13 @metaplan[atomic]\\n14 +!metadeliberate <-\\n15 .findall(SP, ( relevant_sp(SP) & not completed_sp(SP) ),\\n16 RelevantSPs);\\n17 if (RelevantSPs == []) {\\n18 if (selected_sp(CurrentlySelectedSP)) {\\n19 -selected_sp(CurrentlySelectedSP)\\n20 }\\n21 } else {\\n22 if ( sp_selection(RelevantSPs, SelectedSP) &\\n23 not selected_sp(SelectedSP) ) {\\n24 -+selected_sp(SelectedSP)\\n25 }\\n26 for (monitored(Purpose, SP, ID)) {\\n27 if (Purpose) { +completed_landmark(SP, ID, Purpose) }\\n28 }\\n29 }\\n30 .wait(500);\\n31 !!metadeliberate.\\n32\\n\\n33 +selected_sp(SP) <-\\n34 for (landmark(SP, ID, _, _, Purpose)) {\\n35 PurposeNoAnnots[dummy] = Purpose[dummy];\\n36 if (.intend(PurposeNoAnnots)) {\\n37 .suspend(PurposeNoAnnots);\\n38 +suspended_intention(SP, ID, PurposeNoAnnots)\\n39 }\\n40 .add_plan({@suspend_purpose(SP,ID)\\n41 +!PurposeNoAnnots <- .suspend(PurposeNoAnnots)},\\n42 landmark(SP,ID), begin)\\n43 }\\n44 for (landmark(SP, ID, [], Actions, Purpose)) {\\n45 !activate_landmark(SP, ID, Actions, Purpose)\\n46 }.\\n47\\n\\n48 @activate_landmark[atomic]\\n49 +!activate_landmark(SP, ID, Actions, Purpose) <-\\n50 PurposeNoAnnots[dummy] = Purpose[dummy];\\n51 +monitored(PurposeNoAnnots, SP, ID)\\n52 if (Actions = [action(Actors, Act)] &\\n53 (Actors = Me | (.list(Actors) & .member(Me, Actors)))) {\\n54 if (has_plan_generating_action({+!Purpose}, Act, BodyTerms, Path)) {\\n55 !!solve([body_term(\"!\",Purpose)], Path)\\n56 } elif (joint(Act)) {\\n57 !!solve([body_term(\"\", Act[participants(Actors)])])\\n58 } elif (durative(Act)) {\\n59 !!solve([body_term(\"\", Act)])\\n60 } elif (Act =.. [F, _, _] & .substring(\".\", F)) {\\n61 !!solve([body_term(\".\", Act)])\\n62 } else { Act }\\n63 } else { .print(\"Multiple actions are not yet supported\"); }.\\n64\\n\\nListing 2: Rules and plans for social practice reasoning\\n\\n\\n\\n12 Stephen Cranefield and Frank Dignum\\n\\n65 @completed_landmark[atomic]\\n66 +completed_landmark(SP, ID, Purpose) <-\\n67 .succeed_goal(Purpose);\\n68 -monitored(Purpose, SP, ID);\\n69 .remove_plan(suspend_purpose(SP,ID));\\n70 for ( landmark(SP, ID2, PrecedingLMs, Actions, Purpose2) &\\n71 not completed_landmark(SP, ID2, _) &\\n72 .findall(PrecID, (.member(PrecID, PrecedingLMs) &\\n73 completed_landmark(SP, PrecID, _)),\\n74 CompletedPrecIDs) &\\n75 .difference(PrecedingLMs, CompletedPrecIDs, []) ) {\\n76 !activate_landmark(SP, ID2, Actions, Purpose2)\\n77 }\\n78 .findall(ID2, ( landmark(SP, ID2, _, _,_) &\\n79 not completed_landmark(SP, ID2, _) ),\\n80 PendingLandmarks);\\n81 if (PendingLandmarks == []) { +completed_sp(SP) }.\\n\\nListing 2: Rules and plans for social practice reasoning (continued)\\n\\nThe social practice reasoner runs in response to the goal metadeliberate (line\\n10 in Listing 2). Lines 13 to 31 show the plan for this goal. The atomic annotation\\non the plan label ensures that steps of this plan are not interleaved with steps of other\\nplans. The plan begins by (re)considering which social practice (if any) should be active.\\nIt uses the rules in lines 3 to 7 to find social practices that are relevant (i.e. all their\\nrequirements hold), and to select one (currently, the first option is always selected). If\\nnone are relevant (lines 17\\xe2\\x80\\x9320), any existing belief about the currently selected social\\npractice is retracted. Otherwise (lines 22\\xe2\\x80\\x9328), if the selection has changed, the belief\\nabout the selection is updated. Any monitored landmarks are then checked to see if\\ntheir purpose has been fulfilled (lines 26\\xe2\\x80\\x9328). If so, a belief about their completion is\\nadded. The plan then sleeps for period, before triggering itself to be re-run in a new\\nintention (lines 30\\xe2\\x80\\x9331). The new intention is needed for the recursive call as the plan is\\natomic, and the agent\\xe2\\x80\\x99s other plans must be allowed to run.\\n\\nA new belief about a selected social practice is handled by the plan in lines 33\\xe2\\x80\\x9346.\\nThis loops through the landmarks to check if the agent already has intentions to achieve\\nany of their purposes10. If so, these intentions are suspended, and this is recorded in a\\nbelief so the intentions can be later marked as successful if the landmark is completed\\n(see line 67). A plan is also temporarily added (lines 40-42) to ensure that if some\\nother active plan of the agent separately creates this intention, it will be immediately\\nsuspended (the new plan is placed before any existing plans for that goal). For each\\nlandmark in the social practice that has no prior landmarks, a goal is created to activate\\nit (lines 44\\xe2\\x80\\x9346).\\n\\nLandmark activations are handled by the plan in lines 48\\xe2\\x80\\x9363. A belief recording that\\nthe landmark\\xe2\\x80\\x99s purpose should be monitored is added, then the action associated with\\nthe landmark is processed (only a single action is supported currently). If the action is\\nto be performed by the agent, three options are considered. First (line 54), a query is\\nmade to find a solution for achieving the landmark\\xe2\\x80\\x99s purpose that involves performing\\nthe specified action. A set of rules (not shown) handle this query by searching for the\\naction recursively (up to a prespecified depth bound) through the plans that achieve the\\npurpose, and the subgoals in those plans, and so on. The plans\\xe2\\x80\\x99 context conditions are\\n\\n10 The unifications in lines 35 and 50 instantiate the variable on the left with the value of the\\nvariable on the right, but with any annotations removed.\\n\\n\\n\\nIncorporating social practices in BDI agent systems 13\\n\\nchecked for the top level plans (those for the landmark\\xe2\\x80\\x99s purpose), but the recursive\\ncalls do not, as, in general, it cannot be known how the state of the world will change\\nas these plans are executed. If such a solution is found, it is recorded as a goal-plan\\ntree \\xe2\\x80\\x9cpath\\xe2\\x80\\x9d (see Section 5.2) and passed to our Jason metainterpreter (line 55). If no\\nsuch solution is found, and the action is joint, durative or internal, the metainterpreter\\nis called to handle this (lines 56-61). Otherwise, the action is performed directly (l.62).\\n\\nFinally, the plan in lines 65\\xe2\\x80\\x9381 handles completed landmarks\\xe2\\x80\\x94those for which the\\npurpose has been achieved. Any suspended intentions for the purpose are succeeded,\\nthe belief stating that the landmark should be monitored is retracted, and the temporary\\nplan added in lines 40-42 is removed. The plan then checks for subsequent landmarks\\nthat should now be activated (if all their prior landmarks are completed), and finally\\nadds a belief that the social practice has completed if all its landmarks are completed.\\nAnother plan (not shown) is needed to handle social practices that become inactive\\nwhen their relevance conditions cease to hold. In this case, any active landmarks should\\nbe abandoned, and original intentions to achieve their purposes can be resumed.\\n\\nWith these plans and the metadeliberation goal in place, our robot and patient agent\\ncan successfully coordinate their actions across the landmarks of the social practice,\\nensuring that the patient remains in a good mood, and engages in the newspaper reading.\\n\\n5.2 A Jason metainterpreter\\n\\nListing 3 shows our Jason metainterpreter, which extends the AgentSpeak metain-\\nterpreter defined by Winikoff [19], and specialises it for use with Jason. The metain-\\nterpreter is initiated by calling a solve goal with a list of plan body terms, i.e. terms\\nrepresenting the various types of goals and actions that can appear in a plan body. In\\neach plan_body term, the Prefix argument identifies the type of the goal or action\\nby a string (e.g. \\xe2\\x80\\x98?\\xe2\\x80\\x99 for a query to the belief base, \\xe2\\x80\\x98+\\xe2\\x80\\x99 for a belief addition, and \\xe2\\x80\\x98!\\xe2\\x80\\x99 for\\na subgoal). From line 16 onwards, each solve trigger event has additional arguments\\nthat: (a) identify the current intention as a stack of current subgoal indices within each\\nactive plan body, interleaved with the labels for the plans currently active to solve those\\nsubgoals, and (b) a final Path argument, explained below. The intention identifier is\\nused in lines 64 to 71, which sequentially try the plans for a goal, asserting beliefs about\\nthe plans that have been tried. Lines 70 and 71 leverage Jason\\xe2\\x80\\x99s failure-handling mech-\\nanisms (posting achievement goal deletion events upon goal failure) to detect that an\\nattempt to \\xe2\\x80\\x9csolve\\xe2\\x80\\x9d a plan failed, and to try the next plan. Finally, note that there are two\\nwork-arounds for current restrictions of Jason. First, as Jason does not provide a way to\\ndecompose a plan body from within plans, line 30 calls a custom internal action we have\\nimplemented in Java. Given a trigger event (e.g. a new goal event), this action returns a\\nlist of relevant plans, encoded as list of plan terms, each including a list of plan_body\\nterms. Second, internal (in-built) actions cannot be called dynamically via instantiated\\nhigher order variables (as used for other actions: see line 56). Therefore, lines 57 to 60\\nenumerate specific internal actions that are supported (and more can be added).\\n\\nWe made the following extensions to support new capabilities:\\n\\n1. Durative actions, as required by our scenario, are supported (lines 38\\xe2\\x80\\x9355 and 73\\xe2\\x80\\x93\\n86)11. A continuation predicate, and optionally a clean-up goal, for the action are\\n\\n\\n\\n14 Stephen Cranefield and Frank Dignum\\n\\nlooked up (lines 40\\xe2\\x80\\x9345), the time the action was started is recorded as a belief (line\\n53), and a solve_durative goal is created (line 55) to trigger the performance\\nof the action. The plan for this goal (lines 73\\xe2\\x80\\x9384) checks the continuation condi-\\ntion (passed as variable Query). It is intended that the query is a 0-arity predicate\\ndefined by a rule in the agent\\xe2\\x80\\x99s program. If the query succeeds, the action is exe-\\ncuted with a \\xe2\\x80\\x9cdurative\\xe2\\x80\\x9d annotation (which the environment should check for), and\\npossibly an annotation listing the action participants if it is a joint action (see be-\\nlow). The goal is then called recursively. If the query fails, stop(Act) is executed\\n(again, with the appropriate annotations). Thus, durative actions are implemented\\nby repeated execution of an action until the corresponding stop action is called.\\n\\n2. Joint actions are also supported. These are durative actions with an annotation list-\\ning the intended action participants. The environment should notify all intended\\nparticipants (via a percept) when a durative action is called for the first time or is\\nstopped, thus enabling the participants to coordinate their actions. It should also\\nkeep a history of the time intervals over which the participants perform the action,\\nas its outcome will depend on the existence and length of a period of overlap.\\n\\n3. As explained in Section 5.1, when a landmark in a social practice includes an action\\nassociated with the current agent, the plan to activate a landmark attempts to find\\nan existing plan that can achieve the landmark\\xe2\\x80\\x99s purpose while also including the\\nspecified action. This is a recursive search through plans and their subgoals, and\\nit results in a pre-selected path through the goal-plan tree [11] corresponding to\\nthe search space for satisfying the landmark\\xe2\\x80\\x99s purpose. This path can be passed to\\nthe metainterpreter (line 12), to guide it directly to the pre-chosen subplans, and\\neventually the desired action. This feature is useful for plan pre-selection in other\\nmeta-reasoning contexts as well, e.g. choosing plans based on their effect on the\\nvalues of a human user [4].\\n\\n6 Conclusions\\n\\nWe have argued that for interactive settings, as sketched in our scenario, the use of social\\npractices is a good compromise between using a fixed interaction protocol and delib-\\neration and planning from scratch at each point during the interaction. We proposed a\\nmechanism for a BDI agent to maintain awareness about and contribute towards the\\ncompletion of social practices, and presented this as a metadeliberation plan for Jason\\nagents. We also presented a Jason metainterpreter to support this plan and our care robot\\nscenario. These contributions provide a specification of potential extensions to the BDI\\nreasoning cycle, but also allow the approach to be directly applied within Jason agents.\\n\\nOur approach allows BDI agents to use their existing plans to achieve social practice\\nlandmarks that do not detail all actions required to achieve the landmark. In future work\\nwe intend to investigate more complex interactions between social practices and agent\\xe2\\x80\\x99s\\nlocal plans. We also intend to develop elaborate scenarios that use all aspects of a social\\npractice, and compare these with agent implementations where no social practice is\\nused, both in terms of the outcomes of the agent and the ease of design of the agents.\\n\\n11 The first context condition in line 31 binds Act to the action term with annotations removed.\\n\\n\\n\\nIncorporating social practices in BDI agent systems 15\\n\\n1 /* Rules (definition of filter_list/3 is omitted) */\\n2 context_ok(plan(_,_,ContextCond,_)) :- ContextCond.\\n3\\n\\n4 /* Plans */\\n5 // Solve body term list, with optional path through the goal-plan tree\\n6 +!solve(PlanBodyTerms) <- !solve(PlanBodyTerms, [], 1, no_path).\\n7 +!solve(PlanBodyTerms, Path) <- !solve(PlanBodyTerms, [], 1, Path).\\n8\\n\\n9 // Solve body term list\\n10 +!solve([], _, _, _).\\n11 +!solve([body_term(Prefix, Term)|BTs], Intn, N, Path) <-\\n12 Intn2 = [N|Intn];\\n13 !solve(Prefix, Term, Intn2, Path);\\n14 !solve(BTs, Intn, N+1, Path).\\n15\\n\\n16 // Solve body terms\\n17 +!solve(\"?\", B, _, _) <- ?B.\\n18 +!solve(\"+\", B, _, _) <- +B.\\n19 +!solve(\"-\", B, _, _) <- -B.\\n20 +!solve(\"!\", solve(PBTs), Intn, Path) <- !solve(PBTs, Intn, 1, Path).\\n21 +!solve(\"!\", G, Intn, Path) <-\\n22 meta.relevant_plan_bodies_as_terms({+!G}, RPlans);\\n23 if (.list(Path) & Path = [N|PathTail] &\\n24 .nth(N, RPlans, plan(Label,_,_,PlanBodyTerms))) {\\n25 !solve(PlanBodyTerms, [Label|Intn], 1, PathTail);\\n26 } else {\\n27 ?filter_list(RPlans, context_ok, APlans);\\n28 !solve_one(APlans, Intn, Path);\\n29 }.\\n30 +!solve(\"\", AnnotatedAction, _, _) :\\n31 Act[dummy] = AnnotatedAction[dummy] & durative(Act) <-\\n32 ?durative_action_continuation_pred(Act, Query);\\n33 if (durative_action_cleanup_goal(Act, CleanupGoal)) { CUGoal = CleanupGoal; }\\n34 else { CUGoal = true; }\\n35 if (joint(Act) & Act[participants(P)] = AnnotatedAction) {\\n36 ParticipantAnnotation = [participants(P)];\\n37 } else {\\n38 ParticipantAnnotation = [];\\n39 }\\n40 if (time(T)) {\\n41 Act[durative|ParticipantAnnotation];\\n42 -+started(Act, T)[source(meta)];\\n43 }\\n44 !solve_durative(Query, Act, ParticipantAnnotation, CUGoal).\\n45 +!solve(\"\", Action, _, _) <- Action.\\n46 +!solve(\".\", .fail, _, _) <- .fail.\\n47 +!solve(\".\", .wait(Cond), _, _) <- .wait(Cond).\\n48 +!solve(\".\", .wait(Cond, Timeout), _, _) <- .wait(Cond, Timeout).\\n49\\n\\n50 // Solve some plan in a list of plans\\n51 +!solve_one([plan(Label,_,_,PlanBodyTerms)|_], Intn, Path) :\\n52 not tried_plan(Label, Intn) <-\\n53 +tried_plan(Label, Intn);\\n54 !solve(PlanBodyTerms, [Label|Intn], 1, Path);\\n55 -tried_plan(Label, Intn).\\n56 -!solve_one([_|PlanTerms], Intn, Path) <- !solve_one(PlanTerms, Intn, Path).\\n57\\n\\n58 +!solve_durative(Query, Act, ParticipantAnnotation, CleanupGoal) <-\\n59 if (Query) {\\n60 Act[durative|ParticipantAnnotation];\\n61 !solve_durative(Query, Act, ParticipantAnnotation, CleanupGoal);\\n62 } else {\\n63 stop(Act)[durative|ParticipantAnnotation];\\n64 if (CleanupGoal \\\\== true) { !CleanupGoal; }\\n65 }.\\n66 -!solve_durative(_, Act, _) <- -started(Act, _).\\n\\nListing 3: A Jason metainterpreter\\n\\n\\n\\n16 Stephen Cranefield and Frank Dignum\\n\\nReferences\\n\\n1. Augello, A., Gentile, M., Dignum, F.: Social practices for social driven conversations in\\nserious games. In: De Gloria, A., Veltkamp, R. (eds.) GALA 2015. pp. 100\\xe2\\x80\\x93110. Springer\\n(2016)\\n\\n2. Bordini, R.H., Hu\\xcc\\x88bner, J.F., Wooldridge, M.: Programming multi-agent systems in Agent-\\nSpeak using Jason. Wiley (2007)\\n\\n3. Bourdieu (trans. R. Nice), P.: Outline of a theory of practice. Cambridge University Press\\n(1972)\\n\\n4. Cranefield, S., Winikoff, M., Dignum, V., Dignum, F.: No pizza for you: Value-based plan\\nselection in BDI agents. In: Proceedings of the Twenty-Sixth International Joint Conference\\non Artificial Intelligence. pp. 178\\xe2\\x80\\x93184. ijcai.org (2017)\\n\\n5. Dignum, F.: Interactions as social practices: towards a formalization. arXiv (2018), https:\\n//arxiv.org/abs/1809.08751\\n\\n6. Dignum, V., Dignum, F.: Contextualized planning using social practices. In: Coordination,\\nOrganizations, Institutions and Norms in Agent Systems X. pp. 36\\xe2\\x80\\x9352. Springer (2015)\\n\\n7. Folsom-Kovarik, J., Schatz, S., Jones, R.M., Bartlett, K., Wray, R.E.: AI challenge problem:\\nScalable models for patterns of life. AI Magazine 35(1), 10\\xe2\\x80\\x9314 (2014)\\n\\n8. Giddens, A.: Central problems in social theory: Action, structure and contradiction in social\\nanalysis. University of California Press (1979)\\n\\n9. Harel, R., Yumak, Z., Dignum, F.: Towards a generic framework for multi-party dialogue\\nwith virtual humans. In: Proceedings of the 31st International Conference on Computer An-\\nimation and Social Agents. pp. 1\\xe2\\x80\\x936. CASA 2018, ACM, New York, NY, USA (2018)\\n\\n10. Logan, B.: An agent programming manifesto. International Journal of Agent-Oriented Soft-\\nware Engineering 6(2), 187\\xe2\\x80\\x93210 (2018)\\n\\n11. Logan, B., Thangarajah, J., Yorke-Smith, N.: Progressing intention progression: A call for a\\ngoal-plan tree contest. In: Proceedings of the 16th Conference on Autonomous Agents and\\nMultiagent Systems. pp. 768\\xe2\\x80\\x93772. ACM (2017)\\n\\n12. Mercuur, R., Dignum, F., Kashima, Y.: Changing habits using contextualized decision mak-\\ning. In: Jager, W., Verbrugge, R., Flache, A., de Roo, G., Hoogduin, L., Hemelrijk, C. (eds.)\\nAdvances in Social Simulation 2015. pp. 267\\xe2\\x80\\x93272. Springer (2017)\\n\\n13. Miller, T., Dignum, V., Dignum, F.: Planning for human-agent collaboration using social\\npractices. In: First international workshop on socio-cognitive systems at IJCAI 2018 (2018)\\n\\n14. Minsky, M.: A framework for representing knowledge. In: Smith, A., Collins, E. (eds.) Read-\\nings in Cognitive Science, pp. 156\\xe2\\x80\\x93189. Morgan Kaufmann (1988)\\n\\n15. Narasimhan, K., Roberts, T., Xenitidou, M., Gilbert, N.: Using ABM to clarify and refine\\nsocial practice theory. In: Jager, W., Verbrugge, R., Flache, A., de Roo, G., Hoogduin, L.,\\nHemelrijk, C. (eds.) Advances in Social Simulation 2015. pp. 307\\xe2\\x80\\x93319. Springer (2017)\\n\\n16. Reckwitz, A.: Toward a theory of social practices. European Journal of Social Theory 5(2),\\n243\\xe2\\x80\\x93263 (2002)\\n\\n17. Schatzki, T.R.: A primer on practices. In: Practice-Based Education, Practice, Education,\\nWork and Society, vol. 6, pp. 13\\xe2\\x80\\x9326. SensePublishers, Rotterdam (2012)\\n\\n18. Shove, E., Pantzar, M., Watson, M.: The Dynamics of Social Practice. Sage (2012)\\n19. Winikoff, M.: An AgentSpeak meta-interpreter and its applications. In: Programming Multi-\\n\\nAgent Systems, Third International Workshop, ProMAS 2005, Lecture Notes in Computer\\nScience, vol. 3862, pp. 123\\xe2\\x80\\x93138. Springer (2006)\\n\\n\\n'