b'\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nThe \\xe2\\x80\\x9cWhy did you do that?\\xe2\\x80\\x9d Button: Answering\\nWhy-questions for end users of Robotic Systems\\n\\nVincent J. Koeman1, Louise A. Dennis2, Matt Webster2, Michael Fisher2, and\\nKoen Hindriks3\\n\\n1 Delft University of Technology, The Netherlands\\nv.j.koeman@tudelft.nl\\n\\n2 University of Liverpool, UK\\n{L.A.Dennis, M.Webster, MFisher}@liverpool.ac.uk\\n\\n3 Vrije Universiteit Amsterdam, The Netherlands\\nk.v.hindriks@vu.nl\\n\\nAbstract. The issue of explainability for autonomous systems is becom-\\ning increasingly prominent. Several researchers and organisations have\\nadvocated the provision of a \\xe2\\x80\\x9cwhy did you do that?\\xe2\\x80\\x9d button which al-\\nlows a user to interrogate a robot about its choices and actions.\\nWe take previous work on debugging cognitive agent programs and apply\\nit to the question of supplying explanations to end users in the form of\\nanswers to why-questions. These previous approaches are based on the\\ngeneration of a trace of events in the execution of the program and then\\nanswering why-questions using the trace. We implemented this frame-\\nwork in the agent infrastructure layer and, in particular, the Gwen-\\ndolen programming language it supports \\xe2\\x80\\x93 extending it in the process\\nto handle the generation of applicable plans and multiple intentions.\\nIn order to make the answers to why-questions comprehensible to end\\nusers we abstract away from some events in the trace and employ appli-\\ncation specific predicate dictionaries in order to translate the first-order\\nlogic presentation of concepts within the cognitive agent program in nat-\\nural language. A prototype implementation of these ideas is provided.\\n\\n1 Introduction\\n\\nAs Autonomous Systems become more prevalent in society, issues related to\\nthe ways in which humans interact with such systems become more important.\\nAmong these issues is the question of transparency and, in particular, explain-\\nability. Wortham and Theodorou [37], and Sheh [25] (among others) have argued\\nthat the ability for a robot (and by extension any autonomous system) to pro-\\nvide explanations of its behaviour helps users develop accurate mental models\\nof the robot\\xe2\\x80\\x99s reasoning and so interact better with the robot and develop trust.\\nCharisi et al. [6], Turner [27] and The IEEE Global Initiative on Ethics of Au-\\ntonomous and Intelligent Systems [26] in particular advocate the provision of a\\n\\xe2\\x80\\x9cwhy did you do that?\\xe2\\x80\\x9d button to help the user understand a robot\\xe2\\x80\\x99s behaviour.\\n\\nWe take as our focus autonomous systems which employ a cognitive agent\\nto make high level decisions such as [30, 29, 39]. One of the reasons often put\\n\\nlouisedennis\\nPlaced Image\\n\\n\\n\\n2 Authors Suppressed Due to Excessive Length\\n\\nforward for the employment of cognitive agents in this role is their in principle\\nability to explain their decisions to end users. However, in practice, little research\\nhas been performed in actually providing such explanations of reasoning.\\n\\nThere are a number of key problems in the provision of explanations. Firstly\\nthey require a backward view of the program execution (in contrast to common\\ndebugging practice in which a breakpoint is set and the program is then run\\nforwards from the breakpoint) Secondly log files, which are the obvious solution\\nto the first problem tend to be verbose and their production can cause significant\\nperformance overheads. These problems are exacerbated when all the informa-\\ntion needed to understand why something is taking place must be captured.\\n\\nIn this paper we combine work on the debugging of cognitive agent programs\\nin the Beliefs-Desires-Intentions (BDI) paradigm [18] with work on the provision\\nof explanations for programmers in GOAL [16] and AgentSpeak [33]. Koeman\\net al. [18] generate an omniscient trace of key events that take place during\\nprogram execution in a manner which limits the overhead cost of producing\\nthe trace. Each event stores enough information about the agent\\xe2\\x80\\x99s mental state\\nto reconstruct the state of the program execution at that point. This trace is\\nsupported by tools allowing it to be viewed at a high-level of abstraction hiding\\nextraneous information unless a user wants to see it.\\n\\nWe have implemented omniscient tracing in the Agent Infrastructure Layer\\n(AIL) [8, 10], a prototyping tool for verifiable interpreters for cognitive agent\\nprogramming languages, with particular attention to the Gwendolen program-\\nming language [9] but also consider the extent to which this framework is generic.\\nIn applying this framework to the AIL we extended the events considered be-\\nyond changes to the agent\\xe2\\x80\\x99s mental state to include a number of events involved\\nin the generation of plans and the handling of intentions.\\n\\nThe development of omniscient debugging was driven, in part, by a desire to\\nsupport programmers in answering why-questions. Programmers can interrogate\\nthe high level trace at specific points in the program execution and ask \\xe2\\x80\\x9cwhy\\ndid you do that\\xe2\\x80\\x9d (as outlined in [16]). Winikoff [33] reports on a similar system\\nconstructing why and why-not explanations over traces for AgentSpeak.\\n\\nWe implemented this idea in our AIL-based omniscient debugging frame-\\nwork. We developed an explanation generation framework for end users that is\\nspecific to Gwendolen, providing explanations at a higher level of abstraction\\nthan previously considered, and using predicate dictionaries to provide natural\\nlanguage substitutes for application specific logical predicates. This implemen-\\ntation generates explanations when multiple intentions are being executed in an\\ninterleaved fashion (something omitted from [33]).\\n\\n2 Background and Related Work\\n\\n2.1 Cognitive Agent Programming\\n\\nAt its most general, an agent is an abstract concept that represents an au-\\ntonomous computational entity that makes its own decisions [35]. A general\\n\\n\\n\\nTitle Suppressed Due to Excessive Length 3\\n\\nagent is simply the encapsulation of some distributed computational component\\nwithin a larger system. However, in many settings, something more is needed.\\nRather than just having a system which makes its own decisions in an opaque\\nway, it is increasingly important for the agent to have explicit reasons (that it\\ncould explain, if necessary) for making one choice over another.\\n\\nCognitive agents [3, 22, 36] enable the representation of this kind of reasoning.\\nSuch an agent has explicit reasons for making the choices it does. We often\\ndescribe a cognitive agent\\xe2\\x80\\x99s beliefs and goals, which in turn determine the agent\\xe2\\x80\\x99s\\nintentions. Such agents make decisions about what action to perform, given their\\ncurrent beliefs, goals and intentions. This view of cognitive agents is encapsulated\\nwithin the Beliefs-Desires-Intentions (BDI) model [21, 22, 23]. Beliefs represent\\nthe agent\\xe2\\x80\\x99s (possibly incomplete, possibly incorrect) information about itself,\\nother agents, and its environment, desires represent the agent\\xe2\\x80\\x99s long-term goals\\nwhile intentions represent the goals that the agent is actively pursuing (the\\nrepresentation of intentions often includes partially instantiated and/or executed\\nplans and so combines the goal with its intended means).\\n\\nThere are many different agent programming languages and agent platforms\\nbased, at least in part, on the BDI approach [24, 1, 7, 20, 15]. Agents programmed\\nin these languages commonly contain a set of beliefs, a set of goals, and a set of\\nplans. Plans determine how an agent acts based on its beliefs and goals and form\\nthe basis for practical reasoning (i.e., reasoning about actions) in such agents.\\nAs a result of executing a plan, the beliefs and goals of an agent may change\\nand actions may be executed.\\n\\nIt is generally recognised that debugging BDI agent programs is hard [31, 32]\\n(and by extension that agent behaviour can be difficult to understand even\\nwhen performing as desired). In particular agents react to events in dynamic\\nenvironments; events which may combine in unexpected ways and which may be\\nhandled by the agent \\xe2\\x80\\x9cin parallel\\xe2\\x80\\x9d with each other. Furthermore many cognitive\\nagent languages have provision for failure handling which, again, may interact\\nin complex ways with the behaviour of the rest of the program.\\n\\n2.2 Explanations in Cognitive Agent Systems and \\xe2\\x80\\x9cWhy\\xe2\\x80\\x9d Questions\\n\\nKo and Myers [17] created the WHY-LINE tool, which allows developers to\\npose \\xe2\\x80\\x9cwhy did\\xe2\\x80\\x9d or \\xe2\\x80\\x9cwhy didn\\xe2\\x80\\x99t\\xe2\\x80\\x9d questions about the output of Java programs.\\nA trace is generated in memory through bytecode instrumentation, containing\\neverything necessary for reproducing a specific execution. From this trace, a\\nset of questions and associated answers is generated. The authors note that\\ntheir approach is not suited for executions that span more than a few minutes\\nor executions that process or produce substantial amounts of data. However,\\ntheir results do show that the approach enables developers to debug failures\\nsubstantially faster.\\n\\nHindriks [16] and Winikoff [33] both consider a similar model applied to the\\ndebugging of cognitive agent programs in GOAL and AgentSpeak respectively.\\nOf these [16] is the earlier and has a more informal treatment than [33] which\\nsought to extend, formalise and implement the proposal. The two approaches are\\n\\n\\n\\n4 Authors Suppressed Due to Excessive Length\\n\\ntherefore similar in their underlying conception and we use them as the basis for\\nour work. The key idea is that a trace of events is stored as a log. Each event in\\nthe trace can be interrogated and an explanation constructed in a systematic way\\nusing information either stored in that event and/or by referring to a previous\\nevent in the log. For instance the explanation for why some action was executed\\nmight be that \\xe2\\x80\\x9cthe action\\xe2\\x80\\x99s preconditions held and a plan was previously selected\\nwhich contained the action\\xe2\\x80\\x9d. Explanations can then also be given for why the\\npreconditions succeeded and/or why the plan was selected.\\n\\nKoeman et al. [18] propose a trace based mechanism for debugging cognitive\\nagent programs. Although concerned with many of the same issues as [16] and\\n[33] (and indeed, intended as support for the mechanisms proposed in [16]) the\\nauthors focus on more foundational questions of what information needs to be\\nstored in a trace in order to reconstruct the state of an agent at that point, and\\nthe performance overhead of storing such traces for a program. They conclude\\nthat if a trace stored the key events in agent execution, namely the changes to\\nthe agent\\xe2\\x80\\x99s mental state, then the program run could be reconstructed without\\nthe significant performance impact associated with storing the full state of an\\nagent at each step in execution. They develop a space-time visualiser for these\\ntraces which allows a programmer to inspect the trace and query the state of\\nthe underlying program at any point.\\n\\nHindriks [16] and Koeman et al. [18] consider primarily changes to an agent\\xe2\\x80\\x99s\\nmental state (i.e., beliefs and goals) in their tracing and debugging frameworks.\\nWinikoff [33] extends this to include traces and explanations for the selection\\nof plans but assumes that the entirety of a plan is executed before anything\\nelse happens. The AIL allows interleaved execution of plans by manipulating\\nintentions. In our work therefore, we integrated the approach in [33] with that\\nof [18] and then extended it to the handling of multiple intentions4.\\n\\nA few systems have considered the question of providing explanations specif-\\nically for end users of cognitive agent systems. In Harbers [14] explanations of\\nagent behaviour are generated based on the beliefs and goals of the agents us-\\ning a goal hierarchy paired with a behaviour log. Winikoff et al. [34] presents a\\nsimilar system but adds the concept of preferences (or valuings) to the explana-\\ntions presented to end users. This was evaluated using hand-crafted explanations\\ngenerated according to the methodology outlined in the paper.\\n\\n2.3 The Agent Infrastructure Layer and Gwendolen\\n\\nThe Agent Infrastructure Layer (AIL) [8] is a set of Java classes intended to assist\\nin the development of BDI-style programming languages. Gwendolen [9] is the\\nmost mature language in this framework.\\n\\nThe AIL provides default data structures for agents, beliefs, goals, plans and\\nintentions. Individual languages implemented in the AIL define custom reasoning\\n4 Though it should be noted that the implementation of omniscient debugging in\\n\\nGOAL also handles GOAL\\xe2\\x80\\x99s module mechanism (although this is not reported in\\ndepth in [18]) which is not entirely dissimilar to the concept of intention in the AIL.\\n\\n\\n\\nTitle Suppressed Due to Excessive Length 5\\n\\ncycles for agent deliberation. However the toolkit has an underlying assumption\\nthat such reasoning cycles will typically involve the following steps in some order:\\n\\xe2\\x80\\x93 Perception which creates sets of new beliefs and removes beliefs that no\\n\\nlonger hold.\\n\\xe2\\x80\\x93 Posting events (either as new intentions, or added to existing intentions)\\n\\nwhen beliefs are acquired or removed and goals are acquired or removed.\\n\\xe2\\x80\\x93 Selecting plans to react to events.\\n\\xe2\\x80\\x93 Selecting among intentions which represent partially processed plans or un-\\n\\nhandled events.\\n\\xe2\\x80\\x93 Processing one (or more) steps in an intention which include adding and\\n\\nremoving beliefs and goals and executing actions.\\nThese default steps therefore form the core events supported by our implemen-\\ntation of omniscient debugging within the AIL.\\n\\nGwendolen Operational Semantics We use Gwendolen as our key imple-\\nmentation language. We present here a simplified version of the Gwendolen\\noperational semantics which is presented in full in [9]. The semantics presented\\nhere assumes all terms are ground (so ignores issues surrounding the handling of\\nunifiers), and ignores a number of language features such as locking and suspend-\\ning intentions, dropping goals, agent sleeping and waking behaviour, message\\nhandling and special cases such as transitions for handling goals that can\\xe2\\x80\\x99t be\\nplanned. The intention is to present enough information to allow our framework\\nto be understood. This operational semantics is shown in Figure 1. Following [33]\\nwe annotate the transitions (expressions above the arrow) with the events that\\nare stored by the omniscient debugger. These are discussed further in Section 3.\\n\\nThe Gwendolen reasoning cycle shown here has five stages A,B,C,D and\\nE5. One transition in each stage is executed in turn. In the semantics we show\\nthe stage that a transition applies to with a letter to the right of the rule. A\\nGwendolen agent starts in stage A and so (1) is the first rule to apply, followed\\nby (2) and so on. In stage D whichever rule applies to the top of the current\\nintention is applied and then the reasoning cycle moves on to stage E.\\n\\nBDI languages use intentions to store the intended means for achieving goals\\n\\xe2\\x80\\x93 this is generally represented as some form of deed stack (deeds include ac-\\ntions, belief updates, and the commitment to goals). In Gwendolen intention\\nstructures6 also maintain information about the event that triggered them (the\\naddition or removal of a belief or the posting of a (sub-)goal). Gwendolen ag-\\ngregates this information: an intention becomes a stack of tuples of an event and\\na deed. Each tuple associates a particular deed with the event that caused the\\ndeed to be placed on the intention. New events are associated with an empty\\ndeed, \\xef\\xbf\\xbd.\\n\\nWe represent an agent state as a tuple \\xe3\\x80\\x88i, I, B,A\\xe3\\x80\\x89 where: i is the current\\nintention; I is a queue of intentions {i1, i2, ..}; B is a set of the agent\\xe2\\x80\\x99s beliefs;\\nand A is a set of currently applicable plans for the current intention i.\\n5 The implementation of Gwendolen contains a sixth stage for message handling.\\n6 A refinement of the AIL\\xe2\\x80\\x99s intention structure which is more general.\\n\\n\\n\\n6 Authors Suppressed Due to Excessive Length\\n\\nSint(I \\xe2\\x88\\xaa {i}) = (i \\xe2\\x80\\xb2, I \\xe2\\x80\\xb2)\\n\\n\\xe3\\x80\\x88i, I , B, \\xe2\\x88\\x85\\xe3\\x80\\x89\\nseli(i)\\n\\xe2\\x88\\x92\\xe2\\x88\\x92\\xe2\\x88\\x92\\xe2\\x88\\x92\\xe2\\x86\\x92 \\xe3\\x80\\x88i \\xe2\\x80\\xb2, I \\xe2\\x80\\xb2, B, \\xe2\\x88\\x85\\xe3\\x80\\x89\\n\\nA (1)\\n\\nG(i, I , B) = A\\n\\n\\xe3\\x80\\x88i, I , B, \\xe2\\x88\\x85\\xe3\\x80\\x89\\ngenp(A,i);\\xce\\x93\\n\\xe2\\x88\\x92\\xe2\\x88\\x92\\xe2\\x88\\x92\\xe2\\x88\\x92\\xe2\\x88\\x92\\xe2\\x88\\x92\\xe2\\x88\\x92\\xe2\\x88\\x92\\xe2\\x86\\x92 \\xe3\\x80\\x88i, I , B,A\\xe3\\x80\\x89\\n\\nB (2)\\n\\n(e, ds) = Splan(A)\\n\\n\\xe3\\x80\\x88i, I , B,A\\xe3\\x80\\x89\\nselp(e,ds,i)\\n\\xe2\\x88\\x92\\xe2\\x88\\x92\\xe2\\x88\\x92\\xe2\\x88\\x92\\xe2\\x88\\x92\\xe2\\x88\\x92\\xe2\\x88\\x92\\xe2\\x86\\x92 \\xe3\\x80\\x88(e, ds) @ tli(i), I , B, \\xe2\\x88\\x85\\xe3\\x80\\x89\\n\\nC (3)\\n\\nB |= g\\n\\xe3\\x80\\x88(e,+!g);ii, I , B, \\xe2\\x88\\x85\\xe3\\x80\\x89 \\xe2\\x86\\x92 \\xe3\\x80\\x88i, I , B, \\xe2\\x88\\x85\\xe3\\x80\\x89\\n\\nD (4)\\n\\nB 6|= g\\n\\n\\xe3\\x80\\x88(e,+!g);ii, I , B, \\xe2\\x88\\x85\\xe3\\x80\\x89\\nadd((e,+!g),i)\\n\\xe2\\x88\\x92\\xe2\\x88\\x92\\xe2\\x88\\x92\\xe2\\x88\\x92\\xe2\\x88\\x92\\xe2\\x88\\x92\\xe2\\x88\\x92\\xe2\\x88\\x92\\xe2\\x88\\x92\\xe2\\x86\\x92 \\xe3\\x80\\x88(+!g, \\xef\\xbf\\xbd);i(e,+!g);ii, I , B, \\xe2\\x88\\x85\\xe3\\x80\\x89\\n\\nD (5)\\n\\n\\xe3\\x80\\x88(e,+b);ii, I ,B, \\xe2\\x88\\x85\\xe3\\x80\\x89\\ncrei((+b,\\xef\\xbf\\xbd))\\n\\xe2\\x88\\x92\\xe2\\x88\\x92\\xe2\\x88\\x92\\xe2\\x88\\x92\\xe2\\x88\\x92\\xe2\\x88\\x92\\xe2\\x88\\x92\\xe2\\x86\\x92 \\xe3\\x80\\x88i, I \\xe2\\x88\\xaa (+b, \\xef\\xbf\\xbd),B \\xe2\\x88\\xaa {b}, \\xe2\\x88\\x85\\xe3\\x80\\x89\\n\\nD (6)\\n\\n\\xe3\\x80\\x88(e,\\xe2\\x88\\x92b);ii, I ,B, \\xe2\\x88\\x85 . . .\\xe3\\x80\\x89\\ncrei((\\xe2\\x88\\x92b,\\xef\\xbf\\xbd))\\n\\xe2\\x88\\x92\\xe2\\x88\\x92\\xe2\\x88\\x92\\xe2\\x88\\x92\\xe2\\x88\\x92\\xe2\\x88\\x92\\xe2\\x88\\x92\\xe2\\x88\\x92\\xe2\\x86\\x92 \\xe3\\x80\\x88i, I \\xe2\\x88\\xaa (\\xe2\\x88\\x92b, \\xef\\xbf\\xbd),B\\\\{b}, \\xe2\\x88\\x85\\xe3\\x80\\x89\\n\\nD (7)\\n\\ndo(a)\\n\\n\\xe3\\x80\\x88(e, a);ii, I , B, \\xe2\\x88\\x85\\xe3\\x80\\x89\\nact(a,i)\\n\\xe2\\x88\\x92\\xe2\\x88\\x92\\xe2\\x88\\x92\\xe2\\x88\\x92\\xe2\\x88\\x92\\xe2\\x86\\x92 \\xe3\\x80\\x88i, I , B, \\xe2\\x88\\x85\\xe3\\x80\\x89\\n\\nD (8)\\n\\nP = Percepts OP = {b | b \\xe2\\x88\\x88 B\\\\P \\xe2\\x88\\xa7 percept(b)}\\n\\n\\xe3\\x80\\x88i, I ,A,B\\xe3\\x80\\x89 \\xce\\xa0\\xe2\\x88\\x92\\xe2\\x86\\x92\\n\\xe3\\x80\\x88i, I \\xe2\\x88\\xaa {(percept,+b) | b \\xe2\\x88\\x88 P\\\\B} \\xe2\\x88\\xaa {(percept,\\xe2\\x88\\x92b) | b \\xe2\\x88\\x88 OP},B,A\\xe3\\x80\\x89\\n\\nE (9)\\n\\nFig. 1. Simplified Gwendolen Semantics\\n\\nA Gwendolen program consists of a set of plans,\\xe2\\x88\\x86, of the form, e : {g} \\xe2\\x86\\x90 ds\\n(where ds is a sequence of deeds to be executed if event, e is posted and guard,\\ng, follows from the agent\\xe2\\x80\\x99s beliefs and goals), a set of initial beliefs, B, and a set\\nof initial goals, Gs. In an agent\\xe2\\x80\\x99s initial state the current intention is null, the\\nintention set consists of one intention for each of the initial goals provided by the\\nprogrammer of the form (start,+!g). The belief base is B and the applicable\\nplans are empty.\\n\\n(1) governs the selection of intentions. Sint is an application specific function\\nthat selects one intention out of a set of intentions and returns a tuple of the\\nselected intention and the set without that intention in it. By default Sint oper-\\nates on a queue data structure and so in general the current intention is placed\\nat the end of the queue and the intention at the top of the queue is selected.\\n\\n(2) represents the process of inspecting the plan library and finding plans\\nthat match the current intention. These are transformed into applicable plans\\nand returned by the function G. A plan, e : {g} \\xe2\\x86\\x90 ds matches an intention if e\\nmatches the event in the top tuple of the intention, g is a logical consequence\\nof the agent\\xe2\\x80\\x99s beliefs and goals (goals are inferred from the events posted in all\\nintentions) and the deed in the top tuple of the intention is \\xef\\xbf\\xbd. Applicable plans\\nare an interim data structure that describe how the plan changes the current\\n\\n\\n\\nTitle Suppressed Due to Excessive Length 7\\n\\nintention. An applicable plan describes new tuples to be placed on the top of\\nthe intention stack (replacing the existing top tuple). A tuple is created for each\\ndeed in ds and associated with e. Where the top deed on the intention is not\\n\\xef\\xbf\\xbd (\\xe2\\x80\\x9cno plan yet\\xe2\\x80\\x9d) then G simply returns the existing top tuple so there is no\\nchange to the intentin and it continues to be processed as normal7.\\n\\n(3) uses the application specific function Splan to pick an applicable plan\\nto be applied. By default, this treats the set as a list and picks the first plan\\nbased on the order they appear in the Gwendolen program. We use the syntax\\n(e, ds) @ tli(i) to represent the replacement of the top tuple in the intention by\\nthe tuples in the applicable plan.\\n\\n(4), (5), (6), (7) and (8) process the top deed in the intention handling the\\ninstruction to add a goal (depending upon whether the goal is already achieved\\nor not), add a belief, drop a belief and execute an action respectively. (e, d);ii\\nrepresents the addition of the tuple (e, d) to the top of the intention i. do(a)\\nrepresents the execution of an action in some external environment. These rules\\nmake a check on the top deed in the intention to see what type it is (e.g. the\\naddition of a belief, the deletion of a goal). We represent these checks implicitly\\nusing the notation: a for an action; +b for a belief addition; \\xe2\\x88\\x92b for a belief\\nremoval; and +!g for a goal addition.\\n\\n(9) handles perception. A set of Percepts are gathered from the environ-\\nment. New percepts are added as intentions to add a belief. Out of date percepts\\n(i.e., percepts in the belief base that can no longer be perceived) are handled by\\ncreating a new intention to remove them.\\n\\n3 An AIL-based Framework for Omniscient Debugging\\nDriven Explanations for Cognitive Agents\\n\\nAs noted above, omniscient debugging was developed with the intention of sup-\\nporting explanations in the form of answering why- and why-not-questions as\\noutlined in [16] and [33]. The implementation of omniscient debugging in GOAL\\nhad already identified a number of key events that were needed in an omniscient\\ntrace. Since both GOAL and the AIL were implemented in Java it was possible\\nto port much of the framework for efficient storage of event traces directly from\\nGOAL to the AIL. The only necessary step was identifying additional events\\nand agent states that needed storing.\\n\\nOmniscient debugging for GOAL focused on the changes in agent goals and\\nbeliefs as the key events underpinning a trace. We used the analysis from sec-\\ntion 2.3 to extend this to 8:\\n1. Creation of intention, i: crei(i).\\n2. Selection of intention, i: seli(i).\\n\\n7 This somewhat baroque mechanism has its roots in Gwendolen\\xe2\\x80\\x99s origin as an in-\\ntermediate language into which all BDI languages could be translated [11].\\n\\n8 Note this is not the complete set of events shown in figure 1. This is elaborated\\nfurther in section 3.1\\n\\n\\n\\n8 Authors Suppressed Due to Excessive Length\\n\\n3. Successful evaluation of guard, g for (applicable) plan \\xcf\\x80, with unifier, \\xce\\xb8 in\\nintention, i: bel(\\xcf\\x80, g, \\xce\\xb8, i).\\n\\n4. Selection of an applicable plan, (e, ds) in intention, i: selp(e, ds, i).\\n5. Execution of action, a, by intention i: act(a, i).\\n6. Adding or removing goal, g, by intention i: addg(g, i), delg(g, i).\\n7. Adding or removing belief, b by intention i: addb(b, i), delb(b, i).\\n8. Modification of intention, i, by adding or removing tuples, ts: add(ts, i),\\ndel(ts, i).\\nIntentions are stored in events in their current state. In order to track the\\n\\nevolution of intentions in traces more easily, we extended intentions with an ID\\nnumber, k, and will use the notation ik to represent that intention, i, has ID\\nnumber, k. Mostly we will omit ID numbers except where they are necessary to\\nunderstand the construction of explanations.\\n\\n3.1 Adaptation to Gwendolen\\n\\nCommands to log these events were embedded in relevant parts of the AIL\\ntoolkit, primarily in classes used to implement transition rules in reasoning cy-\\ncles. This is why in Figure 1 we were able to annotate the transitions in the\\nGwendolen semantics with the associated events. Given Gwendolen always\\ncreates an intention when beliefs are changed and modifies the current intention\\nwhen a goal is posted we do not use addg(g, i), delg(g, i), or delb(b, i) in Gwen-\\ndolen traces and only use addb(b, start) for the addition of initial beliefs. We\\nrely instead on crei(i) and add(ts, i).\\n\\nWe added a further event just for Gwendolen programs which was the\\ngeneration of a set of applicable plans, A for some intention, i: genp(A, i).\\n\\nIn Figure 1 we reference two further constructs, \\xce\\x93 and \\xce\\xa0, these represent\\nsituations where one transition in the semantics generates several events in the\\ntrace. \\xce\\x93 logs each successful guard evaluation for plans in \\xe2\\x88\\x86 as an event in\\nthe trace and associates them with the relevant applicable plan. \\xce\\xa0 logs the\\ncreation of the intentions caused by the addition and removal of beliefs following\\nperception.\\n\\n3.2 Trace Construction and Visualisation\\n\\nThe way in which these events are presented to an agent programmer, i.e. as\\na log of the execution, is often overlooked. However, an effective and efficient\\nexecution log is vital to the debugging process [38]. Effective here means a log\\ncontains sufficient information for an agent programmer to get insight into the\\nbehaviour of an agent program. Efficient here means that only the information\\nthat potentially provides such insights is provided in a log in a concise manner.\\nArguably, a trace for an agent program contains at least the information needed\\nto construct such a log.\\n\\nRecording events (i.e., tracing) is an important step that facilitates later\\nvisualisation, explanation, and other forms of processing of the execution of an\\n\\n\\n\\nTitle Suppressed Due to Excessive Length 9\\n\\nagent program. Moreover, in order to not significantly influence the execution of\\na traced program, this has to be done as efficiently as possible.\\n\\nWe were able to use the work of Koeman et al. [18] more or less directly to\\nimplement tracing in the AIL with these desirable properties.\\n\\n3.3 From Traces to Explanations\\n\\nFor answering a why-question, a list of recorded events (i.e, a trace) is mapped\\nto a chain of reasons (i.e., an explanation). Reasons thus represent a selection\\nof directly connected events that might span over large parts of the trace. For\\nexample, the event of adopting a goal can be directly connected to the event\\nof evaluating the guard of the plan in which it was adopted, in between which\\nmany other events could be present in the trace (e.g., the evaluations of other\\nguards). Based on the known ordering of (types of) events, an efficient algorithm\\ncan be constructed for deducing reasons from them.\\n\\n3.4 Why-Questions in Gwendolen\\n\\nIn our initial prototype we consider only the answering of why-questions, leaving\\nconsideration of why-not-questions (and their interaction with why-questions) to\\nfurther work.\\n\\nIn order to generate explanations we need to link each of the traced events to\\na local explanation as outlined particularly in [33] but also implied in [16]. To do\\nthis the explanation had to be grounded in the specific language, Gwendolen,\\nunder consideration but nevertheless could be fitted into a general framework.\\n\\nWe consider some event e occurring at step, N , in a trace t and assume, fol-\\nlowing [33], the existence of a language specific function why such that why(eN , t)\\nreturns some representation of an explanation. This representation may recur-\\nsively refer to some previous event(s) which can also be expanded and so on, if\\ndesired, all the way back to the start of program execution. However, our focus\\non end users means that explanations should have a default cut-off point and\\nare not unwrapped further unless requested by the user. These cut-offs are the\\nreasons why a plan was selected and the reason why an intention was created.\\n\\nFigure 2 shows the algorithm for constructing why for Gwendolen.\\nAs noted, why(eN , T ) can be read as \\xe2\\x80\\x9cwhy did e occur at step N in trace\\n\\nT\\xe2\\x80\\x9d where e is one of our traced events. We can also ask why some formula is\\nbelieved at step N , why(bN , T ), and why some formula is a goal at step N ,\\nwhy(!gN , T ). The left hand side of each equation then constructs an explana-\\ntion presented as an expression \\xe2\\x80\\x93 reasons joined by because or and. We use \\xe2\\x88\\xa8\\nto represent situations where there is a choice of possible explanations. So, for\\ninstance in (10) the reason an applicable plan is selected is because the guard\\ng was believed and either the event e created an intention (as happens when\\nbeliefs are posted in Gwendolen) or e was added to an intention (as happens\\nwhen goals are posted in Gwendolen). In constructing an explanation we al-\\nways select the most recent event that satisfies the formula so, again in (10),\\n\\n\\n\\n10 Authors Suppressed Due to Excessive Length\\n\\nwhy(selp((e, ds), ik)N , T ) =bel((e, ds), g, \\xce\\xb8, i\\xe2\\x80\\xb2k)N\\xe2\\x80\\xb2\\nand (crei((e, \\xef\\xbf\\xbd)k)N\\xe2\\x80\\xb2\\xe2\\x80\\xb2 \\xe2\\x88\\xa8 add((e, \\xef\\xbf\\xbd), i\\n\\n\\xe2\\x80\\xb2\\xe2\\x80\\xb2\\nk)N\\xe2\\x80\\xb2\\xe2\\x80\\xb2 ) (10)\\n\\nwhy(bN , T ) =crei((e,+b))N\\xe2\\x80\\xb2 because why(crei((e,+b))N\\xe2\\x80\\xb2 , T )\\n\\xe2\\x88\\xa8 addb(b, start) (11)\\n\\nwhy(!gN , T ) =add((+!g, \\xef\\xbf\\xbd), i)N\\xe2\\x80\\xb2 because why(add((+!g, \\xef\\xbf\\xbd), i)N\\xe2\\x80\\xb2 , T )\\n\\xe2\\x88\\xa8 crei((e,+!g), i)N\\xe2\\x80\\xb2 because why(crei((e,+!g), i)N\\xe2\\x80\\xb2 , T )\\n\\n(12)\\nwhy(add((+!g, \\xef\\xbf\\xbd), ik)N , T ) =selp((e, ds), i\\xe2\\x80\\xb2k)N\\xe2\\x80\\xb2 because\\n\\nwhy(selp((e, ds), i\\xe2\\x80\\xb2k)N\\xe2\\x80\\xb2 , T ) \\xe2\\x88\\xa7+!g \\xe2\\x88\\x88 ds\\n\\xe2\\x88\\xa8 crei((e,+!g)k)N\\xe2\\x80\\xb2 because why(crei((e,+!g)k)N\\xe2\\x80\\xb2 , T )\\n\\n(13)\\nwhy(act(a, ik)N , T ) =selp((e, ds), i\\xe2\\x80\\xb2k)N\\xe2\\x80\\xb2 because\\n\\nwhy(selp((e, ds), i\\xe2\\x80\\xb2k)N\\xe2\\x80\\xb2 , T ) \\xe2\\x88\\xa7 a \\xe2\\x88\\x88 ds (14)\\nwhy(crei((start, d))N , T ) =start (15)\\n\\nwhy(crei((percept, d))N , T ) =percept (16)\\nwhy(crei((e, \\xef\\xbf\\xbd)k)N , T ) =selp((e, ds), ik)N\\xe2\\x80\\xb2 because\\n\\nwhy(selp((e, ds), ik)N\\xe2\\x80\\xb2 , T ) \\xe2\\x88\\xa7 e \\xe2\\x88\\x88 ds (17)\\n\\nFig. 2. Why questions in Gwendolen\\n\\nbel((e, ds), g, \\xce\\xb8, i\\xe2\\x80\\xb2k)N \\xe2\\x80\\xb2 means that N\\n\\xe2\\x80\\xb2 is the most recent step in the trace before\\n\\nN where a matching event occurs. Moreover we need the previous events to\\nhave created or manipulated the same intention (and Gwendolen\\xe2\\x80\\x99s intention\\nselection mechanism means that other intentions may have been manipulated in\\nbetween selecting a plan and creating the intention) so we track the intention\\nID number, k, to ensure we are considering the events occurring in the correct\\nintention.\\n\\n(11) and (12) ask why something is believed at step N or is a goal at step N\\nand the answer in the first case is that at some previous step an intention was\\ncreated to add the belief or it was an initial belief, and in the second case that\\neither the goal event was posted to an intention or an intention was created to\\nadd the goal (as happens with the initial goals).\\n\\n(13) asks why some goal was posted to the top of an intention, this is either\\nbecause a plan was selected previously in the intention which included the post-\\ning of the goal as a deed (+!g \\xe2\\x88\\x88 ds) or because an intention was created to post\\nthe goal.\\n\\nAn action (14) is taken because some plan was selected that included the\\naction in its deeds.\\n\\nThere are three reasons why an intention may have been created: because it\\nis an initial goal or belief (15), because something was perceived (16), or because\\na plan was selected which included posting the event in its deed stack.\\n\\n\\n\\nTitle Suppressed Due to Excessive Length 11\\n\\nOur framework is similar in conception and presentation to that in [33]. We\\nhave in general introduced a pattern of e because why(e, T ) where [33] normally\\nonly has why(e, T ). This is motivated in part by our focus on end users instead of\\nprogrammers: we think it important that explanations mention that \\xe2\\x80\\x9ca plan was\\nselected which contained the deed\\xe2\\x80\\x9d (which is how we render selp((e, ds), i)\\xe2\\x88\\xa7(d \\xe2\\x88\\x88\\nds) into natural language) and then state why that plan was selected. Winikoff\\n[33] leaves this implicit and the explanation is only for why the plan was selected.\\nWinikoff [33] is also concerned with a number of other events in a trace \\xe2\\x80\\x93 for\\ninstance where some deed is not the first to be executed in the body of a plan,\\nthen part of the explanation for the its execution includes that the previous deeds\\nwere successful. We have taken the view that end users will not generally consider\\n\\xe2\\x80\\x9cand the parts of the plan before this succeeded\\xe2\\x80\\x9d as part of an explanation,\\nthough we may well need to incorporate aspects of this when we look at why-\\nnot-questions (i.e., something may not have happened because a previous deed\\nfailed).\\n\\nOther differences arise from differences between the semantics of Gwen-\\ndolen and AgentSpeak.\\n\\n3.5 Natural Language Presentation\\n\\nTo make the printing of reasons (and thus also events) suitable for end-users,\\nadditional steps are needed. First, we propose a predicate dictionary that pro-\\nvides a mapping of predicates to strings (e.g., \\xe2\\x80\\x98state(X)\\xe2\\x80\\x99 to \\xe2\\x80\\x98the robot is in state\\nX\\xe2\\x80\\x99). Second, an internal translation of specific programming symbols is required\\n(e.g., \\xe2\\x80\\x99+!\\xe2\\x80\\x99 to \\xe2\\x80\\x98added the goal\\xe2\\x80\\x99).\\n\\n3.6 Implementation\\n\\nThe AIL is implemented in Java. Therefore we were able to create an abstract\\nclass for events and a framework for storing and presenting visualisations based\\non the work in [18]. We were then able to create specific event types for the\\nevents of interest. We extended the visualiser with an interface to allow why-\\nquestions to be asked \\xe2\\x80\\x93 specifically \\xe2\\x80\\x9cwhy did you perform this action?\\xe2\\x80\\x9d, \\xe2\\x80\\x9cwhy\\ndid you hold this belief?\\xe2\\x80\\x9d and \\xe2\\x80\\x9cwhy did you have this goal?\\xe2\\x80\\x9d.\\n\\nWe then constructed a specific explanation mechanism for the Gwendolen\\nlanguage based on the algorithm in Figure 2. The details of this required some\\nadditional events to be tracked \\xe2\\x80\\x93 for instance in order to correctly identify the\\nplan which caused some deed to be placed on an intention it was necessary\\nto extract information both from the generation of plans in stage B and the\\nselection of a plan from the set in stage C of the Gwendolen reasoning cycle 9.\\n\\nThe AIL comes with a flexible configuration mechanism based on that in\\nJavaPathFinder [28]. We adapted the underlying AIL code to store events in a\\ntrace if tracing was enabled.\\n9 We omit the gory details here, instructions for accessing the source code can be\\nfound in section 6.\\n\\n\\n\\n12 Authors Suppressed Due to Excessive Length\\n\\nWe also implemented a pretty printing mechanism in the AIL which allowed\\nlanguages to customise a pretty printer for the presentation of traces. These\\npretty printers could be further customised by the use of application specific\\ndictionaries specifying natural language substitutions for predicates appearing\\nboth traces and explanations.\\n\\nThis gave us a flexible and extensible framework for implementing omniscient\\ndebugging in order to enable why-questions in AIL languages.\\n\\n4 Evaluation\\n\\nOur current implementation is a prototype only so a full evaluation has yet to\\nbe undertaken. However, it is possible to present initial results.\\n\\n4.1 Traces in Gwendolen for Tutorial examples\\n\\nThe AIL comes with an extensive set of examples based on tutorials for the\\nframework, the Gwendolen language, and the AJPF model-checker [10]. We\\nused these as an ongoing driver for development of our framework \\xe2\\x80\\x93 in particular\\nto help settle on appropriate pretty printing conventions. Figure 3 shows part\\nof a pretty printed version of the event trace for one of these examples as it is\\nconstructed 10. Our visualiser for traces is shown in Figure 4.\\n\\nselected Intention 1: add the goal achieve \"the robot is holding rubble\".\\nconfirmed Intention 1: add the goal achieve \"the robot is holding rubble\"\\ncan still be processed.\\ngenerated 1 applicable plan(s): continue processing: add the goal achieve\\n\"the robot is holding rubble\" for an event.\\nselected continue processing: add the goal achieve \"the robot is holding\\nrubble\".\\nadded achieve \"the robot is holding rubble\" to the agent\\xe2\\x80\\x99s goals.\\nmodified intention by posting an event to become Intention 1: respond to\\nthe event start which has no plan yet AND add the goal achieve \"the robot\\nis holding rubble\".\\nselected Intention 1: respond to the event start which has no plan yet\\nAND add the goal achieve \"the robot is holding rubble\".\\n\\nFig. 3. A Pretty Printed Event Trace for a Gwendolen program\\n\\n10 NB. It is generally accepted that end users prefer natural language presentations\\nwhile developers often prefer something more compact so this log presents the events\\nwith end users in mind, though it remains much more verbose than is required for\\nan explanation.\\n\\n\\n\\nTitle Suppressed Due to Excessive Length 13\\n\\nFig. 4. Trace Visualiser\\n\\n4.2 Explanations for Gwendolen for Tutorial examples\\n\\nFigure 5 shows an example explanation (for why the robot performed the action\\nlift_rubble). Sections of the explanation underlined in blue can be clicked on\\nto generate further explanations \\xe2\\x80\\x93 in this case for \\xe2\\x80\\x9cwhy do you believe at(5, 5)\\xe2\\x80\\x9d.\\n\\n4.3 Potential Use Case: Self-certifying Offshore Assets\\n\\nIn order to validate our intuitions about appropriate explanations for end users\\nwe investigated a prototype agent program for surveying offshore assets such as\\noil rigs and wind farms [12, 13]. This agent guides an unmanned aircraft that\\nmust select a suitable path between the legs of an oil rig based on wind speed,\\nwind direction, and perceived tolerance to risks. Guided by the developers, we\\nproduced a predicate dictionary for the application which converted the pro-\\ngram\\xe2\\x80\\x99s internal representation into natural language \\xe2\\x80\\x93 e.g., enactRoute(route2,\\nt2) becomes enact route2 with target t2 and so on.\\n\\nA sample explanation is shown in Figure 6\\nThese explanations were shown to the developers who confirmed that they\\n\\nprovided explanations of use to their end users (considered to be experts in\\nunmanned aircraft operation and offshore asset inspection), though obviously\\nfurther work is needed on the presentation (e.g., performing unifications rather\\nthan showing the unifier).\\n\\n\\n\\n14 Authors Suppressed Due to Excessive Length\\n\\nFig. 5. Generating an Explanation\\n\\nFig. 6. Explanation of Route Selection\\n\\n\\n\\nTitle Suppressed Due to Excessive Length 15\\n\\n4.4 Traces and Explanations for Other Languages\\n\\nTo evaluate our claim that tracing in the AIL is generic we enabled tracing for\\nanother language implemented in the AIL, without any further customisation\\nfor the language. The language selected was pbdi [4], a reimplementation of a\\nBDI library for Python11 intended to allow agents written using the library to be\\nverified. We generated an omniscient trace for a simple program in this language\\n(one which stops the operation of a small Pi2Go robot using a command done\\nwhen the switch on the side of the robot is pressed). A sample trace is shown in\\nFigure 7.\\n\\nadded obstacle_left to the agent\\xe2\\x80\\x99s beliefs.\\nadded switch_pressed to the agent\\xe2\\x80\\x99s beliefs.\\nevaluating the guard of 1 :: +!_aAny||True||done||[]\\n1 :: +!_aAny||switch_pressed||print(\"Stopping Agent\")||[]\\nresulted in True.\\n\\nevaluating the guard of 2 :: +!_aAny||True||print(\"Obstacle: \",agent.sensor_value(\"obstacle_centre\"))||[]\\nresulted in True.\\n\\ngenerated 2 applicable plan(s): 1 :: +!_aAny||True||done||[]\\n1 :: +!_aAny||switch_pressed||print(\"Stopping Agent\")||[]\\n, 2 :: +!_aAny||True||print(\"Obstacle: \",agent.sensor_value(\"obstacle_centre\"))||[]\\nfor an event.\\n\\nselected 1 :: +!_aAny||True||done||[]\\n1 :: +!_aAny||switch_pressed||print(\"Stopping Agent\")||[]\\n.\\ncreated [empty]::\\n\\n* +state||True||print(\"Stopping Agent\")||[]\\n+state||True||done||[]\\n\\n.\\nselected [empty]::\\n\\n* +state||True||print(\"Stopping Agent\")||[]\\n+state||True||done||[]\\n\\n.\\nStopping Agent\\nperformed print(\"Stopping Agent\").\\n\\nperformed done.\\nremoved obstacle_left from the agent\\xe2\\x80\\x99s beliefs.\\nremoved switch_pressed from the agent\\xe2\\x80\\x99s beliefs.\\n\\nFig. 7. A Sample Trace for BDIPython\\n\\nAs can be seen, the lack of language specific pretty printing for plans renders\\nthis less readable, but nevertheless a clear trace has been generated of the key\\nevents in the execution of the program.\\n11 https://github.com/VerifiableAutonomy/BDIPython\\n\\n\\n\\n16 Authors Suppressed Due to Excessive Length\\n\\n5 Conclusion\\n\\nWe sought to combine omniscient debugging and answering why-questions for\\ncognitive agent programs in order to generate explanations for end users. To do\\nthis we ported omniscient debugging to the AIL toolkit and thus demonstrated\\nits general applicability beyond the GOAL language for which it was developed.\\n\\nOn top of the traces generated by the omniscient debugger we were able to\\nconstruct explanations for programs in the Gwendolen language. To do this we\\nextended work by [16] and [33] aimed at answering why-questions for developers.\\nThis extension involved adding the capability to handle multiple intentions via\\nthe tracking of intention IDs and the use of pretty printing and application\\nspecific dictionaries to render explanations into natural language.\\n\\nWhile this prototype system has yet to be formally evaluated, informal feed-\\nback suggests that the end user explanations are appropriate for the intended\\npurpose. A major piece of further work will involve integration of the frame-\\nwork into an application being developed for offshore inspection of oil rigs and\\nwind farms [12, 13] and the evaluation of the generated explanations by the\\napplication\\xe2\\x80\\x99s users. Work is also needed to integrate the answering of why-not-\\nquestions into the framework in order to provide constrastive explanations as\\ndiscussed in [19] which argues that why-questions answer counter-factuals. Mi-\\nnor work focused on improving the presentation of traces and explanations is also\\nrequired. We would also like to investigate the use of tracing/explanation levels\\nanalogous to the logging levels used by Java in order to increase the flexibility\\nof the provided explanations allowing users to \\xe2\\x80\\x9cdrill down\\xe2\\x80\\x9d into more detail if\\nthe provided explanation does not meet their needs.\\n\\n6 Open Data Statement\\n\\nThe source code for the AIL is available from http://mcapl.sourceforge.net\\nwhere the work in this paper can be found in the omniscient branch of the\\ngit repository. The version discussed here will be archived in the University of\\nLiverpool data catalog in due course.\\n\\nAcknowledgments\\n\\nThis research was partially funded by EPSRC grants Verifiable Auton-\\nomy (EP/LO24845/1) and the Offshore Robotics for Certification of Assets\\n(EP/RO26173) Robotics and Artificial Intelligence Hub.\\n\\n\\n\\nTitle Suppressed Due to Excessive Length 17\\n\\nReferences\\n\\n1. Bordini, R.H., H\\xc3\\xbcbner, J.F., Wooldridge, M.: Programming Multi-agent Sys-\\ntems in AgentSpeak Using Jason. John Wiley & Sons (2007)\\n\\n2. Bordini, R.H., Dastani, M., Dix, J., El Fallah-Seghrouchni, A. (eds.): Multi-\\nAgent Programming: Languages, Platforms and Applications. Springer\\n(2005)\\n\\n3. Bratman, M.E.: Intentions, Plans, and Practical Reason. Harvard University\\nPress (1987)\\n\\n4. Bremner, P., Dennis, L.A., Fisher, M., Winfiled, A.F.: On proactive, trans-\\nparent and verifiable ethical reasoning for robots. Proceedings of the IEEE\\nspecial issue on Machine Ethics: The Design and Governance of Ethical AI\\nand Autonomous Systems (2019), to Appear\\n\\n5. Chakraborti, T., Sreedharan, S., Zhang, Y., Kambhampati, S.: Plan expla-\\nnations as model reconciliation: Moving beyond Explanation as Soliloquy. In:\\nsubbarao Kambhampati (ed.) Proceedings of the Twenty-Sixth International\\nJoint Conference on Artificial Intelligence. pp. 156\\xe2\\x80\\x93163 (2016)\\n\\n6. Charisi, V., Dennis, L.A., Fisher, M., Lieck, R., Matthias, A., Slavkovik,\\nM., Sombetzki, J., Winfield, A.F.T., Yampolskiy, R.: Towards moral au-\\ntonomous systems. CoRR abs/1703.04741 (2017), http://arxiv.org/abs/\\n1703.04741\\n\\n7. Dastani, M., van Riemsdijk, M.B., Meyer, J.J.C.: Programming multi-agent\\nsystems in 3APL. In: [2], chap. 2, pp. 39\\xe2\\x80\\x9367\\n\\n8. Dennis, L., Fisher, M., Webster, M., Bordini, R.: Model check-\\ning agent programming languages. Automated Software Engineer-\\ning pp. 1\\xe2\\x80\\x9359 (2011), http://dx.doi.org/10.1007/s10515-011-0088-x,\\n10.1007/s10515-011-0088-x\\n\\n9. Dennis, L.A.: Gwendolen semantics: 2017. Tech. Rep. ULCS-17-001, Uni-\\nversity of Liverpool, Department of Computer Science (2017)\\n\\n10. Dennis, L.A.: The MCAPL Framework including the Agent Infrastructure\\nLayer and Agent Java Pathfinder. The Journal of Open Source Software\\n3(24) (2018)\\n\\n11. Dennis, L.A., Bordini, R.H., Farwer, B., Fisher, M., Wooldridge, M.: A\\ncommon semantic basis for BDI languages. In: Fifth International Work-\\nshop on Programming in Multi-Agent Systems (ProMAS\\xe2\\x80\\x9907). Lecture Notes\\nin Artificial Intelligence, vol. 4908, pp. 124\\xe2\\x80\\x93139. Springer (2008), http:\\n//dx.doi.org/10.1007/978-3-540-79043-3_8\\n\\n12. Dinmohammadi, F., Page, V., Flynn, D., Robu, V., Fisher, M., Patchett, C.,\\nJump, M., Tang, W., Webster, M.: Certification of safe and trusted robotic\\ninspection of assets. In: 2018 Prognostics and System Health Management\\nConference (PHM-Chongqing). pp. 276\\xe2\\x80\\x93284 (Oct 2018)\\n\\n13. Fisher, M., Collins, E., Dennis, L., Luckcuck, M., Webster, M., Jump, M.,\\nPage, V., Patchett, C., Dinmohammadi, F., Flynn, D., Robu, V., Zhao, X.:\\nVerifiable self-certifying autonomous systems. In: 2018 IEEE International\\nSymposium on Software Reliability Engineering Workshops (ISSREW). pp.\\n341\\xe2\\x80\\x93348 (Oct 2018)\\n\\n\\n\\n18 Authors Suppressed Due to Excessive Length\\n\\n14. Harbers, M.: Explaining Agent Behaviour in Virtual Training. Ph.D. thesis,\\nSIKS Dissertation Series (2011), no. 2011-35\\n\\n15. Hindriks, K.V.: Programming rational agents in GOAL. In: El Fal-\\nlah Seghrouchni, A., Dix, J., Dastani, M., Bordini, R.H. (eds.) Multi-Agent\\nProgramming: Languages, Tools and Applications. pp. 119\\xe2\\x80\\x93157. Springer\\nUS, Boston, MA (2009)\\n\\n16. Hindriks, K.V.: Debugging is explaining. In: Rahwan, I., Wobcke, W., Sen,\\nS., Sugawara, T. (eds.) PRIMA 2012: Principles and Practice of Multi-Agent\\nSystems. pp. 31\\xe2\\x80\\x9345. Springer Berlin Heidelberg, Berlin, Heidelberg (2012)\\n\\n17. Ko, A.J., Myers, B.A.: Extracting and answering why and why not questions\\nabout Java program output. ACM Transactions on Sotware Engineering and\\nMethodology 20(2), 4:1\\xe2\\x80\\x934:36 (2010)\\n\\n18. Koeman, V.J., Hindriks, K.V., Jonker, C.M.: Omniscient debugging for cog-\\nnitive agent programs. In: Proceedings of the 26th International Joint Con-\\nference on Artificial Intelligence. pp. 265\\xe2\\x80\\x93272. IJCAI\\xe2\\x80\\x9917, AAAI Press (2017)\\n\\n19. Miller, T.: Explanation in artificial intelligence: Insights from the social sci-\\nences. Artificial Intelligence 267, 1\\xe2\\x80\\x9338 (Feb 2017)\\n\\n20. Pokahr, A., Braubach, L., Lamersdorf, W.: Jadex: A BDI reasoning engine.\\nIn: [2], pp. 149\\xe2\\x80\\x93174\\n\\n21. Rao, A.S., Georgeff, M.P.: Modeling agents within a BDI-architecture. In:\\nProc. 2nd Int. Conf. Principles of Knowledge Representation and Reasoning\\n(KR&R). pp. 473\\xe2\\x80\\x93484. Morgan Kaufmann (1991)\\n\\n22. Rao, A.S., Georgeff, M.P.: An abstract architecture for rational agents. In:\\nProc. Int. Conf. Knowledge Representation and Reasoning (KR&R). pp.\\n439\\xe2\\x80\\x93449. Morgan Kaufmann (1992)\\n\\n23. Rao, A.S., Georgeff, M.P.: BDI agents: From theory to practice. In: Proc.\\n1st Int. Conf. Multi-Agent Systems (ICMAS). pp. 312\\xe2\\x80\\x93319. San Francisco,\\nUSA (1995)\\n\\n24. Rao, A.: AgentSpeak(L): BDI agents speak out in a logical computable lan-\\nguage. In: Agents Breaking Away: Proc. 7th European Workshop on Mod-\\nelling Autonomous Agents in a Multi-Agent World. LNCS, vol. 1038, pp.\\n42\\xe2\\x80\\x9355. Springer (1996)\\n\\n25. Sheh, R.K.: \\xe2\\x80\\x9cWhy did you do that?\\xe2\\x80\\x9d explainable intelligent robots. In:\\nAAAI-17 Workshop on Human-Aware Artificial Intelligence (2017)\\n\\n26. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Sys-\\ntems: Ethically aligned design: A vision for prioritizing human well-being\\nwith autonomous and intelligent systems. version 2. Report, IEEE (2017)\\n\\n27. Turner, J.: Robot Rules: Regulating Artificial Intelligence. Palgrave Macmil-\\nlan (2019)\\n\\n28. Visser, W., Havelund, K., Brat, G.P., Park, S., Lerda, F.: Model checking\\nprograms. Automated Software Engineering 10(2), 203\\xe2\\x80\\x93232 (2003)\\n\\n29. Webster, M.P., Fisher, M., Cameron, N., Jump, M.: Formal methods for the\\ncertification of autonomous unmanned aircraft systems. In: Proc. 30th Int.\\nConf. Computer Safety, Reliability and Security (SAFECOMP). LNCS, vol.\\n6894, pp. 228\\xe2\\x80\\x93242. Springer (2011)\\n\\n\\n\\nTitle Suppressed Due to Excessive Length 19\\n\\n30. Wei, C., Hindriks, K.V.: An agent-based cognitive robot architecture. In:\\nProgramming Multi-Agent Systems, LNCS, vol. 7837, pp. 54\\xe2\\x80\\x9371. Springer\\n(2013)\\n\\n31. Winikoff, M., Cranefield, S.: On the testability of BDI agent systems. Journal\\nof Artificial Intelligence Research 51 (2015)\\n\\n32. Winikoff, M.: BDI agent testability revisited. Autonomous Agents and\\nMulti-agent Systems 31(1094) (2017)\\n\\n33. Winikoff, M.: Debugging agent programs with Why? questions. In: Proceed-\\nings of the 16th Conference on Autonomous Agents and MultiAgent Systems.\\npp. 251\\xe2\\x80\\x93259. AAMAS \\xe2\\x80\\x9917, International Foundation for Autonomous Agents\\nand Multiagent Systems, Richland, SC (2017)\\n\\n34. Winikoff, M., Dignum, V., Dignum, F.: Why bad coffee? explaining agent\\nplans with valuings. In: Skavhaug, A., Guiochet, J., Schoitsch, E., Bitsch,\\nF. (eds.) SAFECOMP. LNCS, vol. 9923, pp. 521\\xe2\\x80\\x93534. Spinger (2016)\\n\\n35. Wooldridge, M.: An Introduction to Multiagent Systems. John Wiley &\\nSons (2002)\\n\\n36. Wooldridge, M., Rao, A. (eds.): Foundations of Rational Agency. Applied\\nLogic Series, Kluwer Academic Publishers (1999)\\n\\n37. Wortham, R.H., Theodorou, A.: Robot transparency, trust and utility. Con-\\nnection Science 29(3), 24200247 (2017)\\n\\n38. Zeller, A.: Why Programs Fail, Second Edition: A Guide to Systematic\\nDebugging. Morgan Kaufmann Publisheres Inc., San Francisco, CA, USA\\n(2009)\\n\\n39. Ziafati, P., Dastani, M., Meyer, J.J., van der Torre, L.: Agent programming\\nlanguages requirements for programming autonomous robots. In: Dastani,\\nM., H\\xc3\\xbcbner, J.F., Logan, B. (eds.) Programming Multi-Agent Systems. pp.\\n35\\xe2\\x80\\x9353. Springer Berlin Heidelberg, Berlin, Heidelberg (2013)\\n\\n\\n'