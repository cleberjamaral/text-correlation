b'\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFrom Programming Agents to Educating Agents \\xe2\\x80\\x93 A\\nJason-based Framework for Integrating Learning in the\\n\\nDevelopment of Cognitive Agents\\n\\nMichael Bosello and Alessandro Ricci\\n\\nAlma Mater Studiorum \\xe2\\x80\\x93 Universit\\xc3\\xa0 di Bologna\\nDepartment of Computer Science and Engineering, Cesena Campus\\nmichael.bosello@studio.unibo.it, a.ricci@unibo.it\\n\\nAbstract. Recent advances and successes of machine learning techniques are\\npaving the way to what is referred as Software 2.0 era and cognitive computing,\\nin which traditional programming and software development is meant to be re-\\nplaced by such techniques for many applications. If we consider agent-oriented\\nprogramming, we believe that such developments trigger new interesting scenarios\\nblending cognitive architecture such as the BDI one and techniques like Rein-\\nforcement Learning (RL) even more deeply compared to what has been proposed\\nso far in the literature. In that perspective, we aim at exploring the integration of\\ncognitive agent-oriented programming based on BDI with learning techniques so\\nas to systematically exploit them in the agent development stage. The approach\\nshould support the design of BDI agents in which some plans can be explicitly\\nprogrammed and others instead can be learned by the agent during the developmen-\\nt/engineering stage. In that view, the development of an agent is metaphorically\\nsimilar to an education process, in which first an agent is created with a set of\\nbasic programmed plans and then grow up in order to learn plans to achieve the\\ngoals for which the agent is meant to be designed. This paper present and discuss\\nthis medium-term view, introducing a first model for a BDI agent programming\\nframework integrating RL, a first implementation based on Jason programming\\nlanguage/platform and sketching a roadmap for this research line.\\n\\n1 Introduction\\n\\nMachine learning and cognitive computing techniques have been getting a momentum\\nin recent years, thanks to several factors, including theoretical developments in contexts\\nsuch as (deep) neural networks, reinforcement learning, Bayesian networks, the avail-\\nability of big data and the availability of more and more powerful parallel computing\\nmachinery (GPU, cloud) [2,12,13,16]. Their deeper and deeper impact in real-world ap-\\nplications is celebrating a new \\xe2\\x80\\x9cAI Spring\\xe2\\x80\\x9d era, which is generating a strong debate in the\\nliterature as well [19]. Actually, the impact is not only about applications but also about\\nhow applications are programmed and engineered. In particular, a vision of Software 2.0\\nera is emerging [17], in which traditional programming and software development is\\nmeant to be more and more replaced by e.g. machine learning and cognitive computing\\ntechniques, towards the \\xe2\\x80\\x9cthe end of programming\\xe2\\x80\\x9d era [16,10].\\n\\nlouisedennis\\nPlaced Image\\n\\n\\n\\n2 Michael Bosello and Alessandro Ricci\\n\\nBesides the hype and the marketing-oriented claims, if we consider agent-oriented\\nprogramming [23], and \\xe2\\x80\\x93 more generally, multi-agent systems (MAS) engineering \\xe2\\x80\\x93 we\\nbelieve that such recent developments would trigger new interesting scenarios blending\\ncognitive architectures such as the BDI one [21] and techniques like Reinforcement\\nLearning (RL) [27] even more deeply than what has been already proposed so far in liter-\\nature. As far as authors\\xe2\\x80\\x99 knowledge, existing research integrating BDI-based agents and\\nMAS with learning techniques mainly focused on improving agent adaptation, exploiting\\nlearning to improve e.g. plan or action selection at runtime. As a further approach, our\\nobjective is to explore the integration of cognitive agent-oriented programming based\\non BDI with learning techniques so as to systematically exploit them in the agent de-\\nvelopment stage. The basic idea is that an agent developer could integrate the explicit\\nprogramming of plans \\xe2\\x80\\x93 when developing a BDI agent \\xe2\\x80\\x93 with the possibility that, for\\nsome goals, it would be the agent itself to learn the plans to achieve them, by exploiting\\nreinforcement learning based techniques. This is not only for a specific ad hoc problem,\\nbut as a general feature of the agent platform.\\n\\nIn that view, the development of an agent is metaphorically similar to an education\\nprocess, in which first an agent is created with a set of basic programmed plans and then\\ngrow up in order to learn plans to achieve the goals for which the agent is meant to be\\ndesigned. We believe that this vision would trigger interesting research directions about\\nthe evolution of agent-oriented programming in the software 2.0 era. In the remainder\\nof the paper, we present and discuss this view, with a first proof-of-concept framework\\nbased on the Jason agent platform [6,7].\\n\\nAfter giving a background and an account about related works (section 2), we first\\ndescribe our approach integrating learning in the loop of BDI agent programming, using\\nAgentSpeak(L) [20] as reference model (section 3). Then, we describe an implementation\\nof the framework on top of the Jason agent language/platform (section 4), including\\nsome testing using a simple example and a discussion (section 5). We close the paper\\nsketching a roadmap for this research line (section 6).\\n\\n2 Background and Related Works\\n\\nIn this section, we first provide an overview about basic concepts of Reinforcement\\nLearning (RL) \\xe2\\x80\\x93 taking [27] as main reference \\xe2\\x80\\x93 and then an account of existing research\\nworks in the context of agent-oriented programming \\xe2\\x80\\x93 especially focusing on BDI-based\\nmodel [21] \\xe2\\x80\\x93 proposing an integration with RL. It is worth remarking that in this setting\\nwe do not intend to consider the latest advances in RL, but just the core foundational\\nlayer useful to present and discuss our approach.\\n\\n2.1 Reinforcement Learning\\n\\nReinforcement Learning (RL) is a machine learning method with the key idea that a\\ngoal-oriented agent learns how to fulfill a task by interacting with its environment.\\n\\nAny problem of learning goal-directed behavior can be reduced to three signals\\npassing back and forth between an agent and its environment: one signal to represent the\\n\\n\\n\\nFrom Programming Agents to Educating Agents 3\\n\\nchoices made by the agent (the actions), one signal to describe the basis on which the\\nchoices are made (the states), and one signal to define the agent\\xe2\\x80\\x99s goal (the rewards).\\n\\nA state is defined as whatever information is available to the agent about its environ-\\nment, some of what makes up a state could be based on the memory of past perceptions\\nor even be entirely mental or subjective, i.e. the states can be anything we can know that\\nmight be useful in the decision process. The agent must decide what action to take as a\\nfunction of whatever state signal is available. The actions too might be either internal \\xe2\\x80\\x93\\nchanging the agent\\xe2\\x80\\x99s mental state \\xe2\\x80\\x93 or external \\xe2\\x80\\x93 affecting the environment. For example,\\nsome actions might control what an agent chooses to think about, or where it focuses its\\nattention. The reward is a single real number obtained at each interaction step that the\\nagent seeks to maximize over time. The reward signal thus defines what the good and\\nbad events for the agent are, i.e. it is your way of communicating to the agent what it\\nmust achieve.\\n\\nWe refer to each successive stages of interaction between the agent and the environ-\\nment as time steps. In the case of a BDI agent, a reasoning cycle can be pretty assumed\\nas a step. In some applications, there is a natural notion of final time step, that is, when\\nthe agent-environment interaction breaks naturally into subsequences that are referred\\nas episodes. Related to BDI, this is the case of agents pursuing achievement goals. In\\nmany cases, the agent-environment interaction does not break naturally into identifiable\\nepisodes but goes on continually without limit \\xe2\\x80\\x94 these are called continuing tasks\\xe2\\x80\\x94i.e.,\\nmaintenance goals in the BDI case.\\n\\nThe agent learns a policy as a result of the learning process. A policy is a function\\nthat maps states to actions. we seek to learn and exploit an optimal policy, but we need\\nto behave non-optimally to explore all the possibilities. A classic method to balance the\\nexploitation and exploration phases is to use an \\xce\\xb5-greedy policy with which the agent\\nbehave greedily but there is a (small) \\xce\\xb5 probability to select a random action.\\n\\nIn many cases of interest, the agent has only partial information about the state of\\nthe world, so, the states signal is replaced by an observations signal that depend on the\\nenvironment state but provide only partial information about it. In the BDI case, this\\nis directly modeled by percepts and, therefore, by beliefs about the environment. The\\nfundamental property of a state, known as the Markov property, is that it can be used\\nto predict the future. A stochastic process has the Markov property if the conditional\\nprobability distribution of future states depends only upon the current state, not on\\nthe sequence of events that preceded it. From the observations, the agent recover an\\napproximate state i.e., a state that may not be Markov. Actually, we can partially drop\\nthe Markov property; however, this implies that long-term prediction performance can\\ndegrade dramatically. An approximate state will play the same role in RL algorithms as\\nbefore, so we simply continue to call it a state.\\n\\n2.2 Integrating RL into BDI Agents and AOP\\n\\nGenerally speaking, the integration of learning capabilities has been a main research topic\\nin agents and MAS literature since their roots [29]. A first work providing preliminary\\nresults about integrating learning in BDI multi-agent systems is [14], proposing an\\nextension of the BDI architecture to endow agents with learning skills, based on induction\\n\\n\\n\\n4 Michael Bosello and Alessandro Ricci\\n\\nof logical decision trees. Learning, in that case, is about plan failures, that an agent\\nshould reconsider after its experience.\\n\\nOther works in literature exploits RL to improve plan/action selection capability\\nin BDI agent, making them more adaptive [25,1,24]. In [18] an extension of BDI is\\nproposed so as to get a model of decision making by exploiting the ability to learn to\\nrecognize situations and select the appropriate plan based upon this.\\n\\nIn [3,4] Jason is used to realize Temporal Difference (TD) and SARSA, two rein-\\nforcement learning methods, in order to face the RL problem with a more appropriate\\nparadigm which has been remarkably effective. [28] proposes a hybrid framework\\nthat integrates BDI and TD-FALCON, a neural network based reinforcement learner.\\nBDI-FALCON architecture extends the low-level RL agent with a desire and intention\\nmodules. In this way, the agent has explicit goals in the desire module instead of relying\\non an external signal, enhancing the agent with a higher level of self-awareness. The\\nintention module and its plan library permit to reason about a course of actions instead\\nof individual reactive responses. If there isn\\xe2\\x80\\x99t a plan for a situation, the agent performs\\nthe steps suggested by the FALCON module and, if the sequence succeeds, a new plan is\\ncreated with indications about the starting state, the goal state, and the actions sequence.\\nWhen the agent uses a plan, it updates the confidence of the plan according to the\\noutcome.\\n\\nAlso in [15] a hybrid approach BDI-FALCON is proposed. Here, the focus is on the\\nabstraction level: BDI provides a high-level knowledge representation but lacks learning\\ncapabilities, meanwhile low-level RL agents are more adept at learning. The layered\\nproposal wants to retain both advantages. An alternative vision is provided in [11], where\\na policy is learned and then is used to generate a BDI agent.\\n\\n3 The Basic Idea\\n\\nThe simple idea of this paper is to extend the BDI agent development process with a\\nlearning stage in which we can specify plans in the plan library whose body is not meant\\nto be explicitly programmed but learned, using a learning by experience technique. In so\\ndoing, the development of an agent accounts for: (i) explicitly programming plans as\\nin the traditional way\\xe2\\x80\\x94we will refer to them as hard plans; and (ii) implicitly defining\\nplans by allowing the agent itself to learn them by experience (soft plans). Soft plans are\\nmeant to become part of the plan library like hard plans and can be selected and used at\\nruntime \\xe2\\x80\\x93 in terms of instantiated intentions \\xe2\\x80\\x93 without any difference (but allowing for\\ncontinuous learning, if wanted). Actually, at runtime, soft and hard plans are treated in a\\nuniform way: intentions are created to carry on plan execution, hard plans could trigger\\nthe execution of soft plans and vice versa.\\n\\nTo support the learning stage, we would need to run the agent in a proper environment\\nsupporting this learning by experience, like the simulated environments typically used\\nin RL scenarios. Besides, before deploying the agent, there could be some assessment\\nof the soft plans, eventually using an assessment environment which could be different\\nfrom the one used for training. The assessment is actually very similar to a traditional\\nvalidation stage, including tests that consider the soft plans and their integration with\\n\\n\\n\\nFrom Programming Agents to Educating Agents 5\\n\\nhard plans. If the agent overcomes the assessment, it can be deployed\\xe2\\x80\\x94otherwise, the\\nprocess goes back to the learning stage, possibly changing also plans in the hard part.\\n\\nGiven this general idea, in the remainder of the section we first introduce the model in-\\ntegrating key concepts of RL into a BDI framework, and then we describe an extension of\\nthe BDI reasoning cycle supporting the learning process. We will use AgentSpeak(L) [21]\\nand Jason [6] as concrete abstract and practical BDI-based languages\\xe2\\x80\\x94nevertheless, we\\nbelieve that the core idea is largely independent of the specific BDI agent programming\\nlanguage or framework adopted.\\n\\nTo exemplify the approach, we will use the simple gridworld example (p. 80, [27]),\\nin which an agent is located into a bi-dimensional grid environment, where it has to\\nreach some destination cells without knowing in advance the best path for doing it.\\n\\n3.1 A First Model\\n\\nIn devising the model, we aim at abstracting from the specific RL algorithm that can be\\nused. To that purpose, we consider key common RL concepts \\xe2\\x80\\x93 observations, actions,\\nrewards, episodes \\xe2\\x80\\x93 and how they are represented into a BDI setting (see Fig. 1, top).\\nThese concepts are used by a component \\xe2\\x80\\x93 referred as RL reasoner \\xe2\\x80\\x93 extending the BDI\\nreasoner (interpreter) (see Fig. 1, bottom). The classic BDI reasoner handles the hard\\nplans \\xe2\\x80\\x93 i.e., with a body \\xe2\\x80\\x93 and the RL reasoner handles the soft plans\\xe2\\x80\\x94whose behavior\\nis learned.\\n\\nObservations are modeled as a subset of the agent beliefs that will be used by\\nthe particular algorithm to construct the state, including those that are necessary to\\nunderstand when a goal is achieved. Recall that the observations are a generalization\\nof states and if we want to represent a Markov state we just need to include all its\\naspects as observations. In the gridworld case, for instance, the agent must reach a\\nspecific position moving towards four directions. In this case, the observations include\\nthe current position of the agent (pos(X, Y)) and a belief about having reached the\\ntarget (finish_line)\\xe2\\x80\\x94if this belief is missing, it means that the agent has not achieved\\nthe target in the current state.\\n\\nThe action set can contain both primitive actions (a BDI agent action) and compound\\nactions (a BDI agent plan) so that representing different levels of planning granularity.\\nTo have a common representation for both cases, we represent actions as plans, i.e. the\\nset of actions selectable by the RL reasoner is a subset of the plan library defined by\\nthe plans which are relevant for the goal and applicable in the current context. In Fig.\\n1, these plans are still referred as Actions in the plan library, and as Behavior (i.e., the\\nlearned policy) when instantiated at runtime, wrapped into an intention. In the gridworld\\nexample, the action set has one parametric plan to move the agent in one direction:\\n!move(D), where D could be up, down, left, right.\\n\\nRewards are represented by rules reflecting agent desires\\xe2\\x80\\x94we call them Motivation\\nRules. These rules make it possible to weight the current situation of an agent according\\nto some goal to be achieved. We can see these rules as the generators of internal stimuli in\\nthe agent like a reward signal in neuroscience, which is a signal internal to the brain that\\ninfluences decision making and learning. The signal might be triggered by the external\\nenvironment, but it can also be triggered by things that do not exist outside the agent\\nand which can be represented as beliefs as well. We move the reward, that in RL comes\\n\\n\\n\\n6 Michael Bosello and Alessandro Ricci\\n\\nRL Proposed BDI+ construct Representation in BDI\\nObservations Belief about Learning Belief subset\\n\\nActions Actions Relevant Plans\\nRewards Motivational Rule Belief Rule\\nEpisode Terminal Rule Belief Rule\\nPolicy Behavior Intention\\n\\nFig. 1: (Top) The mapping between RL concepts and their counterpart in the BDI model.\\n(Bottom) A graphic representation of the BDI model with the addition of our constructs.\\n\\nfrom the environment, into the agent. This is crucial to separate the agent desires from\\nthe environment so as to allow an agent to define its own goals and rewards about them.\\nIn the AgentSpeak(L)/Jason model, we can represent Motivational Rules as Prolog-like\\nrules in the belief base:\\n\\nreward(Goal, Reward) :- < some condition over the belief base >\\n\\nIn the gridworld example, the motivational rule will give a positive reward when the\\nfinish line is reached (while pursuing a reach_end goal) and a negative reward in other\\nsteps.\\n\\nreward(reach_end, 10) :- finish_line.\\nreward(reach_end, -1) :- not finish_line.\\n\\nFinally, we must include a notion of episode. An episode is an event or a group of events\\noccurring as part of a sequence. Like in the case of rewards, we can assume that the agent\\n(designer) may have her vision about how to define episodes starting from a relevant\\n\\n\\n\\nFrom Programming Agents to Educating Agents 7\\n\\nFig. 2: BDI practical reasoning extended with RL based on SARSA, in pseudo-code.\\n\\nensemble of situation. This condition is well established by a rule that asserts in which\\nbelief state a coherent group of events ends up in an episode, after which a new episode\\nbegins. We refer to this rule as a Terminal Rule.\\n\\nepisode_end(Goal) :- < some condition over the belief base >\\n\\nIn the gridworld example, the episode for achieving a reach_end goal ends when the\\nfinish line is reached:\\n\\nepisode_end(reach_end) :- finish_line.\\n\\nIt is worth noting that this approach could be applied only in the case of achievement\\ngoals. In the case of maintenance goals (continuing tasks in [27]) we would need to\\nreconsider how an episode is modeled.\\n\\n3.2 Extending the Reasoning Cycle\\n\\nIn our framework, a BDI agent is then equipped with general-purpose learning capabili-\\nties that are triggered as soon as a soft plan must be learned, for some goal. Fig. 2 shows\\n\\n\\n\\n8 Michael Bosello and Alessandro Ricci\\n\\nthe pseudo code of a classic BDI agent reasoning cycle (as defined in [30,7]) extended\\nwith such learning capabilities The RL algorithm used in the example is SARSA, adapted\\nfor the context; our additions are in red.\\n\\nIn the standard cycle, the function plan in line (11) generates a plan to achieve\\nthe intention I, we consider that this function is extended to include the case in which\\nthe agent, for any reason, is not able to produce a plan to pursue the intention. In this\\ncase, the agent can choose to instantiate a soft plan that relies on RL to achieve the\\nintention. The execution loop condition remains the same for the hard plans; instead, a\\nsoft plan continues until a terminal state is met or until the intention succeeds or becomes\\nimpossible to reach.\\n\\nThe execution of soft plans is between (16) and (25). First, the agent extracts the\\nobservations from the belief base according to the goal to achieve (16), then, a state is\\nbuilt from these observations (17). The reward is obtained from the motivational rule\\nthat quantifies the fulfillment of the goal in accordance with the current beliefs (18). The\\naction is selected following the RL policy and the current value function Q (19). In (21)\\nthe value-action function is updated. Finally, the selected action is carried out (25).\\n\\n4 Proof-of-Concept Implementation in Jason\\n\\nWe developed a first proof-of-concept implementation on top of Jason, exploiting its\\nextensibility. Knowledge required by the RL part is uniformly represented by specific\\nbeliefs, referred as beliefs about learning. The framework abstracts from the specific RL\\nalgorithm but, depending on the characteristics of the problem, there will be different\\nconstraints on it. Critical factors are the knowledge about the environment and the\\nstate/action space dimensionality: if the state/action space dimensionality increases or\\nmore environment features are hidden (Markov property), then the constraints on the\\nalgorithm will be more stringent. To deal with this problem, we consider the possibility\\nfor the programmer to specify some domain knowledge so as to reduce state/action space\\nand thus obtaining a more efficient/effective learning.\\n\\nAll the RL items are represented as beliefs, including rules, and plans. In this way,\\nthe agent can control everything related to the reinforcement learning process. For the\\nBDI agent, RL is a black box and vice versa. We can see the black box as a block that\\nwe can change without affecting the agent and that can implement any RL algorithm.\\n\\n4.1 RL Concepts Representation in Jason\\n\\nAll beliefs about learning include as first parameter a ground term representing the\\ngoal for which we want a soft plan, i.e. whose plan is learned. This is useful to support\\nmultiple goals with soft plans at the same time. In the gridworld, for instance, we identify\\nthe goal with reach_end.\\n\\nIn order to reduce the state space, we need to declare which beliefs shall be considered\\nrelevant for a goal, so that they will be used as observations. We do this with beliefs\\nrl_observe(G, O), where G is a ground term that refers to the goal and O is the list\\nof the beliefs that will be considered for the goal. In gridworld example:\\n\\nrl_observe(reach_end, [ pos(_,_) ]).\\n\\n\\n\\nFrom Programming Agents to Educating Agents 9\\n\\nAs introduced in the previous section, Motivation Rules defines the rewards for some\\ngoal given the current context:\\n\\nrl_reward(G, R) :- ...\\n\\nwhere G is the goal and R is a real number. The body of the rule represents the state for\\nwhich this reward must be generated, i.e. it represents the goal state. In the gridworld\\nexample:\\n\\nrl_reward(reach_end, 10) :- finish_line.\\nrl_reward(reach_end, -1) :- not finish_line.\\n\\nAt each execution, the RL reasoner gets the sum of all the rewards of the Motivational\\nRules for which the body is true on the basis of the agent beliefs.\\n\\nSimilarly, Terminal Rules are in the form of rl_terminal(G) :- ..., asserting\\nwhen the end of an episode is reached. In the gridworld example:\\n\\nrl_terminal(reach_end) :- finish_line.\\n\\nThe action set is represented as a set of (hard) plans, identified by an @action anno-\\ntation: @action[rl_goal(g1, ..., gn)] where g1, ..., gn is the list of goals for\\nwhich the plan/action can be used. In the gridworld example:\\n\\n@action[rl_goal(reach_end),\\nrl_param(direction(set(right, left, up, down)))]\\n\\n+!move(Direction) <- move(Direction).\\n\\nThis is used to inform the RL reasoner that the move action, wrapped into the cor-\\nresponding plan, is relevant for learning how to achieve the reach_end goal. The\\nannotation allows for specifying also parameters that are used in the action/plan, specify-\\ning the range of the values: @action[rl_goal(g1, ..., gn), rl_param(p1, ...,\\npm)] where p1, ..., pm is the list of literals whose names match the names of the\\nvariables\\xe2\\x80\\x94these literals must contain a predicate that defines the type of the parameter\\nand its range. To define an action space in which the action set is not the same in all\\nstates we can use the context of the plan\\xe2\\x80\\x94if the context is not satisfied for the current\\nstate, the plan will not be considered by the RL algorithm.\\n\\nRL algorithm parameters can be specified as beliefs, enabling the complete control\\nof the learning process by the programmer and the agent. In the gridworld example,\\nsome parameters are:\\n\\nrl_parameter(alpha, 0.26).\\nrl_parameter(gamma, 0.9).\\nrl_parameter(policy, egreedy).\\n\\nFinally a couple of internal actions \\xe2\\x80\\x93 rl.execute(G) and rl.expectedreturn(G,\\nR) \\xe2\\x80\\x93 are provided to drive and inspect the learning process.\\n\\nThe internal action rl.execute(G) makes it possible to perform one run (episode)\\nof the learning process and/or execute the soft plan. The action, implemented in Java,\\nwraps the core part of the RL algorithm. To that purpose, the Java bridge makes it possible\\n\\n\\n\\n10 Michael Bosello and Alessandro Ricci\\n\\nto reuse existing RL libraries, when useful, including libraries written in other languages\\nsuch as Python ones. The action carries on and improves the soft plan under learning\\nand its execution completes when the episode is completed (or until an action failure).\\nThe soft plan\\xe2\\x80\\x99s intention is carried out like any other intention\\xe2\\x80\\x94so the RL execution\\ncompetes with the other intentions for the agent attention and further execution. Typically,\\na full learning process involves the repeated execution of learning episodes, in this case\\nby executing multiple times the rl.execute action.\\n\\nThe internal action rl.expectedreturn(G, R) gets the estimate of future rewards\\nR for the goal G on the basis of the current state and learned policy, i.e. the expected\\nreturn. This could be used to understand the performance of the leaned soft plan for\\nsome goal given the current situation of the agent. For instance, if the expectation of\\nthe learned behavior in the current state is poor, we can fall back on another plan. As a\\nresult, we obtain a notion of context for soft plans.\\n\\n4.2 Jason-RL Reasoning Cycle\\n\\nThe Jason reasoning cycle defines how the Jason interpreter runs an agent program, it\\ncan be seen as a refinement of the BDI decision loop [7]. There are ten main steps: in\\nour framework, some of these steps are extended to include learning aspects. Figure 3\\nshows our extended architecture based on the original one; the red components are the\\nextensions. The detailed description of the cycle can be found in [7]. In the following,\\nwe focus on our extensions. In a learning agent, after the update of the belief base (2a),\\nthe maps that track the Belief about Learning are updated to reflect the new belief;\\nwe call this process Observation Update Function OUF (2b). In this way, when the\\nobservations are required, the agent doesn\\xe2\\x80\\x99t need to iterate multiple time the belief base.\\nIn step (7a), when the plan\\xe2\\x80\\x99s context is bound to the expected reward, the value is asked\\nto the RL black box and then verified against the threshold (7b). Finally, a new step (11)\\nshall be added to the sequence when the next action of the intention selected in (9) is\\nthe RL execution. At this stage, the information that the RL process needs in order to\\ncontinue shall be provided to it. The observations and the parameters are taken from the\\nbelief base, plus, the Motivational Rules and the Terminal Rules are checked against the\\nbelief base to retrieve the reward and the terminal status. The RL reasoner needs also\\nthe set of relevant actions; this is formed by the action set defined in the plan library\\nafter the non-applicable actions are eliminated through the same check context function\\nof (7). So, the agent provides these data to the RL black box and then obtain the next\\nsuggested action (12). This action is pushed on top of the intention queue and, if the state\\nis not terminal, under this action is put a new call to the RL execution action (13). The\\nnext time this intention is further execute, the selected action will be performed, at the\\nsubsequent intention execution, another action will be requested and so on. Recall that\\nan environment action or an achievement goal suspend the intention until the execution\\nis done, same way internal actions are entirely executed before return. Since that and\\ngiven that the RL selected action is above the next RL step in the same intention, the\\nsubsequent RL execution will never begin before the end of the previous action.\\n\\n\\n\\nFrom Programming Agents to Educating Agents 11\\n\\nFig. 3: The Jason Reasoning Cycle extended with learning aspects.\\n\\n\\n\\n12 Michael Bosello and Alessandro Ricci\\n\\nrl_parameter(policy, egreedy).\\nrl_parameter(alpha, 0.26).\\nrl_parameter(gamma, 0.9).\\nrl_parameter(epsilon, 0.22).\\nrl_parameter(epsilon_decay, 0.99992).\\n\\nrl_observe(reach_finish, [ pos(_,_) ]).\\n\\nrl_reward(reach_finish, 10) :- finish_line.\\nrl_reward(reach_finish, -1) :- not finish_line.\\n\\nrl_terminal(reach_finish) :- finish_line.\\n\\n@action[rl_goal(reach_finish), rl_param(direction(set(right, left, up, down)))]\\n+!move(Direction) <- move(Direction).\\n\\n/* in this case we run an infinite learning process - actually it could be\\nstopped when the performance (expected return) is considered good enough */\\n\\n!start.\\n+!start : true <- rl.execute(reach_finish); !start.\\n\\nFig. 4: Full source code of the grid-world agent.\\n\\nFig. 5: Chart of simulation results: x is the episode number and y is the average error.\\n\\nIn Appendix A the interested reader can find further details about the Jason imple-\\nmentation. The full implementation is available here1.\\n\\n1 https://github.com/MichaelBosello/jacamo-rl\\n\\nhttps://github.com/MichaelBosello/jacamo-rl\\n\\n\\nFrom Programming Agents to Educating Agents 13\\n\\n5 Discussion\\n\\nIn order to test our proof-of-concept implementation, we used the gridworld problem\\nintroduced in section 3, performing first simple tests over small ( 5x5) grids. It is worth\\nremarking here that both the kind of the problem used \\xe2\\x80\\x93 the gridworld \\xe2\\x80\\x93and the size of\\nthe environment are clearly not useful for evaluating the approach from the point of view\\nof the performance, the scalability and the generality as well. It has been used essentially\\nfor testing the framework developed so far.\\n\\nThe agent source code is shown in Fig. 42. At every episode, the agent appears in\\na random place and seek to reach a fixed target position. SARSA algorithm performs\\nproperly in this task with a \\xce\\xb5-greedy policy with epsilon decay (i.e., the exploration\\nprobability decreases with increasing steps). Parameters have been: alpha = 0.26, gamma\\n= 0.9, epsilon = 0.22, epsilon decay = 0.99992. The agent learns the policy in about\\n1000 episodes, and when epsilon decreases under 0.05 (with this decay, approximately\\nafter 5000 episodes), the behavior becomes near optimal. The chart in figure 5 shows the\\naverage error (how many extra steps were made compared to the minimum path) on five\\ntrials with 6000 episodes.\\n\\n5.1 About the RL algorithm used\\n\\nThe framework aims at modeling the three fundamental RL signals without any assump-\\ntion on the RL algorithm behind them. Nevertheless, depending on the RL algorithm,\\ndifferent kinds of environments may be considered, with a different impact on the per-\\nformances. In literature, three main characteristics of the environments are typically\\nconsidered to properly select the RL algorithms: the Markov property, the type of task\\n(episodic or continuing), and the state and action spaces dimension. A detailed discussion\\nof this aspect is out of the scope of this paper.\\n\\nHere it is interesting to consider if and how our model/framework would be expressive\\nand flexible enough to include more advanced RL approaches used to tackle complex\\nenvironments. For instance, in literature function approximation is exploited to tackle\\npartial observability, possibly using nonlinear methods such as neural networks, in\\nparticular deep networks\\xe2\\x80\\x94such as in deep reinforcement learning. In our framework,\\nthis accounts for changing/plugging a proper RL reasoner component, without changing\\nthe whole interpreter architecture.\\n\\nIf the RL reasoner component cannot be centralised \\xe2\\x80\\x93 being based e.g. on cloud\\nservices \\xe2\\x80\\x93 then an interesting perspective is to consider the possibility to partially exter-\\nnalise the functionalities of the RL reasoner into an artifact in the A&A perspective [22],\\nexploiting e.g. the full JaCaMo platform [5]. In that case, the RL reasoner is modeled as\\nan external tool extending the cognitive capability of the agent and possibly wrapping\\nthe use of cloud-based services handling the management of e.g. deep neural networks,\\nin a cognition-as-a-service style [26].\\n\\n2 The example is available in the source code of the framework.\\n\\n\\n\\n14 Michael Bosello and Alessandro Ricci\\n\\n6 The Road Ahead\\n\\nThe objective of this paper was to introduce a novel perspective on the integration\\nof learning in BDI Agents programming and agent-oriented programming. In that\\nperspective, the development stage of an agent accounts for setting up a first version of\\nthe agent, eventually including some programmed plans (hard plans), and then grow up\\nthe agent by making itself learning some other plans (soft plans), according to the need.\\nSoft plans become part of the plan library and at deployment time the agent can exploit\\nthem like the hard ones, in a uniform way.\\n\\nIn the paper, we described a first proof-of-concept model and implementation based\\non Jason. In the current state, the framework is not meant to be ready to tackle real-world\\nproblems but to be a first tool in order to further explore and develop this idea. In\\nthat perspective, many interesting aspects \\xe2\\x80\\x93 from our point of view \\xe2\\x80\\x93 are worth to be\\ninvestigated in future work. A list of main ones follows:\\n\\n\\xe2\\x80\\x93 Extending the investigation by considering different kinds of RL algorithms, with\\ndifferent complexities, and a different set of examples/problems as well, eventually\\ndoing a rigorous analysis of the computational complexity and properties of the com-\\nputations performed by the extended reasoning cycle. Among the large spectrum of\\nRL-based approaches, two are particularly interesting with respect to the objective of\\nour research line. The first is about Hierarchical Reinforcement Learning, extending\\nthe reinforcement learning paradigm by allowing the learning agent to aggregate\\nactions into reusable subroutines or skills [8]. In BDI case, reusable subroutines or\\nskills are modelled as plans triggered by subgoals. The second one is about shaping\\nin reinforcement learning3 [9]. There, \\xe2\\x80\\x9ceducation\\xe2\\x80\\x9d is realised through the creation\\nof a proper learning environment and, in particular, through demonstration.\\n\\n\\xe2\\x80\\x93 Exploring further the development/education process lifecycle, analysing how the\\ndifferent stages \\xe2\\x80\\x93 development / training, validation / assessment, deployment/moni-\\ntoring \\xe2\\x80\\x93 are related.\\n\\n\\xe2\\x80\\x93 Designing and developing proper tools to be embedded in existing IDEs \\xe2\\x80\\x93 or extend-\\ning them \\xe2\\x80\\x93 to support this process. Including simulators, which become an essential\\npart of the picture.\\n\\n\\xe2\\x80\\x93 Exploring how software engineering aspects such as modularity, extensibility,\\nreusability, composability can be framed when dealing with soft plans, aside to\\nhard plans. Can we introduce kind of incremental learning to extend existing soft\\nplans?\\n\\n\\xe2\\x80\\x93 Exploring how environment first-class abstractions such as artifacts [22] could be\\nuseful to better structure, modularise and make the way in which actions \\xe2\\x80\\x93 and\\nobservations as well \\xe2\\x80\\x93 are currently considered more dynamic.\\n\\n\\xe2\\x80\\x93 Beyond single agent perspective: what does it mean an education process for a multi-\\nagent system? what does it mean an education process for an agent organisation?\\n\\n\\xe2\\x80\\x93 Methodologies \\xe2\\x80\\x93 what\\xe2\\x80\\x99s the impact on AOSE methodology. Or, can we exploit\\nexisting AOSE methodology to effectively support this process or do we need to\\nextend them?\\n\\n3 We thank the reviewers for this suggestion\\n\\n\\n\\nFrom Programming Agents to Educating Agents 15\\n\\n\\xe2\\x80\\x93 Planning \\xe2\\x80\\x93 in the paper we did not consider at all planning, being our framework\\nfocused on learning. Nevertheless, it would be interesting and important to extend\\nthe conceptual framework to consider also the role that planning can do in such\\nagent education process.\\n\\nReferences\\n\\n1. Airiau, S., Padgham, L., Sardina, S., Sen, S.: Enhancing the adaptation of BDI agents using\\nlearning techniques. Int. J. Agent Technol. Syst. 1(2), 1\\xe2\\x80\\x9318 (Apr 2009)\\n\\n2. Andrew McAfee, E.B.: The Second Machine Age: Work, Progress, and Prosperity in a Time\\nof Brilliant Technologies. W. W. Norton & Company (2014)\\n\\n3. Badica, A., Badica, C., Ivanovic, M., Mitrovic, D.: An approach of temporal difference\\nlearning using agent-oriented programming. In: 2015 20th International Conference on\\nControl Systems and Computer Science. pp. 735\\xe2\\x80\\x93742 (May 2015)\\n\\n4. Badica, C., Becheru, A., Felton, S.: Integration of jason reinforcement learning agents into an\\ninteractive application. In: 2017 19th International Symposium on Symbolic and Numeric\\nAlgorithms for Scientific Computing (SYNASC). pp. 361\\xe2\\x80\\x93368 (Sep 2017)\\n\\n5. Boissier, O., Bordini, R.H., H\\xc3\\xbcbner, J.F., Ricci, A., Santi, A.: Multi-agent oriented program-\\nming with JaCaMo. Science of Computer Programming 78(6), 747\\xe2\\x80\\x93761 (2013)\\n\\n6. Bordini, R.H., H\\xc3\\xbcbner, J.F., Vieira, R.: Jason and the Golden Fleece of Agent-Oriented\\nProgramming, pp. 3\\xe2\\x80\\x9337. Springer US, Boston, MA (2005)\\n\\n7. Bordini, R.H., H\\xc3\\xbcbner, J.F., Wooldridge, M.: Programming Multi-Agent Systems in AgentS-\\npeak Using Jason (Wiley Series in Agent Technology). John Wiley & Sons, Inc., USA (2007)\\n\\n8. Botvinick, M., Niv, Y., C Barto, A.: Hierarchically organized behavior and its neural\\nfoundations: A reinforcement learning perspective. Cognition 113, 262\\xe2\\x80\\x9380 (11 2008).\\nhttps://doi.org/10.1016/j.cognition.2008.08.011\\n\\n9. Brys, T., Harutyunyan, A., Suay, H.B., Chernova, S., Taylor, M.E., Now\\xc3\\xa9, A.: Reinforcement\\nlearning from demonstration through shaping. In: Proceedings of the 24th International\\nConference on Artificial Intelligence. pp. 3352\\xe2\\x80\\x933358. IJCAI\\xe2\\x80\\x9915, AAAI Press (2015), http:\\n//dl.acm.org/citation.cfm?id=2832581.2832716\\n\\n10. end of code, T.: Tanz, jason. Wired (2016)\\n11. Feliu, J.L.: Use of reinforcement learning (rl) for plan generation in belief-desire-intention\\n\\n(bdi) agent systems (2013), https://digitalcommons.uri.edu/theses/160\\n12. Ford, M.: Architects of Intelligence: The truth about AI from the people building it. Packt\\n\\nPublishing (2018)\\n13. Gerrish, S.: How Smart Machines Think. MIT Press (2018)\\n14. Guerra-Hern\\xc3\\xa1ndez, A., El Fallah-Seghrouchni, A., Soldano, H.: Learning in BDI multi-agent\\n\\nsystems. In: Proceedings of the 4th International Conference on Computational Logic in\\nMulti-Agent Systems. pp. 218\\xe2\\x80\\x93233. CLIMA IV\\xe2\\x80\\x9904, Springer-Verlag, Berlin, Heidelberg\\n(2004)\\n\\n15. Karim, S., Sonenberg, L., Tan, A.H.: A hybrid architecture combining reactive plan execution\\nand reactive learning. In: Yang, Q., Webb, G. (eds.) PRICAI 2006: Trends in Artificial\\nIntelligence. pp. 200\\xe2\\x80\\x93211. Springer Berlin Heidelberg, Berlin, Heidelberg (2006)\\n\\n16. Kelly, J.E.: Computing, cognition and the future of knowing (2015), IBM Research and\\nSolutions, white paper\\n\\n17. Meijer, E.: Behind every great deep learning framework is an even greater programming\\nlanguages concept (2018), Invited Talk at the 26th ACM Joint European Software Engineering\\nConference and Symposium on the Foundations of Software Engineering (ESEC/FSE)\\n\\n18. Norling, E.: Folk psychology for human modelling: Extending the BDI paradigm (2004)\\n\\nhttps://doi.org/10.1016/j.cognition.2008.08.011\\nhttp://dl.acm.org/citation.cfm?id=2832581.2832716\\nhttp://dl.acm.org/citation.cfm?id=2832581.2832716\\nhttps://digitalcommons.uri.edu/theses/160\\n\\n\\n16 Michael Bosello and Alessandro Ricci\\n\\n19. Parnas, D.L.: The real risks of artificial intelligence. Commun. ACM 60(10), 27\\xe2\\x80\\x9331 (Sep\\n2017). https://doi.org/10.1145/3132724, http://doi.acm.org/10.1145/3132724\\n\\n20. Rao, A.S.: AgentSpeak(L): BDI agents speak out in a logical computable language. In: Van de\\nVelde, W., Perram, J.W. (eds.) Agents Breaking Away. pp. 42\\xe2\\x80\\x9355. Springer Berlin Heidelberg,\\nBerlin, Heidelberg (1996)\\n\\n21. Rao, A.S., Georgeff, M.P.: Bdi agents: From theory to practice pp. 312\\xe2\\x80\\x93319 (1995)\\n22. Ricci, A., Piunti, M., Viroli, M.: Environment programming in multi-agent systems: an\\n\\nartifact-based perspective. Autonomous Agents and Multi-Agent Systems 23(2), 158\\xe2\\x80\\x93192\\n(Sep 2011)\\n\\n23. Shoham, Y.: Agent-oriented programming. Artif. Intell. 60(1), 51\\xe2\\x80\\x9392 (Mar 1993), http:\\n//dx.doi.org/10.1016/0004-3702(93)90034-9\\n\\n24. Singh, D., Hindriks, K.V.: Learning to improve agent behaviours in goal. In: Dastani, M.,\\nH\\xc3\\xbcbner, J.F., Logan, B. (eds.) Programming Multi-Agent Systems. pp. 158\\xe2\\x80\\x93173. Springer\\nBerlin Heidelberg, Berlin, Heidelberg (2013)\\n\\n25. Singh, D., Sardina, S., Padgham, L., James, G.: Integrating learning into a BDI agent for\\nenvironments with changing dynamics. In: Proceedings of the Twenty-Second International\\nJoint Conference on Artificial Intelligence - Volume Volume Three. pp. 2525\\xe2\\x80\\x932530. IJCAI\\xe2\\x80\\x9911,\\nAAAI Press (2011). https://doi.org/10.5591/978-1-57735-516-8/IJCAI11-420, http://dx.\\ndoi.org/10.5591/978-1-57735-516-8/IJCAI11-420\\n\\n26. Spohrer, J., Banavar, G.: Cognition as a service: An industry perspective. AI Magazine 36(4),\\n71\\xe2\\x80\\x9386 (Winter 2015)\\n\\n27. Sutton, R.S., Barto, A.G.: Reinforcement learning : an introduction. The MIT Press (2018)\\n28. Tan, A.H., Ong, Y.S., Tapanuj, A.: A hybrid agent architecture integrating desire, intention\\n\\nand reinforcement learning. Expert Syst. Appl. 38(7), 8477\\xe2\\x80\\x938487 (Jul 2011)\\n29. Wei\\xc3\\x9f, G.: Adaptation and learning in multi-agent systems: Some remarks and a bibliography.\\n\\nIn: Wei\\xc3\\x9f, G., Sen, S. (eds.) Adaption and Learning in Multi-Agent Systems. pp. 1\\xe2\\x80\\x9321. Springer\\nBerlin Heidelberg, Berlin, Heidelberg (1996)\\n\\n30. Wooldridge, M.: Introduction to Multi-Agent Systems. Wiley (2009)\\n\\nA Details about the Implementation\\n\\nIn this appendix, we include further details about how the Jason interpreter has been\\nextended to include the RL reasoner, following the model depicted in Fig. 1 and the\\narchitecture shown in Fig. 3\\n\\nThe belief base is amended to track belief about learning and observations instruc-\\ntions to prevent the necessity to loop multiple time over the belief base when the agent\\nneeds to retrieve the observations. Every time a belief is added or deleted, the observa-\\ntions maps are updated. To retrieve the reward and verify if the current state is terminal,\\nthe related rules are checked to find out if they are a logical consequence of the current\\nbelief base.\\n\\nWe develop also two internal actions: rl.execute and rl.expected_return.\\nWhen rl.execute is carried out, the agent retrieves from the belief base the observa-\\ntions, the algorithm parameters, the reward, the boolean value that assert if the state is\\nterminal. The agent retrieves the plans that are properly labeled, as described in section\\n1, from the plan library to form the action set. The action set is reduced through the\\ncheck context function: step (7) in 2. All the information are passed to the RL algorithm\\nwhich returns the next action (an achievement goal in our representation) to execute.\\n\\nhttps://doi.org/10.1145/3132724\\nhttp://doi.acm.org/10.1145/3132724\\nhttp://dx.doi.org/10.1016/0004-3702(93)90034-9\\nhttp://dx.doi.org/10.1016/0004-3702(93)90034-9\\nhttps://doi.org/10.5591/978-1-57735-516-8/IJCAI11-420\\nhttp://dx.doi.org/10.5591/978-1-57735-516-8/IJCAI11-420\\nhttp://dx.doi.org/10.5591/978-1-57735-516-8/IJCAI11-420\\n\\n\\nFrom Programming Agents to Educating Agents 17\\n\\nThen, rl.execute puts the selected action on top of the current intention\\xe2\\x80\\x99s stack. If\\nthe current step isn\\xe2\\x80\\x99t a final step, another rl.execute is pushed in the stack under the\\nadded action. As a result, rl.execute appears like a mere static plan; rl.execute and\\nits intention acts and competes for execution like any other intention. Moreover, the next\\nrl.execute will never be executed before the selected action end.\\n\\nIn the proof of concept, we implemented the SARSA algorithm with an \\xce\\xb5-greedy\\npolicy. An RL algorithm must just implement the interface as follows:\\n\\np u b l i c i n t e r f a c e Algori thmRL {\\nAc t i on n e x t A c t i o n ( Map<Term , Term> p a r a m e t e r ,\\n\\nSet < Act ion > a c t i o n ,\\nSet < L i t e r a l > o b s e r v a t i o n ,\\ndouble reward , boolean i s T e r m i n a l ) ;\\n\\ndouble e x p e c t e d R e t u r n ( Set < Act ion > a c t i o n ,\\nSet < L i t e r a l > o b s e r v a t i o n ) ;\\n\\n}\\n\\n\\n\\tFrom Programming Agents to Educating Agents \\xe2\\x80\\x93 A Jason-based Framework for Integrating Learning in the Development of Cognitive Agents\\n\\n'