b'\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAgents are More Complex than Other Software:\\nAn Empirical Investigation\\n\\nAlon T. Zanbar and Gal A. Kaminka\\n\\nThe MAVERICK Group\\nComputer Science Department\\n\\nBar-Ilan University, Israel\\n\\nAbstract. We empirically investigate agent software repositories using\\ncommonly used software metrics, which are used in software engineering\\nliterature to quantify meaningful characteristics of software based on its\\nsource code. We contrast the measurements with those of software in\\nother categories. Analyzing hundreds of software projects, we find that\\nagent software is clearly and significantly different from other types of\\nsoftware of comparable size.\\n\\n1 Introduction\\nFor many years, significant research efforts have been spent on investigating\\nmethodologies, tools, models and technologies for engineering autonomous agents\\nsoftware. Research into agent architectures and their structure, programming\\nlanguages specialized for building agents, formal models and their implementa-\\ntion, development methodologies, middle-ware software, have been discussed in\\nthe literature, encompassing multiple communities of researchers, with at least\\npartial overlaps in interests and approaches.\\n\\nThe most important underlying assumption of these research efforts is that\\nsuch specialization is needed, because autonomous agent software poses engineer-\\ning requirements that may not be easily met by more general (and more famil-\\niar) software engineering and programming paradigms. Specialized tools, models,\\nprogramming languages, code architectures and abstractions make sense, if the\\nsoftware engineering problem is specialized.\\n\\nA broad overview of the literature reveals that for the most part, the truth\\nof this assumption has been supported by qualitative arguments and anecdotal\\nevidence. Agent-oriented programming [36] is by now a familiar and accepted\\nprogramming paradigm, and countless discussions of its merits and its distinc-\\ntiveness with respect to other programming paradigms (e.g., object-oriented\\nprogramming, aspect-oriented programming) are commonly found on the in-\\nternet. Agent architectures are commercially available as development platforms\\nand are incorporated into products. Indeed, agent-oriented software development\\nmethodologies are taught and utilized in and out of academic [5, 16,21,31].\\n\\nHowever, there is a disturbing lack of quantitative, empirical evidence for\\nthe distinctiveness of autonomous agent software. Lacking such evidence, agent\\nsoftware engineers rely on intuition, experience, and philosophical arguments\\nwhen they evaluate or advocate specialized methods.\\n\\nlouisedennis\\nPlaced Image\\n\\n\\n\\n2 A. T. Zanbar, G. A. Kaminka\\n\\nThis paper provides the first empirical evidence for the distinctiveness of\\nautonomous agent software, compared to other software categories. We quanti-\\ntatively analyze over 500 software projects: 140 autonomous agent and robotics\\nprojects (from RoboCup, the Agent Negotiations Competitions, Chess, and other\\nsources), together with close to 400 automatically selected software projects from\\ngithub, of various types. With each, we utilize general source code metrics, such\\nas Cyclomatic Complexity, Cohesion, Coupling, and others used by general soft-\\nware engineering researchers to quantify meaningful characteristics of software\\n(over 250 measures\\xe2\\x80\\x94see Section 2).\\n\\nWe conducted both statistical and machine-learning analysis, to determine\\n(1) whether agents emerge as a distinguishable sub-group within the pool, and\\n(2) whether there are clear distinguishing measures. We find that agent software\\nis clearly and significantly different from other types of software of comparable\\nsize. This result appears both when using manual statistical analysis, as well\\nas machine learning methods. Specifically, autonomous agents software is sig-\\nnificantly more complex (in the sense of control flow complexity) than other\\nsoftware categories. We discuss potential implications of these results.\\n\\n2 Background\\nThere is vast literature reporting on software engineering of autonomous agents.\\nWe cannot do justice to these efforts for lack of space. For brevity, we use the\\nterm agent-oriented software engineering (AOSE) to refer to the combined re-\\nsearch area, With due apologies to all the different threads of work whose unique\\ncontributions are blurred by our choice.\\n\\nAOSE is a thriving area of research [25,31, 33, 35, 38, 41]. For the most part,\\nthe arguments for the study of AOSE as distinct from general software engi-\\nneering are well argued philosophically, and qualitatively pointing out inherent\\nconceptual differences between the software engineering of agents. To the best\\nof our knowledge, little quantitative empirical evidence\\xe2\\x80\\x94certainly not at the\\nscale detailed below\\xe2\\x80\\x94has been offered to support these important conceptual\\narguments.\\n\\nClosely related, pioneering works into software engineering in robotics\\n(e.g., [6\\xe2\\x80\\x939,20,34,40] similarly likewise argue qualitatively for distinguishing soft-\\nware engineering in robotics. Some emphasize specific middleware frameworks\\n(e.g., [11,17,20,39]), while others focus on critical capabilities [10,14,30]). The un-\\nderlying implicit assumption is similar to those in AOSE: that robotics software\\nis sufficiently different from general software, that it merits distinct methodolo-\\ngies and tools to ease software development. Indeed, we report below that robot\\ncode is similar in some aspects to autonomous agents code, but is not as easily\\ndistinguished from general software.\\n\\nThe rarity of quantitative investigations in AOSE (see also below) is not for\\nlack of quantitative methods in general software engineering. Beginning with\\nthe 1970s pioneering research on Cyclomatic Complexity [28] and Halstead mea-\\nsures [22] there have been many investigations both proposing quantitative met-\\nrics of software constructs, and relating the measurements to software qual-\\nity, development effort, software type, and other attributes of interest [1, 4, 26].\\n\\n\\n\\nAgents are More Complex than Other Software: An Empirical Investigation 3\\n\\nFor example, metrics such as Cyclomatic Complexity, Coupling, and Cohesion\\xe2\\x80\\x94\\ngenerated from analysis of the software source code and the program control\\nflow graph\\xe2\\x80\\x94 have been shown to correlate with defects [12,23,28]. Maintaining\\ntheir values within specific ranges (or below some thresholds) tends to lower\\nthe expected defect creation rate, and improve other measures of software qual-\\nity. Development and exploration of software metrics continues today, e.g., for\\nparadigms such as aspect-oriented programming [32]. See [18] for a comprehen-\\nsive survey.\\n\\nSoftware metrics have been used to classify software, or cluster together soft-\\nware based on measured characteristics, as we do in this paper [27]. For example,\\nDe Souza et al. defined software metric thresholds based on context [15]. Sto-\\njkovski [37] showed this approach is applicable for Android projects. Another\\nexample can be found in Meirelles at al. [29], who found linkage between the\\nsize and complexity of open source projects, to attractiveness of the project for\\ncontributors.\\n\\n3 Software Project Data Collection and Curation\\nWe begin with an overview of the data collection and curation process. The data\\ncollected will be used in the analysis processes described in Sections 4\\xe2\\x80\\x935.\\n\\n3.1 Data Sources\\n\\nRoboCup. RoboCup is one of the oldest and largest annual global robotics com-\\npetition events in the world\\xe2\\x80\\x94taking place since 1997. The event is organized in\\nseveral different divisions.Within each division, there are multiple leagues, with\\ntheir own rules. For example, within the soccer division, there were over the years\\nup to three different simulation-based leagues (2D, 3D, and coach), and several\\nphysical robot competitions (standard platform, small-size, mid-size, and two\\nhumanoid leagues). The competitions themselves are between completely au-\\ntonomous agents/robots; no human in the loop. In most cases, the agents run in\\ncompletely distributed fashion, without a centralized controller.\\n\\nThe bulk of the code in the various leagues is written by graduate students\\nand researchers in robotics and artificial intelligence, some from top universities\\nin these fields. The simulation leagues follow an internal rule, which requires all\\nteams to release a binary version of their code within a year following the com-\\npetition. Source code release is not required, but strongly encouraged. Indeed,\\nwe use the source code from many 2D simulation league teams, downloaded from\\ntheir repository server. In addition, we used source code from other RoboCup\\nsoccer leagues, gathered from the internet.\\n\\nAutomated Negotiating Agent Competition (ANAC). The annual International\\nAutomated Negotiating Agents Competition (ANAC) is used by the automated\\nnegotiation research community to benchmark and evaluate its work and to chal-\\nlenge itself. The benchmark problems and evaluation results and the protocols\\nand strategies developed are available to the wider research community. ANAC\\nhas similar properties to the RoboCup in the sense of emphasizing autonomous\\nagents. It is a popular competition for software agent researchers, maintains a\\n\\n\\n\\n4 A. T. Zanbar, G. A. Kaminka\\n\\nrequirement that all the sources of the agents participating in the competition\\nare made available for research. We collected ANAC software agent projects from\\nthe competition web site.\\n\\nGitHub. GitHub has more than 24 million users and more than 67 million code\\nrepositories. It is the largest repository of open source projects in the world.\\nGitHub exposes robust API for finding repositories using extensive query lan-\\nguage, which we used to find relevant project for analysis. Repositories in GitHub\\nare categorized by users using tags, which we used to categorize software projects.\\n\\nAdditional data. We additionally found open source robotics projects from the\\nDARPA Grand and Urban challenges, and from industrial projects where our\\nlab was involved in research.\\n\\n3.2 Automatic Data Harvesting\\nFrom the sources above, we first collected agent and robot software projects\\xe2\\x80\\x94all\\nwe could find and use: 2D RoboCup teams for which source code is available, the\\nANAC agent projects, robotics software from RoboCup and the other sources\\ndescribed above. To counter bias from competition sources, we also used github\\nprojects tagged chess as agents software.\\n\\nThe process of collecting and filtering of repositories from GitHub was au-\\ntomatic. The primary constraint in selecting software projects is comparability.\\nThe source code collected for agents uses C, C++, and Java, and so we restricted\\nourselves to projects in these languages, to prevent language-specific bias in the\\nmetrics. Similarly, we restricted ourselves to software size (measured in lines of\\ncode\\xe2\\x80\\x94LOC) in comparable ranges, and belonging to software categories other\\nthan agents or AI:\\n\\n\\xe2\\x80\\x93 Programming languages : C , C++ , Java\\n\\xe2\\x80\\x93 high Level of maturity\\n\\xe2\\x80\\x93 Distinct classification in github (for github projects)\\n\\xe2\\x80\\x93 Size > 900 lines of code (LOC)\\n\\nTable 1 shows a breakdown of the number and categories of the harvested\\nsoftware projects in the dataset (almost a terabyte). In total, there were 118\\nprojects generally classified as autonomous agents for software or virtual envi-\\nronments, 20 projects classified as autonomous robots, and 377 projects in other\\ncategories. Table 2 lists the mininum, maximum and median project size in each\\ndomain, measured in LOC.\\n\\n3.3 The Measurement Pipeline\\nThe essence of the process is the measurement, i.e., the generation of measure-\\nments from applying code metrics to the software. We focus on source code\\nmetrics in this paper. The source code of each project was processed to extract\\ntwo different data structures: a control flow graph, and a code statistics database.\\nThese, in turn, are used to calculate several different metrics. Additionally we\\nsave information on the context of the repository (name , location, category)\\n\\n\\n\\nAgents are More Complex than Other Software: An Empirical Investigation 5\\n\\nClassification Source Software Domain Size Maturity Indicator\\n\\nAutonomous\\nAgents\\n\\nRoboCup 2D simulation Virtual Robots 64 Qualification for RoboCup\\nANAC Negotiating Agents 26 Qualification for ANAC\\nGitHub Chess-Playing Engines 28 > 5 GitHub stars\\n\\nGeneral GitHub\\n\\nAudio 54\\n\\n> 5 GitHub stars\\n\\nEducation 50\\nFinance 26\\nGames 34\\nGraphics 60\\nIDE 53\\nMobile Applications 42\\nSecurity 58\\n\\nRobots\\nDARPA Challenges Autonomous Car 2 Qualification for Challenge\\nRoboCup competition Soccer Physical Robots 15\\nApplied R&D Projects Robots 3\\n\\nTable 1. Software project data breakdown.\\n\\nand other information like source code language, competition results, the year\\nin which the code was deployed, etc.\\n\\nWe used two different tools, independently, to allow validation of the results:\\nCCCC1 and Analizo2. The two tools were run on two 24-core XEON servers,\\neach with 76GB of ram. Total CPU time is more than a month.\\n\\nThe measurement tools provide the following general software metrics, for\\ndifferent level of analysis (see [18] for detailed descriptions). As with the restric-\\ntion on choice of language, we are restricted to using general metrics as they\\nallow for measuring non-agent code. Otherwise, we\\xe2\\x80\\x99d be able to use code metrics\\nspecific to AOSE [2,3,13,19], and specialized languages (e.g., 2/3APL, JASON).\\n\\nSummary & Project Level Metrics: Total Lines of Code (total loc), Total Num-\\nber of Modules (total modules), Total Number of Methods (total nom).\\n\\nModule Level Metrics: Afferent Connections per Class (ACC), Average Cyclo-\\nmatic Complexity per Method (ACCM), Average Method Lines of Code (AM-\\nLOC), Average Number of Parameters (ANPM), Coupling Between Objects\\n(CBO), Coupling Factor (COF), Depth of Inheritance Tree (DIT), Lack of Cohe-\\nsion of Methods (LCOM4), Lines of Code (LOC), Number of Attributes (NOA),\\nNumber of Children (NOC), Number of Methods (NOM), Number of Public At-\\ntributes (NPA), Number of Public Methods (NPM), Response for Class (RFC),\\nStructural Complexity (SC).\\n\\nWe collected not only the raw metrics above, but also their aggregation in\\nvarious ways, so as to minimize the inherent loss of information. Thus for each\\nmetric, we also computed its mean, mode, minimum value, maximum value,\\nquantiles (lower, max, median, min, ninety five, upper), standard deviation,\\n\\n1 http://cccc.sourceforge.net/\\n2 http://www.analizo.org/\\n\\n\\n\\n6 A. T. Zanbar, G. A. Kaminka\\n\\nSoftware Domain Min, Max, Median Size in LOC\\n\\nVirtual Robots 1010, 153661, 23495\\nNegotiating Agents 1031, 102816, 1352\\nChess-Playing Engines 1084, 59108, 5311\\nAudio 1065, 1912860, 17010\\nEducation 1026, 393360, 6933\\nFinance 1136, 450524, 10455\\n34 1393, 185784, 5064\\nGraphics 1168, 385036, 18769\\nIDE 1457, 401897, 32486\\nMobile Applications 1210, 129366, 4658\\nSecurity 1214, 164228, 10341\\nAutonomous Car 117848, 117848, 117848\\nSoccer Physical Robots 15335, 793966, 54895\\nRobots 3131, 64028, 10588\\n\\nTable 2. Software projects min, max, and median LOC.\\n\\nvariance, skewness, and kurtosis. All in all, each software project was represented\\nby more than 250 measurements.\\n\\n4 Statistical Analysis\\n\\nWe conducted two separate analysis efforts which had common general goal.\\nThis section details the results of a statistical analysis, while the next section\\npresents the use of machine-learning analysis. The focus in both is to reveal\\ndifferences, if they occur, between the different software categories, as expressed\\nin the measurements of different metrics.\\n\\nEvery project is represented by approximately 250 different metrics. As such,\\nit is difficult to attempt to find differentiating metric by hand. We therefore\\nused a heuristic procedure to assist in finding promising features. Algorithm 1\\ndescribes the procedure. We emphasize that this is a heuristic procedure, to draw\\nhuman attention to features of interest, not for statistical inference.\\n\\nThe idea is to iterate over the software domains. For each domain r, we sep-\\narate it out from the others, and then use a two-tailed t-test to contrast the\\ndistribution of the metric values in the domain and in all others. A lower p value\\nfrom the t-test is used as a heuristic, indicating that potentially a good differ-\\nentiating feature has been detected. We collect all the domains differentiated by\\nthe metric f into a common set indexed by f . We then look for sets larger than\\ntwo. We use a threshold to avoid distractions from a metric that may distinguish\\na specific domain from all others, by chance.\\n\\nTable 3 shows the output of the algorithm for each individual metric, when\\nlisted in increasing order of probability (i.e., in order of decreasing indication\\nof separation power). The top four metrics are the ACCM mean and its upper\\nand median quantiles, and the Coupling Factor (CoF) metric, which measures\\ncoupling between modules. These four metrics clearly distinguish between the\\nagent domains (RoboCup 2D Simulation, Chess, ANAC agents).\\n\\n\\n\\nAgents are More Complex than Other Software: An Empirical Investigation 7\\n\\nAlgorithm 1 Common differentiator algorithm\\n\\n1: for all r \\xe2\\x88\\x88 Domains do\\n2: others\\xe2\\x86\\x90 (Domains\\xe2\\x88\\x92 {r})\\n3: for all f \\xe2\\x88\\x88 metrics do\\n4: if statistic2sampletest(rf , othersf ) < 0.05 then\\n5: CommonSetf \\xe2\\x86\\x90 CommonSetf\\n\\n\\xe2\\x8b\\x83\\nr\\n\\n6: for all f \\xe2\\x88\\x88 metrics do\\n7: if |CommonSetf | >= 3 then . 3 or more clustered together?\\n8: selectedf \\xe2\\x86\\x90 CommonSetf\\n\\nreturn all selectedf\\n\\nWe use the p value in Table 3 as a heuristic indicator for the human analyst.\\nIt gives an indication of the strength of the clustering, independent of the content\\nof the cluster. Even if the agent domains could be distinguished from the others,\\nwe could easily expect other software domains to be so clustered. However, the\\nfact is that the strongest distinguishing metrics put autonomous agents together,\\napart from other domains.\\n\\nMetric Repositories p value\\n\\naccm mean [RoboC-2D, Chess, ANAC] 1.18E-04\\naccm quantile upper [RoboC-2D, Chess, ANAC] 8.78E-04\\naccm quantile median [RoboC-2D, Chess, ANAC] 1.17E-03\\ntotal cof [RoboC-2D, Chess, ANAC] 1.19E-03\\nnoa skewness [RoboC-2D, IDE, Graphics] 2.59E-03\\nnom quantile upper [RoboC-2D, ANAC, Audio] 6.27E-03\\namloc quantile upper [RoboC-2D, Chess, ANAC, ...] 7.99E-03\\nnom mean [RoboC-2D, ANAC, Graphics, Audio] 1.07E-02\\nanpm quantile upper [RoboC-2D, ANAC, Graphics] 1.14E-02\\nnoa kurtosis [RoboC-2D, Ide, Games, Graphics] 1.28E-02\\n\\nTable 3. Top distinguishing features in descending order, and the software domains\\nthey cluster.\\n\\nWe then moved to examining the results visually, using box-plots to display\\nthe distribution of specific metrics of each software domain. We seek features\\nwhich, as clearly as possible, distinguish the three classes of domains.\\n\\nIndeed some metrics clearly are different between domains. For example, Fig-\\nure 1 show the box-plot distribution of the Lack-of-Cohesion (LCOM4) metric,\\nwhich received generally low rank by the heuristic procedure (i.e., a relative high\\np value). Here, we clearly see that the RoboCup-Other-Leagues group stands out,\\ncompared to the other software domains. However, it is the only domain in the\\ncluster, and does not distinguish the agents or robots domains from others.\\n\\nOther metrics may sometimes cluster together more than one domain, but\\nare not able to distinguish agents from non-agents code. For example, Figure 2\\nshow the distribution of the Structural Complexity metric. We can see that the\\ninner-quantile range and median are similar between RoboCup 2D and Other\\nRoboCup Leagues, suggesting some commonality in behavior of the structure of\\n\\n\\n\\n8 A. T. Zanbar, G. A. Kaminka\\n\\nRo\\nbo\\ncu\\np-2\\nD\\nAn\\nac\\n\\nCh\\nes\\ns\\n\\nAu\\ntod\\nriv\\ners\\n\\nRo\\nbo\\n-Pr\\noje\\ncts\\n\\nRo\\nbc\\nup\\n-O\\nthe\\nr-L\\nea\\ngu\\nes\\n\\nGr\\nap\\nhic\\ns\\n\\nSe\\ncu\\nrit\\ny\\n\\nRo\\nbo\\nt-S\\nim\\nula\\ntio\\nn\\nGa\\nme\\ns\\n\\nAu\\ndio Ide\\n\\nMo\\nbil\\ne\\n\\nEd\\nuc\\nati\\non\\n\\nFin\\nan\\nce\\n\\ncategory\\n\\n2\\n\\n4\\n\\n6\\n\\n8\\n\\n10\\n\\n12\\n\\nFig. 1. Box plot distribution of LCOM4 mean.\\n\\nclasses and objects. However, it reveals no commonality between the different\\ndomains of the same class (Agents, Robots, or General Software).\\n\\nIn contrast, metrics that were ranked high by Alg. 1 visually show much\\nmore promise. For example, Table 3 suggests the mean ACCM is promising,\\nin terms of its ability to distinguish between agents and non-agent software.\\nFigure 3 shows the box plot distributions for this metric. Visually, the box-plots\\nfor the Agents class (RoboCup 2D, ANAC, Chess) are clearly prominent relative\\nto other software domains.\\n\\n5 Machine Learning Analysis\\nA second approach for our investigation uses machine learning techniques, to\\ncomplement the manual analysis. Humans detect patterns in visualizations that\\ncomputers may miss, yet may also fall prey to misconceptions. Thus an auto-\\nmated analysis can complement the manual process.\\n\\nWe attempted to use several different machine learning classifiers to distin-\\nguish agent and non-agent software domains, with the goal of analyzing success-\\nful classification schemes, to reveal the metrics, or metric combinations, which\\nprove meaningful in the classification.\\n\\nPre-processing the data. We filtered outliers at the top and bottom 3% of the\\ndata (i.e., within the 3\\xe2\\x80\\x9397 percentiles). Aggregated features (total, median, etc.)\\nwere removed to minimize the effect of project size on the model, and to reduce\\nthe number of features (standing originally at around 250). The data was divided\\ninto a training (85%) and testing (15%) sets.\\n\\nClassification procedure. We choose one vs many classification strategy, similarly\\nto the manual analysis above. Iterating over all software classes, we trained a\\n\\n\\n\\nAgents are More Complex than Other Software: An Empirical Investigation 9\\n\\nRo\\nbo\\ncu\\np-2\\nD\\nAn\\nac\\n\\nCh\\nes\\ns\\n\\nAu\\ntod\\nriv\\ners\\n\\nRo\\nbo\\n-Pr\\noje\\ncts\\n\\nRo\\nbc\\nup\\n-O\\nthe\\nr-L\\nea\\ngu\\nes\\n\\nGr\\nap\\nhic\\ns\\n\\nSe\\ncu\\nrit\\ny\\n\\nRo\\nbo\\nt-S\\nim\\nula\\ntio\\nn\\nGa\\nme\\ns\\n\\nAu\\ndio Ide\\n\\nMo\\nbil\\ne\\n\\nEd\\nuc\\nati\\non\\n\\nFin\\nan\\nce\\n\\ncategory\\n\\n0\\n\\n10\\n\\n20\\n\\n30\\n\\n40\\n\\nFig. 2. Box plot distribution of the Structural Complexity metric for software domains.\\nRoboCup 2D simulation and RoboCup-Other-Leagues have larger variance and higher\\nvalues than all other software domains.\\n\\nRo\\nbo\\ncu\\np-2\\nD\\nAn\\nac\\n\\nCh\\nes\\ns\\n\\nAu\\ntod\\nriv\\ners\\n\\nRo\\nbo\\n-Pr\\noje\\ncts\\n\\nRo\\nbc\\nup\\n-O\\nthe\\nr-L\\nea\\ngu\\nes\\n\\nGr\\nap\\nhic\\ns\\n\\nSe\\ncu\\nrit\\ny\\n\\nRo\\nbo\\nt-S\\nim\\nula\\ntio\\nn\\nGa\\nme\\ns\\nAu\\ndio Ide\\n\\nMo\\nbil\\ne\\n\\nEd\\nuc\\nati\\non\\n\\nFin\\nan\\nce\\n\\ncategory\\n\\n1\\n\\n2\\n\\n3\\n\\n4\\n\\n5\\n\\n6\\n\\n7\\n\\nanalizo_accm_mean\\n\\nFig. 3. Box plot distribution of mean ACCM of software domains.\\n\\n\\n\\n10 A. T. Zanbar, G. A. Kaminka\\n\\nbinary classifier to differentiate between samples of one software domain (ex. Au-\\ndio) to all other software classes. This creates an inherent imbalance in the num-\\nber of examples presented, which we alleviated by using random over-sampling\\nof the minority class.\\n\\nFor classification, we used the following classification algorithms: Support\\nVector Machines, Logistics Regression, and Gradient-Boosted Decision Trees.\\nThe implementations are open-source packages (scikit-learn3 and XGBoost4).\\nThe performance of classifiers was carried out using two scoring functions, famil-\\niar to machine learning practitioners: F1 and AUC (area under the ROC curve).\\nIn both, a greater value indicates better performance. Each of the tables below\\n(Tables 4\\xe2\\x80\\x936) shows the top classifers built using the classification algorithms. In\\neach, we list the top classification results of a single domain versus all others.\\nOur interest, however, is not so much on being able to classify a specific domain,\\nbut instead in the metrics used as features when classifying Agent software. The\\nlast column of each table lists the most informative 3\\xe2\\x80\\x934 features (metrics) used\\nby the classifier. Frequent recurrence may hint at important metrics.\\n\\nTable 4 shows the top results from the SVM classifiers, in decreasing or-\\nder of performance. SVM classification output is only \\xe2\\x80\\x9dHard decision\\xe2\\x80\\x9d without\\nprobability distribution of the different classes and thus the AUC score is not\\navailable for it. We used the default SVM parameters in the implementation.\\nThe F1 scores in the table are far from indicating great success, yet we note\\nthe presence of the mean ACCM metric in the list of features important for\\nclassification for the repositories belongs to the \\xe2\\x80\\x9dAgent\\xe2\\x80\\x9d class.\\n\\nagent type class 1 auc f1 score feature rank\\n\\n0 agent Robocup-2D - 0.67 [accm quantile median, accm quantile upper, noc mean]\\n1 agent Anac - 0.62 [accm quantile lower, noa quantile lower, noc mean]\\n2 non-agent Ide - 0.33 [acc quantile lower, dit mean, noc quantile upper]\\n3 non-agent Mobile - 0.32 [acc quantile lower, acc quantile median, accm quantile lower]\\n4 non-agent Graphics - 0.20 [accm quantile lower, dit quantile lower, dit quantile median]\\n\\nTable 4. SVM top five scoring software domain classifiers.\\n\\nWe next used classifiers built using Logistic Regression (LR). We used L2\\nregularization, and stopping criteria of 100 iterations. The top LR classifiers are\\nreported in Table 5. In general their scores are lower than the SVM classifiers\\nreported above.\\n\\nFinally, we used Gradient-Boosted Decision Tree classifiers. The idea in this\\ntechnique is to use an ensemble of decision trees based on subsets of the samples\\nand features, to lower the risk of over-fitting while maintaining high accuracy.\\nThe classifiers were built using the XGBoost package, using the default parame-\\nters. The results are shown in Table 6. Overall, the results are much better than\\n\\n3 https://scikit-learn.org/\\n4 https://github.com/dmlc/xgboost\\n\\n\\n\\nAgents are More Complex than Other Software: An Empirical Investigation 11\\n\\nagent type class 1 auc f1 score feature rank\\n\\n0 agent Anac 0.99 0.80 [accm quantile upper, noa quantile lower, rfc quantile lower]\\n1 agent Robocup-2D 0.97 0.70 [amloc quantile upper, noc mean, npa mean]\\n2 Robot Robcup-Other-Leagues 0.87 0.50 [amloc quantile lower, cbo mean, nom mean]\\n3 non-agent Ide 0.77 0.44 [amloc quantile median, anpm mean, npa mean]\\n4 non-agent Graphics 0.76 0.24 [anpm mean, lcom4 mean, mmloc mean]\\n\\nTable 5. Logistic Regression top scoring software classes.\\n\\nthe other two classification attempts. Some individual domain classifiers achieve\\nhigh scores.\\n\\nMost importantly, however, we note that the top performing classifiers (1)\\nare those that are able to distinguish agent software from other types of soft-\\nware, and (2) utilize the mean ACCM and AMLOC metrics in their classification\\ndecisions. These results concur with the conclusions. of the manual analysis de-\\nscribed earlier. We also observe that software from physical robots participating\\nin RoboCup (domain: Robocup-Other-Leagues) has also been classified success-\\nfully, using the AMLOC metric (among other metrics).\\n\\nAgent/General Class (Domain) AUC F1\\n\\n0 Agent Robocup-2D 0.97 0.85\\n1 Agent ANAC 0.98 0.67\\n2 Agent Chess 0.84 0.44\\n3 Robot Robcup-Other-Leagues 0.89 0.40\\n4 General Graphics 0.65 0.31\\n5 General Security 0.76 0.27\\n6 General Mobile 0.80 0.22\\n7 General Games 0.49 0.00\\n8 General Audio 0.56 0.00\\n9 General Robot-Simulation 0.66 0.00\\n10 General Education 0.66 0.00\\n11 General Finance 0.73 0.00\\n12 General IDE 0.75 0.00\\n13 Robot Robo-Projects 0.86 0.00\\n\\nTable 6. Gradient Boosted Decision Trees top scoring software classes. Mean ACCM\\nis a recurring important feature.\\n\\n6 Discussion\\n\\nUltimately, our goal in this investigation is not only finding out if there is a\\ndifference between agent or robot software, and other software domains, but\\n\\n\\n\\n12 A. T. Zanbar, G. A. Kaminka\\n\\nFig. 4. Top 10 features with highest importance for XGBoost classification in projects\\nin the agent repositories\\n\\nalso to uncover the nature of this difference. This section discusses the results\\npresented above, and attempts to draw conclusions, lessons, and hypotheses for\\nfuture investigations.\\n\\nACCM and Control Complexity of Agents. First, it is clear that the ACCM\\nmeasure is a recurring metric in successful classification schemes distinguishing\\nagent software from other software. This is true both in the manual analysis, as\\nwell as in classifiers generated by machine learning algorithms. In general, Agent\\nsoftware seems to have high ACCM measurements, compared to other software\\ndomains. Robot software does have higher ACCM (on average) than non-agent\\nsoftware, but the difference is much less pronounced than between Agent and\\ngeneral software. It is therefore immediately interesting to better understand\\nwhat the ACCM actually measures.\\n\\nACCM\\xe2\\x80\\x94Average Cyclomatic Complexity per Method\\xe2\\x80\\x94is a more modern\\nvariant of the Cyclomatic Complexity (CC) metric introduced by McCabe in\\n1976 [28]. Briefly, the cyclomatic complexity of software is a measure of the\\nnumber of possible execution paths through its control flow graph. The more\\nbranching points, conditional loops, and decision points in the software, the\\ngreater its CC. The ACCM measures the CC value at the method level, for all\\nmethods within a module. It then computes the mean of these measurements\\nto introduce a single value which represents the complexity of the module as a\\nwhole.\\n\\nCyclomatic Complexity has been generally shown to be inversely correlated\\nto code quality and defect frequency. Greater CC is correlated with a greater\\nnumber of defects in the software, persistent bugs, and other indications of poor\\ndesign and code quality. Indeed, the correlation is sufficiently accepted, that there\\nexists recommended practices for the maintenance of CC values of new software\\nwithin accepted safe range, below the ACCM measurements we generally see\\nhere.\\n\\nIs Agent Software Inherently More Complex? (In short: YES!) There are alter-\\nnative explanations for the higher ACCM values we observe in agent software:\\n(1) that agent software is just inherently more complex, because the tasks tackled\\nby the software requires greater complexity in the control flow of the software.\\n\\n\\n\\nAgents are More Complex than Other Software: An Empirical Investigation 13\\n\\nOr, (2) that the agent code is just more buggy, or written by programmers who\\nare not as well-trained, e.g., too academic?\\n\\nWe offer evidence that the first explanation is the correct one, i.e., that\\nagent software is inherently more complex. One benefit of using competition\\nsoftware in this study is that alongside the software metrics, we also have clear\\nquality metrics in terms of the success of the software. Specifically, we show below\\n(Figure 5) a plot of the ACCM measure from a subset of RoboCup software\\nagent, vs the code effectiveness as measured by the mean goal difference of\\nthe agents in competitions. We see a clear inverse relation between the two:\\nhigher ACCM is associated with poor performance, just as it is in other software\\ndomains. However, the ACCM of winning agents is still higher than standard\\npractice in software.\\n\\nFig. 5. ACCM (Average Cyclomatic Complexity per Module, vertical axis) vs Effec-\\ntiveness (here, measured by mean goal difference per game\\xe2\\x80\\x94horizontal axis, larger\\nis better).The goal difference was extracted automatically from log files of individual\\ngames.\\n\\nWhat about other measures? A critical look at the results of this study raises the\\nissue of other measures. It is true that ACCM is a clear distinguishing charac-\\nteristic of agent vs non-agent code. However, it is not so clear that the machine\\nlearning classifiers can use it, ignoring other metrics. Indeed, some very successful\\nclassifiers do not use ACCM at all. Indeed, we saw also that the AMLOC mea-\\nsure is also a potentially good metric from this respect, as well as the MMLOC\\nmeasure.\\n\\nWhile we do not refute the possibility that other metrics may be as good\\nas ACCM or complement it, we point out that many metrics are known to be\\ncorrelated in practice (see, e.g., [24]), and thus it may be that a machine learning\\nclassifier using a particular metric could have also worked as well with a different\\none, that is highly correlated. In particular, in our own study here, we found that\\nthe Pearson correlation between AMLOC and ACCM is 0.84, and the correlation\\n\\n\\n\\n14 A. T. Zanbar, G. A. Kaminka\\n\\nbetween MMLOC and ACCM is 0.90. So a preference for one metric over another\\ndoes not necessarily mean that the other metric was not as useful.\\n\\n7 The Big Picture\\n\\nThis papers offers the first empirical evidence that agent software is indeed\\ninherently different from other types of software, intended for other domains. The\\nempirical evidence was collected by analyzing hundreds of software projects of\\ncomparable sizes, using two different types of analysis. In particular, we find that\\nagent software has greater control flow complexity in general, which conjecture\\nto be inherent to the types of tasks agents are deployed to solve\\xe2\\x80\\x94tasks that\\nrequire autonomy in decision-making, and thus careful deliberation over many\\npossibilities.\\n\\nGiven this conclusion, it becomes clear that agent-oriented software engineer-\\ning can increase their impact by providing tools, methodologies, and frameworks\\nthat directly tackle the issue of complexity. For instance, agent architectures may\\nbe so successful because they assist in breaking down the inherent complexity of\\ntasks. We leave this question for future work.\\n\\nReferences\\n\\n1. A. J. Albrecht. Measuring application development productivity. In IBM Appli-\\ncations Development Joint SHARE/GUIDE Symposium, pages 83\\xe2\\x80\\x9392, Monterey,\\nCalifornia, 1979.\\n\\n2. F. Alonso, J. L. Fuertes, L. Martinez, and H. Soza. Towards a set of Measures\\nfor Evaluating Software Agent Autonomy. In 2009 Eighth Mexican International\\nConference on Artificial Intelligence, pages 73\\xe2\\x80\\x9378, Nov. 2009.\\n\\n3. F. Alonso, J. L. Fuertes, L. Martnez, and H. Soza. Measuring the Pro-Activity\\nof Software Agents. 2010 Fifth International Conference on Software Engineering\\nAdvances, pages 319\\xe2\\x80\\x93324, 2010.\\n\\n4. B. W. Boehm. Software Engineering Economics. Prentice Hall PTR, Upper Saddle\\nRiver, NJ, USA, 1st edition, 1981.\\n\\n5. O. Boissier, R. H. Bordini, J. F. Hbner, and A. Ricci. Unravelling Multi-agent-\\nOriented Programming. In Agent-Oriented Software Engineering, pages 259\\xe2\\x80\\x93272.\\nSpringer, Berlin, Heidelberg, 2014.\\n\\n6. D. Brugali. Software Engineering for Experimental Robotics. Springer, 2007.\\nGoogle-Books-ID: DEpsCQAAQBAJ.\\n\\n7. D. Brugali. Model-Driven Software Engineering in Robotics: Models Are Designed\\nto Use the Relevant Things, Thereby Reducing the Complexity and Cost in the\\nField of Robotics. IEEE Robotics Automation Magazine, 22(3):155\\xe2\\x80\\x93166, Sept. 2015.\\n\\n8. D. Brugali and P. Scandurra. Component-based robotic engineering (Part I). IEEE\\nRobotics Automation Magazine, 16(4):84\\xe2\\x80\\x9396, 2009.\\n\\n9. D. Brugali and A. Shakhimardanov. Component-Based Robotic Engineering (Part\\nII). IEEE Robotics Automation Magazine, 17(1):100\\xe2\\x80\\x93112, 2010.\\n\\n10. H. Bruyninckx. Open robot control software: the OROCOS project. In Proceedings\\n2001 ICRA. IEEE International Conference on Robotics and Automation (Cat.\\nNo.01CH37164), volume 3, pages 2523\\xe2\\x80\\x932528 vol.3, 2001.\\n\\n\\n\\nAgents are More Complex than Other Software: An Empirical Investigation 15\\n\\n11. D. Calisi, A. Censi, L. Iocchi, and D. Nardi. Openrdk: A modular framework for\\nrobotic software development. In IEEE/RSJ International Conference on Intelli-\\ngent Robots and Systems, pages 1872\\xe2\\x80\\x931877, 2008.\\n\\n12. S. R. Chidamber and C. F. Kemerer. A metrics suite for object oriented design.\\nIEEE Transactions on Software Engineering, 20(6):476\\xe2\\x80\\x93493, 1994-06.\\n\\n13. M. Cossentino, C. Lodato, S. Lopes, P. Ribino, and V. Palermo. Metrics for Eval-\\nuating Modularity and Extensibility in HMAS Systems. In AAMAS, 2015.\\n\\n14. N. T. Dantam, K. B\\xef\\xbf\\xbdondergaard, M. A. Johansson, T. Furuholm, and L. E.\\nKavraki. Unix Philosophy and the Real World: Control Software for Humanoid\\nRobots. Frontiers in Robotics and AI, 3, 2016.\\n\\n15. L. B. L. De Souza and M. D. A. Maia. Do software categories impact coupling\\nmetrics? In Proceedings of the 10th Working Conference on Mining Software Repos-\\nitories, MSR \\xe2\\x80\\x9913, pages 217\\xe2\\x80\\x93220. IEEE Press, 2013.\\n\\n16. S. A. DeLoach. O-MaSE: An Extensible Methodology for Multi-agent Systems. In\\nAgent-Oriented Software Engineering, pages 173\\xe2\\x80\\x93191. Springer, Berlin, Heidelberg,\\n2014.\\n\\n17. A. Elkady and T. Sobh. Robotics Middleware: A Comprehensive Literature Survey\\nand Attribute-Based Bibliography. Journal of Robotics, 2012:1\\xe2\\x80\\x9315, 2012.\\n\\n18. N. Fenton and J. Bieman. Software Metrics: A Rigorous and Practical Approach,\\nThird Edition. CRC Press, 2014-10-01. Google-Books-ID: lx OBQAAQBAJ.\\n\\n19. I. Garca-Magario, M. Cossentino, and V. Seidita. A Metrics Suite for Evaluating\\nAgent-oriented Architectures. In Proceedings of the 2010 ACM Symposium on\\nApplied Computing, SAC \\xe2\\x80\\x9910, pages 912\\xe2\\x80\\x93919, New York, NY, USA, 2010. ACM.\\nevent-place: Sierre, Switzerland.\\n\\n20. B. P. Gerkey, R. T. Vaughan, and A. Howard. The player/stage project: Tools\\nfor multi-robot and distributed sensor systems. In Proceedings of the International\\nConference on Advanced Robotics, 2003.\\n\\n21. J. J. Gomez-Sanz. Ten Years of the INGENIAS Methodology. In Agent-Oriented\\nSoftware Engineering, pages 193\\xe2\\x80\\x93209. Springer, Berlin, Heidelberg, 2014.\\n\\n22. M. H. Halstead. Elements of Software Science (Operating and Programming Sys-\\ntems Series). Elsevier Science Inc., 1977.\\n\\n23. R. V. Hudli, C. L. Hoskins, and A. V. Hudli. Software metrics for object-oriented\\ndesigns. In Proceedings 1994 IEEE International Conference on Computer Design:\\nVLSI in Computers and Processors, pages 492\\xe2\\x80\\x93495, 1994-10.\\n\\n24. G. Jay, J. E. Hale, R. K. Smith, D. Hale, N. A. Kraft, and C. Ward. Cyclomatic\\ncomplexity and lines of code: Empirical evidence of a stable linear relationship.\\nJournal of Software Engineering and Applications, 02(3):137\\xe2\\x80\\x93143, 2009.\\n\\n25. N. R. Jennings. On agent-based software engineering. Artificial Intelligence,\\n117(2):277\\xe2\\x80\\x93296, 2000-03-01.\\n\\n26. C. Jones. Applied Software Measurement: Global Analysis of Productivity and\\nQuality. McGraw-Hill, New York, 3rd edition, 2008.\\n\\n27. R. V. Kumar and R. Chandrasekaran. Classification of software projects using k-\\nmeans, discriminant analysis and artificial neural network. International Journal\\nof Scientific & Engineering Research, 4(2):7, 2013.\\n\\n28. T. J. McCabe. A complexity measure. IEEE Transactions on Software Engineering,\\nSE-2(4):308\\xe2\\x80\\x93320, 1976-12.\\n\\n29. P. Meirelles, C. Santos Jr., J. Miranda, F. Kon, A. Terceiro, and C. Chavez. A\\nstudy of the relationships between source code metrics and attractiveness in free\\nsoftware projects. In 2010 Brazilian Symposium on Software Engineering, pages\\n11\\xe2\\x80\\x9320. IEEE, 2010.\\n\\n\\n\\n16 A. T. Zanbar, G. A. Kaminka\\n\\n30. M. Montemerlo, N. Roy, and S. Thrun. Perspectives on standardization in mo-\\nbile robot programming: the Carnegie Mellon Navigation (CARMEN) Toolkit. In\\nProceedings 2003 IEEE/RSJ International Conference on Intelligent Robots and\\nSystems (IROS 2003) (Cat. No.03CH37453), volume 3, pages 2436\\xe2\\x80\\x932441 vol.3,\\nOct. 2003.\\n\\n31. L. Padgham, J. Thangarajah, and M. Winikoff. Prometheus Research Directions.\\nIn Agent-Oriented Software Engineering, pages 155\\xe2\\x80\\x93171. Springer, Berlin, Heidel-\\nberg, 2014.\\n\\n32. E. K. Piveta, A. Moreira, M. S. Pimenta, J. Arajo, P. Guerreiro, and R. T. Price.\\nAn empirical study of aspect-oriented metrics. Science of Computer Programming,\\n78(1):117\\xe2\\x80\\x93144, 2012-11.\\n\\n33. E. Platon, N. Sabouret, and S. Honiden. An architecture for exception manage-\\nment in multiagent systems. International Journal of Agent-Oriented Software\\nEngineering, 2(3):267, 2008.\\n\\n34. M. Quigley, B. Gerkey, K. Conley, J. Faust, T. Foote, J. Leibs, E. Berger,\\nR. Wheeler, and A. Ng. ROS: an open-source Robot Operating System. In Inter-\\nnational Conference on Robotics and Automation, page 6, 2009.\\n\\n35. Y. Shoham. Agent-oriented programming. Artificial Intelligence, 60:51\\xe2\\x80\\x9392, 1991.\\n36. Y. Shoham. Agent-oriented programming. Artif. Intell., 60(1):51\\xe2\\x80\\x9392, Mar. 1993.\\n37. M. Stojkovski. Thresholds for software quality metrics in open source android\\n\\nprojects. Master\\xe2\\x80\\x99s thesis, NTNU, 2017.\\n38. A. Sturm and O. Shehory. Agent-Oriented Software Engineering: Revisiting the\\n\\nState of the Art. In Agent-Oriented Software Engineering, pages 13\\xe2\\x80\\x9326. Springer,\\nBerlin, Heidelberg, 2014.\\n\\n39. E. Tsardoulias and P. Mitkas. Robotic frameworks, architectures and middleware\\ncomparison. arXiv:1711.06842 [cs], Nov. 2017. arXiv: 1711.06842.\\n\\n40. R. T. Vaughan and B. P. Gerkey. Really Reusable Robot Code and the\\nPlayer/Stage Project. In Brugali, D., editor, Software Engineering for Experi-\\nmental Robotics, page 24. 2006.\\n\\n41. M. Winikoff. Future Directions for Agent-Based Software Engineering. Int. J.\\nAgent-Oriented Softw. Eng., 3(4):402\\xe2\\x80\\x93410, May 2009.\\n\\n\\n\\n\\n'