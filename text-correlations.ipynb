{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text correlations\n",
    "\n",
    "Creates a correlation matrix among a set of given documents showing how strong is the relationship or similarity with them. It grab all documents from a specified folder using tensorflow universal sentence encoder creates vectors of them. \n",
    "\n",
    "By default, the folder with the documents is called `my_documents/` and the resulting files with all correlations will be placed on `correlation/` forlder. \n",
    "\n",
    "The csv output file `correlation/correlation.csv` can be imported in spreadsheet processors such as Microsoft Excel, Libre Office and Google Spreadsheets. The correlation near to 1 is maximum, close to zero is minimal, meaning no correlation. It is also suggested to use their tools for conditional formating to facilitate visualization.\n",
    "\n",
    "By default, intermediary files greater than 500 kBytes are skipped.\n",
    "\n",
    "Two other folders `correlation/texts/` and `correlation/vectors/` are created with intermediary files.\n",
    "\n",
    "The original documents can be PDF, DOC and EPUB and others ([see tika documentation](https://tika.apache.org/0.9/formats.html))\n",
    "\n",
    "Developed using Python 3.7.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from tika import parser # For prsing PDF and other to TXT\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "from os import path, stat\n",
    "import glob # For listing files in forlders\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import tensorflow_text\n",
    "import dill # For binary files\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "# Path for local documents (pdfs, docs, txts, etc.)\n",
    "PATH_FOR_DOCUMENTS = \"my_documents/\"\n",
    "# Path to put plain texts (can leave is as it is)\n",
    "PATH_TEXTS = \"correlation/texts/\"\n",
    "# Path to put vectors - enconded texts according to their contexts (can leave is as it is)\n",
    "PATH_VECTORS = \"correlation/vectors/\"\n",
    "# Path to put the output csv file\n",
    "PATH_CORRELATION_FILE = \"correlation/\"\n",
    "# Max size for the plain text file, skipping bigger than since taks too long to process\n",
    "MAX_FILESIZE = 500*1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers\n",
    "def parse_document(file):\n",
    "    content = parser.from_file(file)\n",
    "    if 'content' in content:\n",
    "        text = content['content']\n",
    "    else:\n",
    "        return\n",
    "    text = str(text)\n",
    "    # Ensure text is utf-8 formatted\n",
    "    safe_text = text.encode('utf-8', errors='ignore')\n",
    "    # Escape any \\ issues\n",
    "    safe_text = str(safe_text).replace('\\\\', '\\\\\\\\').replace('\"', '\\\\\"')\n",
    "    return safe_text\n",
    "    \n",
    "def encode_text(text):\n",
    "        return embed(text)\n",
    "\n",
    "def create_folder(folder):\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)    \n",
    "\n",
    "def save_text(file, text):\n",
    "    text_file = open(PATH_TEXTS + os.path.basename(file) + \".txt\", \"w\")\n",
    "    text_file.write(text)\n",
    "    text_file.close()\n",
    "    \n",
    "def read_text(file):\n",
    "    text_file = open(PATH_TEXTS + os.path.basename(file), \"r\")\n",
    "    text = text_file.read()\n",
    "    text_file.close()\n",
    "    return text\n",
    "    \n",
    "def read_vector(file):\n",
    "    vector = dill.load(open(PATH_VECTORS + os.path.basename(file), \"rb\"))\n",
    "    return vector\n",
    "\n",
    "def save_vector(file, vector):\n",
    "    dill.dump(vector, open(PATH_VECTORS + os.path.basename(file) + \".vec\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folders if does not exist\n",
    "create_folder(PATH_TEXTS)\n",
    "create_folder(PATH_VECTORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow universal-sentence-encoder-multilingual\n",
    "# 16 languages (Arabic, Chinese-simplified, Chinese-traditional, English, French, German, Italian, Japanese, Korean, Dutch, Polish, Portuguese, Spanish, Thai, Turkish, Russian) text encoder.\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are old and unnecessary output files\n",
    "# It will not \n",
    "unnecessary_files = []\n",
    "files = glob.glob(PATH_FOR_DOCUMENTS + \"*\")\n",
    "text_files = glob.glob(PATH_TEXTS + \"*\")\n",
    "vector_files = glob.glob(PATH_VECTORS + \"*\")\n",
    "for tf in text_files: \n",
    "    original_name = os.path.basename(tf[:-4])\n",
    "    if PATH_FOR_DOCUMENTS + original_name not in files:\n",
    "        unnecessary_files.append(tf)\n",
    "for vf in vector_files: \n",
    "    original_text_name = os.path.basename(vf[:-4])\n",
    "    if PATH_TEXTS + original_text_name not in text_files:\n",
    "        unnecessary_files.append(vf)\n",
    "if (unnecessary_files != []):\n",
    "    print(\"The following files seems to be unnecessary:\")\n",
    "    print(unnecessary_files)\n",
    "# Uncomment to remove them\n",
    "    #for f in unnecessary_files:\n",
    "        #os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of files that will be converted to plain text\n",
    "files = glob.glob(PATH_FOR_DOCUMENTS + \"*\")\n",
    "# Create plain text files from original documets\n",
    "for f in files: \n",
    "    if not path.exists(PATH_TEXTS + os.path.basename(f) + \".txt\"):\n",
    "        parsed_document = parse_document(f)\n",
    "        save_text(f, parsed_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of plain text documents that exist in the PATH_TEXTS folder\n",
    "text_files = glob.glob(PATH_TEXTS + \"*\")\n",
    "# Create vectors for the plain texts\n",
    "for tf in text_files: \n",
    "    if os.stat(tf).st_size < MAX_FILESIZE and not path.exists(PATH_VECTORS + os.path.basename(tf) + \".vec\"):\n",
    "        text = read_text(tf)\n",
    "        vector = encode_text(text)\n",
    "        save_vector(tf, vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of vectors that exist in the PATH_VECTORS folder\n",
    "vector_files = glob.glob(PATH_VECTORS + \"*\")\n",
    "# Read existing vectors\n",
    "vectors = []\n",
    "for vf in vector_files: \n",
    "    vector = read_vector(vf)\n",
    "    vectors.append(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all existing files, create a dataframe putting them as rows and coluns in a \"n x n\" structure\n",
    "file_names = []\n",
    "for vf in vector_files:\n",
    "    file_name = os.path.basename(vf)\n",
    "    file_names.append(file_name)\n",
    "df = pd.DataFrame(columns=[file_names])\n",
    "# Start the structure with zeros in the correlations\n",
    "for fn in file_names:\n",
    "    df.loc[fn] = np.zeros(len(file_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process correlations across each document with all others, uptade the \"n x n\" structure with these correlations\n",
    "for i in range(len(file_names)):\n",
    "    for j in range(len(file_names)):\n",
    "        # df.values[row, column] = value\n",
    "        df.values[i, j] = np.inner(vectors[i], vectors[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the correlations into a csv file\n",
    "df.to_csv(PATH_CORRELATION_FILE + \"correlations.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credits\n",
    "Developed by [Cleber Jorge Amaral](https://cleberjamaral.github.io/), acknowledging it is highly inspired by a work presented by Aladdin Shamoug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
